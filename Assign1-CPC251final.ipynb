{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         f1        f2        f3        f4        f5   response\n0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075\n5 -0.699867  0.019159  1.103377 -0.671614 -0.119063 -18.597563\n6 -1.028250  0.962967  0.471027 -1.941219 -0.465591 -73.174734\n7  0.337585  1.352948 -1.789795 -0.885796 -0.846150 -25.865464\n8  0.295433 -0.907789  0.275980 -0.675526 -0.942592  -9.001596\n9  0.442269 -0.704559 -1.127342  1.030206  0.800113  57.076963",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.764216</td>\n      <td>-1.016209</td>\n      <td>0.149410</td>\n      <td>-0.050119</td>\n      <td>-0.578127</td>\n      <td>6.242514</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.763880</td>\n      <td>-1.159509</td>\n      <td>-0.721492</td>\n      <td>-0.654067</td>\n      <td>-0.431670</td>\n      <td>-8.118241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.519329</td>\n      <td>-0.664621</td>\n      <td>-1.694904</td>\n      <td>1.339779</td>\n      <td>0.182764</td>\n      <td>66.722455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.177388</td>\n      <td>0.515623</td>\n      <td>0.135144</td>\n      <td>-0.647634</td>\n      <td>-0.405631</td>\n      <td>-27.716793</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.104022</td>\n      <td>0.749665</td>\n      <td>-0.939338</td>\n      <td>-0.090725</td>\n      <td>-0.639963</td>\n      <td>8.192075</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.699867</td>\n      <td>0.019159</td>\n      <td>1.103377</td>\n      <td>-0.671614</td>\n      <td>-0.119063</td>\n      <td>-18.597563</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-1.028250</td>\n      <td>0.962967</td>\n      <td>0.471027</td>\n      <td>-1.941219</td>\n      <td>-0.465591</td>\n      <td>-73.174734</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.337585</td>\n      <td>1.352948</td>\n      <td>-1.789795</td>\n      <td>-0.885796</td>\n      <td>-0.846150</td>\n      <td>-25.865464</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.295433</td>\n      <td>-0.907789</td>\n      <td>0.275980</td>\n      <td>-0.675526</td>\n      <td>-0.942592</td>\n      <td>-9.001596</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.442269</td>\n      <td>-0.704559</td>\n      <td>-1.127342</td>\n      <td>1.030206</td>\n      <td>0.800113</td>\n      <td>57.076963</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset to be processed with pandas & displaying the top 10 result\n",
    "dt = pd.read_csv('assignment1_dataset.csv', sep=',')\n",
    "dt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                f1           f2           f3           f4           f5  \\\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \nmean      0.012255    -0.043030    -0.065785     0.039616     0.008074   \nstd       0.998816     1.042413     0.982640     1.023960     1.006679   \nmin      -3.174809    -3.381691    -3.158010    -2.764936    -2.946633   \n25%      -0.655282    -0.759477    -0.734505    -0.660802    -0.685371   \n50%      -0.001177    -0.038444    -0.049838    -0.006831    -0.000368   \n75%       0.697331     0.696343     0.591642     0.737806     0.710398   \nmax       3.092866     3.534175     3.406115     3.145835     3.007734   \n\n          response  \ncount  1000.000000  \nmean     11.229435  \nstd      40.028188  \nmin    -103.044475  \n25%     -16.580272  \n50%      10.554227  \n75%      38.485118  \nmax     157.890314  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.012255</td>\n      <td>-0.043030</td>\n      <td>-0.065785</td>\n      <td>0.039616</td>\n      <td>0.008074</td>\n      <td>11.229435</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.998816</td>\n      <td>1.042413</td>\n      <td>0.982640</td>\n      <td>1.023960</td>\n      <td>1.006679</td>\n      <td>40.028188</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-3.174809</td>\n      <td>-3.381691</td>\n      <td>-3.158010</td>\n      <td>-2.764936</td>\n      <td>-2.946633</td>\n      <td>-103.044475</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.655282</td>\n      <td>-0.759477</td>\n      <td>-0.734505</td>\n      <td>-0.660802</td>\n      <td>-0.685371</td>\n      <td>-16.580272</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.001177</td>\n      <td>-0.038444</td>\n      <td>-0.049838</td>\n      <td>-0.006831</td>\n      <td>-0.000368</td>\n      <td>10.554227</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.697331</td>\n      <td>0.696343</td>\n      <td>0.591642</td>\n      <td>0.737806</td>\n      <td>0.710398</td>\n      <td>38.485118</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.092866</td>\n      <td>3.534175</td>\n      <td>3.406115</td>\n      <td>3.145835</td>\n      <td>3.007734</td>\n      <td>157.890314</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying additional description\n",
    "dt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "f2         -0.031751\nf5         -0.028999\nf3          0.015218\nf1          0.308474\nf4          0.947255\nresponse    1.000000\nName: response, dtype: float64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a correlation matrix between the columns/features and target in ascending order\n",
    "corr_matrix = dt.corr()\n",
    "corr_matrix['response'].sort_values(ascending=True)\n",
    "# Correlation between f4 and response are the closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'relationship between f4 & response')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's plot f4 & response, cuz f4 corr value is close to 1\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(dt.f4, dt.response)\n",
    "plt.xlabel('f4')\n",
    "plt.ylabel('response')\n",
    "plt.title('relationship between f4 & response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         f1        f2        f3        f4        f5   response\n0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.764216</td>\n      <td>-1.016209</td>\n      <td>0.149410</td>\n      <td>-0.050119</td>\n      <td>-0.578127</td>\n      <td>6.242514</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.763880</td>\n      <td>-1.159509</td>\n      <td>-0.721492</td>\n      <td>-0.654067</td>\n      <td>-0.431670</td>\n      <td>-8.118241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.519329</td>\n      <td>-0.664621</td>\n      <td>-1.694904</td>\n      <td>1.339779</td>\n      <td>0.182764</td>\n      <td>66.722455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.177388</td>\n      <td>0.515623</td>\n      <td>0.135144</td>\n      <td>-0.647634</td>\n      <td>-0.405631</td>\n      <td>-27.716793</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.104022</td>\n      <td>0.749665</td>\n      <td>-0.939338</td>\n      <td>-0.090725</td>\n      <td>-0.639963</td>\n      <td>8.192075</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine each column to be processed\n",
    "columns = ['f1','f2','f3','f4','f5','response']\n",
    "dt = dt.loc[:, columns]\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the training and test set with the ratio of 8:2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "features = ['f1','f2','f3','f4','f5'] # Data that we want to utilize as training & test\n",
    "#X_data = dt.loc[:, features] # X are the data we want to use from 'features' = independent variable\n",
    "#y_data = dt.loc[:, ['response']] # y is the data we want to use as target = dependent variable\n",
    "\n",
    "X_data = np.array(dt.iloc[:, 3])\n",
    "y_data = np.array(dt.iloc[:, -1])\n",
    "\n",
    "\"\"\"\n",
    "X = dt[['f1','f2','f3','f4','f5']]\n",
    "y = dt['response']\n",
    "X_data = np.array(X)\n",
    "y_data = np.array(y)\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=42, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.1 # Set learning rate to 0.15\n",
    "max_epoch = 1500 # Set max iteration to 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) (800,)\n",
      "(200,) (200,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def loss_fn(y, yhat):\n",
    "    loss = np.sum((y-yhat)**2)/len(y)\n",
    "    return loss\n",
    "    #loss = mean_squared_error(y, yhat)\n",
    "    #return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train_model(X, y, alpha, max_epoch):\n",
    "    w = b = 0\n",
    "    n = len(X)\n",
    "    losses = []\n",
    "    weights = []\n",
    "\n",
    "    def prediction(w, X):\n",
    "            yhat = (w * X) + b\n",
    "            return yhat;\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        y_predict = prediction(w, X)\n",
    "        loss = loss_fn(y, y_predict)\n",
    "\n",
    "        losses.append(loss)\n",
    "        weights.append(w)\n",
    "\n",
    "        loss_fn(y, y_predict)\n",
    "\n",
    "        wd = -(2/n)*sum(X*(y-y_predict))\n",
    "        bd = -(2/n)*sum(y-y_predict)\n",
    "\n",
    "        w = w - alpha * wd\n",
    "        b = b - alpha * bd\n",
    "\n",
    "        print(f\"Iteration {i+1}: Loss {loss}, Weight {w}, Bias {b}\");\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(weights, losses)\n",
    "    plt.scatter(weights, losses, marker='o', color='red')\n",
    "    plt.title(\"Loss vs Weights\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Weight\")\n",
    "    plt.show()\n",
    "\n",
    "    return w, b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss 1721.3069867699319, Weight 7.797401385304804, Bias 2.4463995536742518\n",
      "Iteration 2: Loss 1125.1444709007278, Weight 13.944652018010773, Bias 4.292207376852859\n",
      "Iteration 3: Loss 757.3538254506886, Weight 18.792153417127615, Bias 5.681098546150019\n",
      "Iteration 4: Loss 530.3093438034626, Weight 22.615670596550096, Bias 6.723010965240647\n",
      "Iteration 5: Loss 390.05685794210285, Weight 25.632274588849672, Bias 7.501958273032889\n",
      "Iteration 6: Loss 303.35766820936175, Weight 28.012870951330264, Bias 8.082052582674432\n",
      "Iteration 7: Loss 249.72365997280704, Weight 29.89204702170392, Bias 8.5121438220432\n",
      "Iteration 8: Loss 216.51874025450874, Weight 31.375813091554644, Bias 8.829390631620935\n",
      "Iteration 9: Loss 195.94477175394292, Weight 32.547689023852804, Bias 9.062006573654124\n",
      "Iteration 10: Loss 183.18614719911707, Weight 33.47349082767004, Bias 9.23137020996169\n",
      "Iteration 11: Loss 175.26703274799772, Weight 34.20509557430037, Bias 9.353644833423814\n",
      "Iteration 12: Loss 170.34716731412925, Weight 34.78340329071205, Bias 9.441020507105033\n",
      "Iteration 13: Loss 167.28766707286684, Weight 35.24066756775813, Bias 9.50266541303658\n",
      "Iteration 14: Loss 165.38314901465003, Weight 35.60232980326758, Bias 9.545453660639849\n",
      "Iteration 15: Loss 164.1963601779056, Weight 35.88846309291413, Bias 9.574521348724117\n",
      "Iteration 16: Loss 163.45601949094146, Weight 36.11490908186563, Bias 9.593690802817592\n",
      "Iteration 17: Loss 162.99366491557464, Weight 36.294173262073265, Bias 9.605793735904165\n",
      "Iteration 18: Loss 162.7045842504927, Weight 36.4361301959652, Bias 9.612916996194473\n",
      "Iteration 19: Loss 162.52362636389822, Weight 36.54857914528869, Bias 9.61658909792047\n",
      "Iteration 20: Loss 162.4102130697454, Weight 36.63768193893933, Bias 9.61792151409035\n",
      "Iteration 21: Loss 162.3390440765964, Weight 36.7083081196986, Bias 9.617715459909403\n",
      "Iteration 22: Loss 162.29432740888038, Weight 36.76430706963819, Bias 9.616542392373713\n",
      "Iteration 23: Loss 162.26619480358238, Weight 36.80872261573262, Bias 9.614804525218345\n",
      "Iteration 24: Loss 162.24847245606375, Weight 36.843962316162525, Bias 9.612780177273018\n",
      "Iteration 25: Loss 162.23729325410065, Weight 36.87193103168795, Bias 9.610657634499672\n",
      "Iteration 26: Loss 162.23023195448744, Weight 36.894136344391605, Bias 9.608560332824098\n",
      "Iteration 27: Loss 162.22576566096265, Weight 36.91177177949309, Bias 9.606565499495762\n",
      "Iteration 28: Loss 162.22293686514885, Weight 36.92578252169782, Bias 9.60471787814247\n",
      "Iteration 29: Loss 162.22114275548557, Weight 36.9369173225104, Bias 9.603039770680232\n",
      "Iteration 30: Loss 162.2200033216085, Weight 36.945769511624874, Bias 9.601538329838187\n",
      "Iteration 31: Loss 162.21927868524287, Weight 36.952809408724754, Bias 9.600210807720359\n",
      "Iteration 32: Loss 162.21881722054502, Weight 36.95840994627174, Bias 9.599048291960344\n",
      "Iteration 33: Loss 162.21852295517886, Weight 36.962866931213085, Bias 9.598038328866883\n",
      "Iteration 34: Loss 162.21833506002676, Weight 36.96641507204131, Bias 9.597166732694276\n",
      "Iteration 35: Loss 162.2182149277323, Weight 36.9692406600296, Bias 9.596418804264388\n",
      "Iteration 36: Loss 162.218138021346, Weight 36.9714916061602, Bias 9.595780124833249\n",
      "Iteration 37: Loss 162.21808872520944, Weight 36.973285387576055, Bias 9.595237047902149\n",
      "Iteration 38: Loss 162.21805708790586, Weight 36.97471534090875, Bias 9.594776979226921\n",
      "Iteration 39: Loss 162.21803675921765, Weight 36.975855647949984, Bias 9.59438851098495\n",
      "Iteration 40: Loss 162.21802368159348, Weight 36.97676528662876, Bias 9.594061457935826\n",
      "Iteration 41: Loss 162.2180152590465, Weight 36.97749116302975, Bias 9.593786829947668\n",
      "Iteration 42: Loss 162.21800982857013, Weight 36.97807059500805, Bias 9.593556765307063\n",
      "Iteration 43: Loss 162.2180063235013, Weight 36.97853328227666, Bias 9.593364441912163\n",
      "Iteration 44: Loss 162.2180040588407, Weight 36.97890286965919, Bias 9.593203978103173\n",
      "Iteration 45: Loss 162.21800259416622, Weight 36.97919818793141, Bias 9.59307033101053\n",
      "Iteration 46: Loss 162.21800164597965, Weight 36.979434239074315, Bias 9.592959197519804\n",
      "Iteration 47: Loss 162.21800103158944, Weight 36.97962297884637, Bias 9.59286692097864\n",
      "Iteration 48: Loss 162.21800063313765, Weight 36.97977393857817, Bias 9.592790405390698\n",
      "Iteration 49: Loss 162.21800037451206, Weight 36.97989471938759, Bias 9.592727037894377\n",
      "Iteration 50: Loss 162.21800020650997, Weight 36.979991385125274, Bias 9.592674619690625\n",
      "Iteration 51: Loss 162.2180000972932, Weight 36.980068774907686, Bias 9.592631305175681\n",
      "Iteration 52: Loss 162.21800002624087, Weight 36.98013075177805, Bias 9.592595548785727\n",
      "Iteration 53: Loss 162.21799997998497, Weight 36.980180400616106, Bias 9.592566058922822\n",
      "Iteration 54: Loss 162.21799994985216, Weight 36.98022018570878, Bias 9.592541758270405\n",
      "Iteration 55: Loss 162.21799993021034, Weight 36.98025207624662, Bias 9.5925217497963\n",
      "Iteration 56: Loss 162.2179999173995, Weight 36.98027764630907, Bias 9.592505287763574\n",
      "Iteration 57: Loss 162.21799990903935, Weight 36.98029815455171, Bias 9.592491753111915\n",
      "Iteration 58: Loss 162.21799990358076, Weight 36.98031460773774, Bias 9.592480632625126\n",
      "Iteration 59: Loss 162.21799990001497, Weight 36.98032781140637, Bias 9.592471501358203\n",
      "Iteration 60: Loss 162.21799989768454, Weight 36.98033841029613, Bias 9.592464007855666\n",
      "Iteration 61: Loss 162.21799989616085, Weight 36.980346920605534, Bias 9.592457861749164\n",
      "Iteration 62: Loss 162.21799989516413, Weight 36.98035375574804, Bias 9.592452823375023\n",
      "Iteration 63: Loss 162.21799989451196, Weight 36.98035924692007, Bias 9.592448695100618\n",
      "Iteration 64: Loss 162.21799989408504, Weight 36.98036365953219, Bias 9.592445314091856\n",
      "Iteration 65: Loss 162.21799989380546, Weight 36.980367206339864, Bias 9.592442546292594\n",
      "Iteration 66: Loss 162.21799989362236, Weight 36.980370057940114, Bias 9.592440281420723\n",
      "Iteration 67: Loss 162.2179998935024, Weight 36.98037235116521, Bias 9.592438428815202\n",
      "Iteration 68: Loss 162.21799989342372, Weight 36.980374195796934, Bias 9.592436913993845\n",
      "Iteration 69: Loss 162.21799989337217, Weight 36.9803756799391, Bias 9.592435675803715\n",
      "Iteration 70: Loss 162.21799989333837, Weight 36.98037687431787, Bias 9.592434664064736\n",
      "Iteration 71: Loss 162.2179998933162, Weight 36.98037783572485, Bias 9.592433837623197\n",
      "Iteration 72: Loss 162.21799989330168, Weight 36.9803786097748, Bias 9.5924331627454\n",
      "Iteration 73: Loss 162.2179998932921, Weight 36.980379233114974, Bias 9.592432611793209\n",
      "Iteration 74: Loss 162.21799989328585, Weight 36.98037973519571, Bias 9.592432162132962\n",
      "Iteration 75: Loss 162.21799989328173, Weight 36.98038013968984, Bias 9.59243179523731\n",
      "Iteration 76: Loss 162.21799989327903, Weight 36.980380465630866, Bias 9.592431495946432\n",
      "Iteration 77: Loss 162.21799989327727, Weight 36.98038072832592, Bias 9.592431251860758\n",
      "Iteration 78: Loss 162.21799989327613, Weight 36.980380940088295, Bias 9.592431052842114\n",
      "Iteration 79: Loss 162.21799989327533, Weight 36.980381110825306, Bias 9.592430890604184\n",
      "Iteration 80: Loss 162.21799989327485, Weight 36.98038124851027, Bias 9.592430758376485\n",
      "Iteration 81: Loss 162.2179998932745, Weight 36.98038135956149, Bias 9.592430650628804\n",
      "Iteration 82: Loss 162.21799989327428, Weight 36.98038144914667, Bias 9.592430562845347\n",
      "Iteration 83: Loss 162.21799989327417, Weight 36.98038152142748, Bias 9.592430491339709\n",
      "Iteration 84: Loss 162.21799989327405, Weight 36.98038157975615, Bias 9.592430433103353\n",
      "Iteration 85: Loss 162.217999893274, Weight 36.98038162683342, Bias 9.592430385681595\n",
      "Iteration 86: Loss 162.21799989327394, Weight 36.980381664835626, Bias 9.592430347072138\n",
      "Iteration 87: Loss 162.2179998932739, Weight 36.98038169551687, Bias 9.592430315642073\n",
      "Iteration 88: Loss 162.2179998932739, Weight 36.98038172029119, Bias 9.59243029006003\n",
      "Iteration 89: Loss 162.2179998932739, Weight 36.980381740298725, Bias 9.59243026924073\n",
      "Iteration 90: Loss 162.2179998932739, Weight 36.98038175645891, Bias 9.592430252299671\n",
      "Iteration 91: Loss 162.21799989327388, Weight 36.98038176951337, Bias 9.59243023851613\n",
      "Iteration 92: Loss 162.21799989327388, Weight 36.98038178006037, Bias 9.592430227302938\n",
      "Iteration 93: Loss 162.21799989327388, Weight 36.98038178858264, Bias 9.592430218181821\n",
      "Iteration 94: Loss 162.21799989327386, Weight 36.98038179546973, Bias 9.592430210763268\n",
      "Iteration 95: Loss 162.21799989327388, Weight 36.980381801036074, Bias 9.592430204730109\n",
      "Iteration 96: Loss 162.21799989327388, Weight 36.98038180553548, Bias 9.59243019982412\n",
      "Iteration 97: Loss 162.21799989327386, Weight 36.98038180917287, Bias 9.592430195835098\n",
      "Iteration 98: Loss 162.21799989327388, Weight 36.98038181211372, Bias 9.592430192591953\n",
      "Iteration 99: Loss 162.21799989327388, Weight 36.98038181449167, Bias 9.592430189955456\n",
      "Iteration 100: Loss 162.21799989327388, Weight 36.98038181641467, Bias 9.592430187812312\n",
      "Iteration 101: Loss 162.21799989327388, Weight 36.98038181796991, Bias 9.592430186070345\n",
      "Iteration 102: Loss 162.21799989327388, Weight 36.98038181922786, Bias 9.59243018465457\n",
      "Iteration 103: Loss 162.21799989327388, Weight 36.980381820245434, Bias 9.592430183503991\n",
      "Iteration 104: Loss 162.21799989327388, Weight 36.98038182106864, Bias 9.592430182569002\n",
      "Iteration 105: Loss 162.21799989327386, Weight 36.98038182173467, Bias 9.59243018180926\n",
      "Iteration 106: Loss 162.21799989327388, Weight 36.98038182227358, Bias 9.592430181191956\n",
      "Iteration 107: Loss 162.21799989327388, Weight 36.98038182270967, Bias 9.592430180690421\n",
      "Iteration 108: Loss 162.21799989327388, Weight 36.98038182306258, Bias 9.592430180282967\n",
      "Iteration 109: Loss 162.21799989327388, Weight 36.9803818233482, Bias 9.592430179951966\n",
      "Iteration 110: Loss 162.21799989327388, Weight 36.980381823579386, Bias 9.592430179683088\n",
      "Iteration 111: Loss 162.2179998932739, Weight 36.980381823766514, Bias 9.592430179464685\n",
      "Iteration 112: Loss 162.21799989327388, Weight 36.980381823917995, Bias 9.592430179287291\n",
      "Iteration 113: Loss 162.21799989327388, Weight 36.980381824040634, Bias 9.592430179143214\n",
      "Iteration 114: Loss 162.21799989327386, Weight 36.980381824139926, Bias 9.592430179026202\n",
      "Iteration 115: Loss 162.21799989327388, Weight 36.980381824220316, Bias 9.592430178931174\n",
      "Iteration 116: Loss 162.21799989327388, Weight 36.98038182428541, Bias 9.592430178854006\n",
      "Iteration 117: Loss 162.21799989327386, Weight 36.980381824338124, Bias 9.592430178791341\n",
      "Iteration 118: Loss 162.21799989327386, Weight 36.980381824380814, Bias 9.592430178740457\n",
      "Iteration 119: Loss 162.21799989327388, Weight 36.98038182441538, Bias 9.59243017869914\n",
      "Iteration 120: Loss 162.21799989327388, Weight 36.98038182444338, Bias 9.592430178665593\n",
      "Iteration 121: Loss 162.21799989327388, Weight 36.98038182446606, Bias 9.592430178638356\n",
      "Iteration 122: Loss 162.21799989327388, Weight 36.980381824484425, Bias 9.592430178616242\n",
      "Iteration 123: Loss 162.21799989327388, Weight 36.980381824499304, Bias 9.592430178598288\n",
      "Iteration 124: Loss 162.21799989327388, Weight 36.980381824511355, Bias 9.592430178583713\n",
      "Iteration 125: Loss 162.21799989327388, Weight 36.98038182452112, Bias 9.592430178571881\n",
      "Iteration 126: Loss 162.21799989327386, Weight 36.98038182452903, Bias 9.592430178562276\n",
      "Iteration 127: Loss 162.21799989327388, Weight 36.98038182453544, Bias 9.59243017855448\n",
      "Iteration 128: Loss 162.21799989327388, Weight 36.980381824540636, Bias 9.59243017854815\n",
      "Iteration 129: Loss 162.21799989327386, Weight 36.98038182454484, Bias 9.592430178543013\n",
      "Iteration 130: Loss 162.21799989327388, Weight 36.98038182454825, Bias 9.592430178538844\n",
      "Iteration 131: Loss 162.21799989327388, Weight 36.98038182455102, Bias 9.592430178535459\n",
      "Iteration 132: Loss 162.21799989327386, Weight 36.980381824553255, Bias 9.592430178532712\n",
      "Iteration 133: Loss 162.21799989327388, Weight 36.98038182455507, Bias 9.592430178530483\n",
      "Iteration 134: Loss 162.21799989327388, Weight 36.98038182455654, Bias 9.592430178528673\n",
      "Iteration 135: Loss 162.21799989327388, Weight 36.98038182455773, Bias 9.592430178527204\n",
      "Iteration 136: Loss 162.21799989327388, Weight 36.9803818245587, Bias 9.592430178526012\n",
      "Iteration 137: Loss 162.21799989327388, Weight 36.98038182455948, Bias 9.592430178525044\n",
      "Iteration 138: Loss 162.21799989327388, Weight 36.98038182456011, Bias 9.592430178524259\n",
      "Iteration 139: Loss 162.21799989327388, Weight 36.980381824560624, Bias 9.592430178523621\n",
      "Iteration 140: Loss 162.21799989327388, Weight 36.98038182456104, Bias 9.592430178523104\n",
      "Iteration 141: Loss 162.21799989327388, Weight 36.980381824561384, Bias 9.592430178522685\n",
      "Iteration 142: Loss 162.21799989327388, Weight 36.980381824561654, Bias 9.592430178522344\n",
      "Iteration 143: Loss 162.21799989327388, Weight 36.980381824561874, Bias 9.592430178522068\n",
      "Iteration 144: Loss 162.21799989327388, Weight 36.98038182456205, Bias 9.592430178521845\n",
      "Iteration 145: Loss 162.21799989327388, Weight 36.9803818245622, Bias 9.592430178521663\n",
      "Iteration 146: Loss 162.21799989327386, Weight 36.98038182456232, Bias 9.592430178521516\n",
      "Iteration 147: Loss 162.21799989327386, Weight 36.98038182456242, Bias 9.592430178521395\n",
      "Iteration 148: Loss 162.21799989327388, Weight 36.9803818245625, Bias 9.592430178521298\n",
      "Iteration 149: Loss 162.21799989327386, Weight 36.98038182456256, Bias 9.59243017852122\n",
      "Iteration 150: Loss 162.21799989327388, Weight 36.98038182456261, Bias 9.592430178521155\n",
      "Iteration 151: Loss 162.21799989327388, Weight 36.980381824562656, Bias 9.592430178521104\n",
      "Iteration 152: Loss 162.21799989327388, Weight 36.98038182456269, Bias 9.592430178521061\n",
      "Iteration 153: Loss 162.21799989327386, Weight 36.98038182456272, Bias 9.592430178521028\n",
      "Iteration 154: Loss 162.21799989327386, Weight 36.98038182456274, Bias 9.592430178520999\n",
      "Iteration 155: Loss 162.21799989327388, Weight 36.980381824562755, Bias 9.592430178520976\n",
      "Iteration 156: Loss 162.21799989327388, Weight 36.98038182456277, Bias 9.592430178520958\n",
      "Iteration 157: Loss 162.21799989327388, Weight 36.980381824562784, Bias 9.592430178520944\n",
      "Iteration 158: Loss 162.21799989327388, Weight 36.98038182456279, Bias 9.592430178520932\n",
      "Iteration 159: Loss 162.21799989327388, Weight 36.9803818245628, Bias 9.592430178520921\n",
      "Iteration 160: Loss 162.21799989327388, Weight 36.980381824562805, Bias 9.592430178520914\n",
      "Iteration 161: Loss 162.21799989327388, Weight 36.98038182456281, Bias 9.592430178520907\n",
      "Iteration 162: Loss 162.21799989327388, Weight 36.98038182456281, Bias 9.592430178520901\n",
      "Iteration 163: Loss 162.21799989327388, Weight 36.98038182456281, Bias 9.592430178520898\n",
      "Iteration 164: Loss 162.21799989327388, Weight 36.98038182456281, Bias 9.592430178520894\n",
      "Iteration 165: Loss 162.21799989327388, Weight 36.98038182456281, Bias 9.592430178520893\n",
      "Iteration 166: Loss 162.21799989327388, Weight 36.98038182456281, Bias 9.59243017852089\n",
      "Iteration 167: Loss 162.21799989327388, Weight 36.98038182456281, Bias 9.592430178520889\n",
      "Iteration 168: Loss 162.21799989327388, Weight 36.98038182456281, Bias 9.592430178520887\n",
      "Iteration 169: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520885\n",
      "Iteration 170: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 171: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 172: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 173: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 174: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 175: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 176: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 177: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 178: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 179: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 180: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 181: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 182: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 183: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 184: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 185: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 186: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 187: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 188: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 189: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 190: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 191: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 192: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 193: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 194: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 195: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 196: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 197: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 198: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 199: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 200: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 201: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 202: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 203: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 204: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 205: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 206: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 207: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 208: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 209: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 210: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 211: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 212: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 213: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 214: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 215: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 216: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 217: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 218: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 219: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 220: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 221: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 222: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 223: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 224: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 225: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 226: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 227: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 228: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 229: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 230: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 231: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 232: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 233: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 234: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 235: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 236: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 237: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 238: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 239: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 240: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 241: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 242: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 243: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 244: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 245: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 246: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 247: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 248: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 249: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 250: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 251: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 252: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 253: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 254: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 255: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 256: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 257: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 258: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 259: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 260: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 261: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 262: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 263: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 264: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 265: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 266: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 267: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 268: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 269: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 270: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 271: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 272: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 273: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 274: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 275: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 276: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 277: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 278: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 279: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 280: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 281: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 282: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 283: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 284: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 285: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 286: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 287: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 288: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 289: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 290: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 291: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 292: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 293: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 294: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 295: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 296: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 297: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 298: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 299: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 300: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 301: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 302: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 303: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 304: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 305: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 306: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 307: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 308: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 309: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 310: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 311: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 312: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 313: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 314: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 315: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 316: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 317: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 318: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 319: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 320: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 321: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 322: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 323: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 324: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 325: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 326: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 327: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 328: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 329: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 330: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 331: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 332: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 333: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 334: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 335: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 336: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 337: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 338: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 339: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 340: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 341: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 342: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 343: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 344: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 345: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 346: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 347: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 348: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 349: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 350: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 351: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 352: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 353: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 354: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 355: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 356: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 357: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 358: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 359: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 360: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 361: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 362: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 363: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 364: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 365: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 366: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 367: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 368: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 369: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 370: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 371: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 372: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 373: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 374: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 375: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 376: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 377: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 378: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 379: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 380: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 381: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 382: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 383: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 384: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 385: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 386: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 387: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 388: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 389: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 390: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 391: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 392: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 393: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 394: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 395: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 396: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 397: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 398: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 399: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 400: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 401: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 402: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 403: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 404: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 405: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 406: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 407: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 408: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 409: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 410: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 411: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 412: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 413: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 414: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 415: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 416: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 417: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 418: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 419: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 420: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 421: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 422: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 423: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 424: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 425: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 426: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 427: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 428: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 429: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 430: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 431: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 432: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 433: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 434: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 435: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 436: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 437: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 438: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 439: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 440: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 441: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 442: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 443: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 444: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 445: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 446: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 447: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 448: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 449: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 450: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 451: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 452: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 453: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 454: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 455: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 456: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 457: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 458: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 459: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 460: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 461: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 462: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 463: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 464: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 465: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 466: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 467: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 468: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 469: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 470: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 471: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 472: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 473: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 474: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 475: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 476: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 477: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 478: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 479: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 480: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 481: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 482: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 483: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 484: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 485: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 486: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 487: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 488: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 489: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 490: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 491: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 492: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 493: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 494: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 495: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 496: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 497: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 498: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 499: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 500: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 501: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 502: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 503: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 504: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 505: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 506: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 507: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 508: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 509: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 510: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 511: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 512: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 513: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 514: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 515: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 516: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 517: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 518: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 519: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 520: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 521: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 522: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 523: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 524: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 525: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 526: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 527: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 528: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 529: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 530: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 531: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 532: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 533: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 534: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 535: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 536: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 537: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 538: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 539: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 540: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 541: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 542: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 543: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 544: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 545: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 546: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 547: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 548: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 549: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 550: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 551: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 552: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 553: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 554: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 555: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 556: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 557: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 558: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 559: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 560: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 561: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 562: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 563: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 564: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 565: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 566: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 567: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 568: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 569: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 570: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 571: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 572: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 573: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 574: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 575: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 576: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 577: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 578: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 579: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 580: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 581: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 582: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 583: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 584: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 585: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 586: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 587: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 588: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 589: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 590: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 591: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 592: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 593: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 594: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 595: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 596: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 597: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 598: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 599: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 600: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 601: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 602: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 603: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 604: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 605: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 606: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 607: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 608: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 609: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 610: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 611: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 612: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 613: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 614: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 615: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 616: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 617: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 618: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 619: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 620: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 621: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 622: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 623: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 624: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 625: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 626: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 627: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 628: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 629: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 630: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 631: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 632: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 633: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 634: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 635: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 636: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 637: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 638: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 639: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 640: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 641: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 642: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 643: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 644: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 645: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 646: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 647: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 648: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 649: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 650: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 651: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 652: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 653: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 654: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 655: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 656: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 657: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 658: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 659: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 660: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 661: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 662: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 663: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 664: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 665: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 666: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 667: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 668: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 669: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 670: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 671: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 672: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 673: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 674: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 675: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 676: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 677: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 678: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 679: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 680: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 681: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 682: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 683: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 684: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 685: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 686: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 687: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 688: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 689: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 690: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 691: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 692: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 693: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 694: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 695: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 696: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 697: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 698: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 699: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 700: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 701: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 702: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 703: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 704: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 705: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 706: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 707: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 708: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 709: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 710: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 711: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 712: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 713: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 714: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 715: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 716: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 717: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 718: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 719: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 720: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 721: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 722: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 723: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 724: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 725: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 726: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 727: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 728: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 729: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 730: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 731: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 732: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 733: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 734: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 735: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 736: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 737: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 738: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 739: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 740: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 741: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 742: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 743: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 744: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 745: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 746: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 747: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 748: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 749: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 750: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 751: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 752: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 753: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 754: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 755: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 756: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 757: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 758: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 759: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 760: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 761: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 762: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 763: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 764: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 765: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 766: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 767: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 768: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 769: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 770: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 771: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 772: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 773: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 774: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 775: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 776: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 777: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 778: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 779: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 780: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 781: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 782: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 783: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 784: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 785: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 786: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 787: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 788: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 789: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 790: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 791: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 792: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 793: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 794: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 795: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 796: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 797: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 798: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 799: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 800: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 801: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 802: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 803: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 804: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 805: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 806: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 807: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 808: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 809: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 810: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 811: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 812: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 813: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 814: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 815: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 816: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 817: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 818: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 819: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 820: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 821: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 822: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 823: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 824: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 825: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 826: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 827: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 828: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 829: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 830: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 831: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 832: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 833: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 834: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 835: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 836: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 837: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 838: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 839: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 840: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 841: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 842: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 843: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 844: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 845: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 846: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 847: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 848: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 849: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 850: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 851: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 852: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 853: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 854: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 855: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 856: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 857: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 858: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 859: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 860: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 861: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 862: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 863: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 864: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 865: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 866: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 867: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 868: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 869: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 870: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 871: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 872: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 873: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 874: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 875: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 876: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 877: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 878: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 879: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 880: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 881: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 882: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 883: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 884: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 885: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 886: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 887: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 888: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 889: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 890: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 891: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 892: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 893: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 894: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 895: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 896: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 897: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 898: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 899: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 900: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 901: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 902: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 903: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 904: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 905: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 906: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 907: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 908: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 909: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 910: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 911: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 912: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 913: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 914: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 915: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 916: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 917: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 918: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 919: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 920: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 921: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 922: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 923: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 924: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 925: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 926: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 927: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 928: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 929: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 930: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 931: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 932: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 933: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 934: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 935: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 936: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 937: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 938: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 939: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 940: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 941: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 942: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 943: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 944: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 945: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 946: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 947: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 948: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 949: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 950: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 951: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 952: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 953: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 954: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 955: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 956: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 957: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 958: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 959: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 960: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 961: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 962: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 963: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 964: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 965: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 966: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 967: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 968: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 969: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 970: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 971: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 972: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 973: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 974: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 975: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 976: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 977: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 978: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 979: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 980: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 981: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 982: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 983: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 984: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 985: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 986: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 987: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 988: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 989: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 990: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 991: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 992: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 993: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 994: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 995: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 996: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 997: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 998: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 999: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1000: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1001: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1002: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1003: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1004: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1005: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1006: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1007: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1008: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1009: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1010: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1011: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1012: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1013: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1014: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1015: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1016: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1017: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1018: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1019: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1020: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1021: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1022: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1023: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1024: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1025: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1026: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1027: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1028: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1029: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1030: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1031: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1032: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1033: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1034: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1035: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1036: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1037: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1038: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1039: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1040: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1041: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1042: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1043: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1044: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1045: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1046: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1047: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1048: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1049: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1050: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1051: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1052: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1053: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1054: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1055: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1056: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1057: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1058: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1059: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1060: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1061: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1062: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1063: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1064: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1065: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1066: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1067: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1068: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1069: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1070: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1071: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1072: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1073: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1074: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1075: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1076: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1077: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1078: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1079: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1080: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1081: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1082: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1083: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1084: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1085: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1086: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1087: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1088: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1089: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1090: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1091: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1092: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1093: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1094: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1095: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1096: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1097: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1098: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1099: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1100: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1101: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1102: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1103: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1104: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1105: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1106: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1107: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1108: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1109: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1110: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1111: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1112: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1113: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1114: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1115: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1116: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1117: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1118: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1119: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1120: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1121: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1122: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1123: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1124: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1125: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1126: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1127: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1128: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1129: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1130: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1131: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1132: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1133: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1134: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1135: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1136: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1137: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1138: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1139: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1140: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1141: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1142: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1143: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1144: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1145: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1146: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1147: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1148: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1149: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1150: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1151: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1152: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1153: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1154: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1155: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1156: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1157: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1158: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1159: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1160: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1161: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1162: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1163: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1164: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1165: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1166: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1167: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1168: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1169: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1170: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1171: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1172: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1173: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1174: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1175: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1176: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1177: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1178: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1179: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1180: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1181: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1182: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1183: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1184: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1185: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1186: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1187: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1188: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1189: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1190: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1191: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1192: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1193: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1194: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1195: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1196: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1197: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1198: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1199: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1200: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1201: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1202: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1203: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1204: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1205: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1206: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1207: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1208: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1209: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1210: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1211: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1212: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1213: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1214: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1215: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1216: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1217: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1218: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1219: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1220: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1221: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1222: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1223: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1224: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1225: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1226: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1227: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1228: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1229: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1230: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1231: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1232: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1233: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1234: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1235: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1236: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1237: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1238: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1239: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1240: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1241: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1242: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1243: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1244: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1245: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1246: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1247: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1248: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1249: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1250: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1251: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1252: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1253: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1254: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1255: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1256: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1257: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1258: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1259: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1260: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1261: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1262: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1263: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1264: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1265: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1266: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1267: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1268: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1269: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1270: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1271: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1272: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1273: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1274: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1275: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1276: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1277: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1278: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1279: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1280: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1281: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1282: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1283: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1284: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1285: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1286: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1287: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1288: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1289: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1290: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1291: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1292: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1293: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1294: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1295: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1296: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1297: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1298: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1299: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1300: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1301: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1302: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1303: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1304: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1305: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1306: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1307: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1308: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1309: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1310: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1311: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1312: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1313: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1314: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1315: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1316: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1317: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1318: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1319: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1320: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1321: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1322: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1323: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1324: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1325: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1326: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1327: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1328: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1329: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1330: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1331: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1332: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1333: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1334: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1335: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1336: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1337: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1338: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1339: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1340: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1341: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1342: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1343: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1344: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1345: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1346: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1347: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1348: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1349: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1350: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1351: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1352: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1353: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1354: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1355: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1356: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1357: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1358: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1359: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1360: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1361: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1362: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1363: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1364: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1365: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1366: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1367: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1368: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1369: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1370: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1371: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1372: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1373: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1374: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1375: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1376: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1377: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1378: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1379: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1380: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1381: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1382: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1383: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1384: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1385: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1386: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1387: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1388: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1389: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1390: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1391: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1392: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1393: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1394: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1395: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1396: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1397: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1398: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1399: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1400: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1401: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1402: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1403: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1404: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1405: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1406: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1407: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1408: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1409: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1410: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1411: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1412: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1413: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1414: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1415: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1416: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1417: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1418: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1419: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1420: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1421: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1422: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1423: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1424: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1425: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1426: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1427: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1428: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1429: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1430: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1431: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1432: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1433: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1434: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1435: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1436: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1437: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1438: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1439: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1440: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1441: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1442: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1443: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1444: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1445: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1446: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1447: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1448: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1449: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1450: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1451: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1452: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1453: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1454: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1455: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1456: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1457: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1458: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1459: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1460: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1461: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1462: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1463: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1464: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1465: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1466: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1467: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1468: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1469: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1470: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1471: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1472: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1473: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1474: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1475: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1476: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1477: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1478: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1479: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1480: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1481: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1482: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1483: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1484: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1485: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1486: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1487: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1488: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1489: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1490: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1491: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1492: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1493: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1494: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1495: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1496: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1497: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1498: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1499: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n",
      "Iteration 1500: Loss 162.21799989327386, Weight 36.98038182456281, Bias 9.592430178520884\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Weight: 36.98038182456281\n",
      "Estimated Bias: 9.592430178520884\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAFzCAYAAAA0dtAgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABBN0lEQVR4nO3deZxT1fnH8c9hAHWoWBl3ZQa11lbbAoJYqVtdUHGvWsFREawIqFWr1gWr2ErdWutWF6ooMoO7Vqu4VdDWDR0ErLY/d0DFBcG6FGSb5/fHyUgmk5v1JrlJvu/XK6+Z3CQ3J5kXPPec85znODNDREREKl+nUjdAREREikNBX0REpEoo6IuIiFQJBX0REZEqoaAvIiJSJRT0RUREqkTnUjeg0NZbbz3r1atXqZshIiJSFDNnzvzUzNZP9ljFB/1evXrR0tJS6maIiIgUhXNuXtBjGt4XERGpEgr6IiIiVUJBX0REpEoo6IuIiFQJBX0REZEqoaAvIiJSJRT0RUREqoSCvoiISJVQ0BcREakSCvoiIiKl0NwMvXpBp07+Z3Nzwd+y4svwioiIRE5zM4wcCUuW+Pvz5vn7AI2NBXtb9fRFRESKbexYWLKEp9iV99nUH1uyxB8vIAV9ERGRIls57wN+w2/ZnWmcz29XPzB/fkHfV8P7IiIiRTR/Phy5xnM8u2x7hjORazh59YP19QV9b/X0RUREiuS++6B3b3jF9WFK12OZyHF0IzavX1sL48cX9P0V9EVERAps6VIYPRoOPRS22gpmvdqFoRP3goYGcM7/nDChoEl8oOF9ERGRgnrtNRgyBF59Fc48Ey66CLp2BbZsLHiQT6SgLyIiUgBm8Je/wKmnwtprw6OPwt57l7ZNGt4XEREJ2X//C0ccASecADvtBHPmlD7gQ4mDvnNuonPuE+fcq3HHejjnnnDOvRn7uW7cY+c4595yzr3unIvA1yciItLe889Dnz5w//1w6aW+h7/RRqVulVfqnv6twD4Jx84GnjSzrYAnY/dxzm0DDAG2jb3mOudcTfGaKiIiEqy1FS6+GHbe2VfWfeYZ+PWv/e9RUdKmmNk/gMUJhw8CJsV+nwQcHHf8DjNbZmbvAm8BA4rRThERkVQ+/BAGDYJzz4XDDoNZs2CHHUrdqo4idP3xjQ3N7EOA2M8NYsc3Bd6Le977sWMdOOdGOudanHMtCxcuLGhjRUSkuj3yiF97/9xzcNNNcPvtsM46pW5VclEM+kFckmOW7IlmNsHM+ptZ//XXX7/AzRIRkWq0fDmcfjoMHgwbbwwzZ8Jxx/ll91EVxaD/sXNuY4DYz09ix98HesY9bzNgQZHbJiIiwltvwcCBcMUVcOKJMGMGfP/7pW5VelEM+g8Cw2K/DwMeiDs+xDm3hnNuc2Ar4MUStE9ERKpYUxP07QvvvOMz9K+9FtZcs9StykxJi/M4524HdgPWc869D1wAXALc5Zw7DpgPHA5gZq855+4C/g2sBE40s1UlabiIiFSdr77yvfrbbvMZ+s3N0LNn+tdFSUmDvpkNDXhoj4DnjwcKuxuBiIhIgpdf9qV0334bLrgAzjsPOpdhTdsoDu+LiIhEghlceSXsuCMsWQLTpsG4ceUZ8EG190VERJJauBCGD4eHH4YDD4SJE6GurtStyo96+iIiEr7mZujVy5ej69XL3y8j06f7tfdPPAHXXAN//Wv5B3xQ0BcRkbA1N8PIkTBvnh8fnzfP3y+DwL9ypZ+v32MP6N4dXnwRTjop2mvvs6GgLyIi4Ro71k+Ax1uyxB+PsHnzYNddYfx4P6w/c6bv7VcSzemLiEi45s/P7ngE3Hsv/OIXsGoVTJkCQ4PWlpU59fRFRCRc9fXZHS+hpUth1Ci/Sc5WW/mNcio14IOCvoiIhG38eKitbX+sttYfj5DXXoMBA+DGG/0WuM88A1tuWepWFZaCvoiIhKuxESZMgIYGnwHX0ODvNzaWumWAzy2cMAG23x4++QQefRQuvRS6di11ywpPc/oiIhK+xsbIBPl4//0vHH883HMP7LWXL6m70UalblXxqKcvIiLlJccaAM89B336+DX3l17qe/jVFPBBQV9ERMpJDjUAVq2C3/8edtnFXyc884yfw+9UhRGwCj+yiIgUTdiV+bKsAbBgAQwa5B8+/HCfnb/DDvk1oZxpTl9ERAqjrVfeFqTbeuWQ+3x/FjUApk6FYcP82998sy+4UymV9XKlnr6IiBRGISrzZVADYNky+NWvYL/9YJNNoKUFRoxQwAcFfRERKZRCVOZLUwPgzTdh4ED40598zfwZM+D738/97SqNgr6IiBRGISrzpagBMHkybLcdzJ3rM/SvuQbWXDP3t6pECvoiIlIYharM19joI3trK8ydy5cHNnLMMXDMMT7oz54NBx2U31tUKgV9EREpjCJU5ps5E/r18zmD48bBtGnQs2dop684yt4XEZHCKVBlPjO48ko46yzYcEOYPt2vw5fU1NMXEZGysnAh7L+/z9AfPNgP54cW8MOuK1Ds86ehnr6IiJSNadPgqKNg8WK49loYMybEpXiFqCtQzPNnwJlZUd6oVPr3728tLS2lboaIiORh5Uq44AK4+GLYemu44w7o3TvkN+nVywfiRA0NPnEw6uePcc7NNLP+yR7T8L6IiETavHmw666+fv6IEb7YTu/ehD9UXoi6AsU8fwYU9EVEJLLuvdfvjPevf8Htt8NNN0G3buS08U5ahagrUMzzZ0BBX0REImfpUhg1Cg47DL77XZ+sN2RI3BMKUeK3UHUFinX+DCjoi4hIpLz6Kmy/Pdx4o98C95lnYIstEp5UiKHyQtcVKELdgnSUyCciIpFg5mPgqafCOuvAbbf5bXGTKlJSXDkqu0Q+59zWzrnZcbcvnHOnOufGOec+iDs+uNRtFRGR/H32md/vftQov+Z+zpwUAR+Ch8oHDy7pOvioi+Q6fTN7HegD4JyrAT4A7geGA38ysz+UrnUiIhKm556DoUNhwQK47DI4/XQfs1NqGxIfO9YP6dfX+4A/aVJJ18FHXSR7+gn2AN42syTjOCIiUq5WrfId9l12gc6d4dln4cwzMwj4bRI23mHq1PCT+ypMOQT9IcDtcfdPcs694pyb6JxbN9kLnHMjnXMtzrmWhQsXFqeVIiLloMRlYNssWAB77QXnnQc//znMmgUDBuR50gisg4+6SAd951xX4EDg7tih64Et8UP/HwJ/TPY6M5tgZv3NrP/6669fjKaKiERfIda25+Dhh31xnRkzYOJE//bdu4dw4gisg4+6SAd9YF/gZTP7GMDMPjazVWbWCvwFyPe6UESkegStbR82rCiBf9kyOO00v1nOppv6bXGHDw+xdn4E1sFHXdSD/lDihvadcxvHPXYI8GrRWyQiUq6ChrlXrSp4j/+NN2DgQL8d7sknwwsvwPe+F/KbRGAdfNRFdp2+c64WeA/Ywsw+jx2bjB/aN2AucIKZfZjqPFqnLyISE7S2vU2B1rjfdpvfDW+NNeCWW+DAA0N/C4lTduv0AcxsiZnVtQX82LGjzeyHZvYjMzswXcAXEZE4yYa/44Wc8Pbll3D00X72oF8/v/ZeAb+0Ihv0RUQkZG3D3zU1yR8PMeFt5kzYbjuYMgUuvBCmTYPNNgvt9JIjBX0RkWrS2OgL2BQo4a21Fa64AnbcEb7+Gp56Cs4/P/g6Q4pLQV9EpNqEkfCWZL3/J5/AAQf4inr77eeH83feuVAfQnKhoC8iUo0Sq9klC/hBhXySrPd/8rgp9P7uUp58Eq69Fu67D3r0KEC7I1JcqFxFsva+iIiUWFtgT1bHPm69/wo6M45xXLzsHLZe9Q6PtnyH3r1L0CYty8uIevoiItJRUCGftg1ugLk0sCtP83vGMoKJtKzsTe+DehWu952qTZIRBX0RkXJVyKHuVHXs6+u5m8Pow2xeY1vu4Ahu4ni6saSwpX1VWz9vCvoiIuWo0HX0A5bvLdnsu5zQbTI/526+x/8xmz4cwV0JTypQ71u19fOmoC8iUo4KPdQ9fjx06dLu0Kud+zDAXmDCv3fmLC7hn+zM5sxN/vpC9L5VWz9vCvoiIuWoGEPdsZ1wDLiBE9h+5XN8+uUaPM4gLuEcurAy+LWF6H2rtn7eFPRFRMpRNkPducz9jx0Ly5fzGd/mcO5mNDewK08zZ+2d2avhjdSvLWTvO5OlhhJIQV9EpBxlOtSdau4/1cXA/Pk8y0D6MJsHOIjLOYOpDGbD92f6cyTuh9t2X73vSNM6fRGRctQWVNuW0NXX+4CfGGyD5v5POQWWLk265n3VkEYu7n4Z4z4/lQbm8RwD2Z6E3UrNfKA384E+2XtL5ER2a92waGtdEalqnTr5wJyhDzYdwNHfncH06XBkzZ1cv+p4uvNl8AsKtB2v5K4st9YVEZEQZJFQ9xD70fuDh5kxw+9733TrSro39Og4lB9Pa+TLioK+iEg5yjQ5L2juv67um7vL6Mqp/IkDeIieXT7m5Zfh2GPBHRWXNNfQkPz8WiNfVhT0RUTKTTaFeYKWuV11FdTW8gZbsSPPcxWn8kuu5vljb2TrrZO8Z6rEQW2CUz7MrKJv/fr1MxGRQE1NZg0NZs75n01NpW5Reg0NZj7ct781NGR1mkm732rd+NLqWGgPsr8/R21t8HeQ7LtqavKviW9HqnNIwQEtFhATlcgnItUrcdc28L3XTJacNTenz5wvlKDkPOf8UHwaX34JY8ZAUxPsylM008imLFj9hGyS83r18iMNiZTgVzJK5BMRSSbXUraFrnufTrrCPCmG21taoG9fmDIFLuQCnmSP9gEfskvO0yY4ZUVBX0SqV64Bq9RbvKaaXx8zBo4+usMFSevkZq64AgYOhOXL4amn4PyGSdSQZGQgm+Q8bYJTVhT0RaR65RqwSt27DUrOA7jhhg5D/58s6cb+Izfh9NNhv/1g9mzYeWfC2cBGm+CUFQV9EaleuQasKPRuk9WgHzu2Q8B/kt3pzRymfb0jf/4z3Hcf9OgRd458N7DRJjhlRUFfRKpXrgErqr3buJGGFXTmXMazF0+wLp/x4sYHM2adZtzmvdrP9YexgU0m59CyvmgISuuvlJuW7IlIQURxqV9sKd+7NNiPec7A7BdMsK/oZjZ6dOmW1mlZX1GRYsmeevoiIrmI4hav48dzd9dG+jCbf7MNd3AEf3En0G30MTB1au4rFfLtoZc68VG+oV32REQqwJIlcOrTjfxleSM7dJ3F7csPZfOGVhg/2V+QdAro46VKPkysYxC3E19WFzmlTnyUb6inLyJS5v71L9h+e7jpJjj7bPjnV33Z3N5ZXRynV6/gnfZSJR+G1UOPQuKjABEO+s65uc65fznnZjvnWmLHejjnnnDOvRn7uW6p2ykiUipmcP31MGAALF4Mjz0GF18MXbrEnhBfRCiZdMmHYfXQo5r4WIUiG/RjfmpmfWx1OcGzgSfNbCvgydh9EZGqs3gxHHaYr8Wz224wZw7stVfCk5L11NtkslIhrB66lvVFRmRr7zvn5gL9zezTuGOvA7uZ2YfOuY2Bp8ws2X5Q31DtfRGpNM88A0ceCR9+CJdcAqedFjBln2eN/rz2JpCSKdfa+wY87pyb6ZyLZY6woZl9CBD7uUHJWiciUmSrVsFFF8Guu0LXrvDcc3D66cE5enn31NVDrzhRDvo/MbPtgH2BE51zu2T6QufcSOdci3OuZeHChYVroYhIOiEVpfngA9hzT/jNb2DIEHj5ZZ+8l1IYc+lRXJooOYts0DezBbGfnwD3AwOAj2PD+sR+fhLw2glm1t/M+q+//vrFarKISHsh7cb30EPQuze89BLceqvfErd79wxeqJ66JIhk0HfOdXPOrd32OzAIeBV4EBgWe9ow4IHStFBEJAOZLnkLGA1YtgxOOQUOOAB69oSZM2HYMB+/M6aeusSJZNAHNgSecc7NAV4EHjazR4FLgL2cc28Ce8Xui4hEUyZL3gJGA16//EF+/GO4+mr45S/hhRdg6/i05ULUsld9/MoXVJ+3Um6qvS8iWQmzpn6sFn6HW0ND4HNawW7lGOvmvrK6OrMHHwxoY9i17FUfv2Kg2vsiIhkIaQ7+G5kk0sX1+r9gbY5mMscyif72EnPm+KH9DtJNG+TSY1d9/KqgoC8i0ibswJdJIl1s+VwL/diOl7mdofyW3/Bk/Qg23TTgvKmmDXK9cFF9/KoQ2eI8YVFxHhHJWL7FbHLQOrmZPx33KuesuJCN+IgpHMlOtbNSZ9n36pW8tG5Dg/8Z9FhbLf5sz5nqdRI55VqcR0SkuIq8Mcwnn8B+Uxo5Y8XF7L/WNGbTl50a3k+/rC7VtEGuPXbVx68KCvoiIm2KGPj+/ne/9n76dLjuOrj3f/vQwxZltqwu1bRBrhcuWtNfFRT0RUTahB34kiTUrVgB55wDgwbBuuv6gjujR2e59r6trcnW3+dz4aI1/RVPc/oiIoWQZLOad9f8Pkdu9jQvvLU+xx8PV17ZMT6H9t5jx/oh/fp6H/AVwKuG5vRFpPyVW+GYhJUAd3E4fb5+nv+8vQZ33ukHEBTwpdgU9EUk+sJeP18MscS5JazF8UzgCO5iG/7NbOvNz39eoPcsx+9JikpBX0SiL8z188UaMaiv51/8gP60cDPHcQ6/5x/sQq+GAk6pqsCOpNG51A0QEUkrrMIxifPsbT1hCHUI3Axu+OldnHbrj1iXz3icQezJk4VfAqcCO5KGevoiEn1hrZ8vQk948WI49FAYc+sAdu+9iDmb7c+eblpxlsAVuc6AlB8FfREpvXRD7mGtn0/XE25rh3PQuXP7nxlMBTzzDPTpAw89BH/8Izz08qZs8N7M4i2BU4EdSSdoJ55KuWmXPZGIy3R3tzB2v0u1612ydiTeAnadW7nS7MILzTp1MttyS7OXXsrhewhLmLsESlkixS57WqcvIqVVzJrvSdbOU1vrh93Hjk3ejjTtev99OOooePpp35G/7jro3j3cZotkQ+v0RSS6ipl8lqriXqbvF/e8v/3ND+e3tMCtt8LkyQr4Em0K+iJSWsVOPgsqNZvp+9XX8/XXcMopcOCB/mUvvwzDhuVQSlekyBT0RaS0Mkk+y3VtfTavS9aORLW1vH7i1ey4I1x9tQ/8zz8P3/1uZs3Juk0iYQua7K+UmxL5RMpAquSzTBP9kp0z29e1tQPMamra/Wytb7BbRj5n3bqZ1dWZ/e1vGbQ9jDaJZAkl8imRT6Rs5ZroF2KC4Bdf+J3wpkyB3XaDpibYdFNSJwYmW55XzKRFqVqpEvkU9EUk2jp18n3iRM75efmwX5fgpZdg6FAfk8eN89vi1tTEHsw2iIfUJpFUlL0vIuWpudkHymTSJd7lmSDY2gp/+AMMHAgrVvgleeedFxfwIfuVB0Hv3aOH5vmlKBT0RSSa2obOV63q+FiqKnNtiXLz5nVMp8+wOt3HH8PgwXDmmT5Df/Zs+MlPkjwx2wuLZMmCXbv6+QPtjCdFoKAvIuEJMzM9WZ188F3toDnz+K1lwQfRtsBfU7O6zn6Kdj3xBPTu7Xv2118P99wD664b8ORsy94mqxOw9tp+KCGedsaTAlHQF5FwhL2Xe9AQedvcd7KLi2QXCm2Bv23EIKBdK1bA2WfD3ntDXZ2fyx81Ks3a+7YgXle3+thaa6X+XIl1AhYvTv68efM03C+hU9AXkXCEvYNdqvnvoIuLoAuFxOS5+HY1N/Pupjuxc9cXuPRSOH63N3npJfjBD7Jo69Klq39ftCi7i51UOQYa7peQKeiLSDjCLqcbNHQOwRcX2VTxmz8fmpu5c8Rj9FnwMP/H97iLw7lxRh9q788iwOZ7sZNJUSAN90tIFPRFJBxhl9MNqpMfNBw+f35mATTmf5ttzfHHtTJk+W1sy2vMpg+Hc0/2ATbfi53Ez5nt+4hkIZJB3znX0zk33Tn3H+fca865U2LHxznnPnDOzY7dBpe6rSISU4i93JPVyU91cdEWQNutq+volTUH0P/L6dy8rJFzGc/T7Eov4tbbZxNgw7jYif+cDQ35nU9lfiWVoFJ9pbwBGwPbxX5fG3gD2AYYB5yRzblUhlekiIqxl3smpWyda/947NYKdm2P82yNLitto5qP7e/snvR537Q9k88SdmndfM6nMr9iqcvwljzAZ3IDHgD2UtAXKaF8A3qYFwTpztVWPz/utoh17eC1HjUw23dfs4/ZIHnAB7PRo7MLntnW30/33Fy/qySf+5uLGKkaZR30gV7AfKB7LOjPBV4BJgLrpnu9gr5IHuI3oEnsPWfTgyx2DzTh/f7BTtbTzbcuNSvtj0e22Kr6XsEBv66ucMGz0N9DwAiHORfO+aUslG3QB74FzAR+Fru/IVCDz0UYD0wMeN1IoAVoqa+vD/nrFKkSyQJUrkGwFD3QpiZbWb+5Xcj51omV9p0NP7eW301N/ZnaAnChgmehvwf19MVSB/1IJvIBOOe6APcCzWZ2H4CZfWxmq8ysFfgLMCDZa81sgpn1N7P+66+/fvEaLVJJTjkleUW8eJkmvGWT4Z5rIlrC695ftBZ7bP4OF3AhRx5Vw8tvdqffTaODP1Pb6oB0yYL5CHtZY6JCJFNKZQm6GijlDXDAbcCVCcc3jvv9NOCOdOfS8L5IDpqaUvfwC9XTz3X4O+F1D3CA9eBT67bGcps0Ke55mfbgU7UjmwS/xOcVoydejGRKiTTKbXgf2Akw/Nz97NhtMDAZ+Ffs+IPxFwFBNwV9kRwEBadCz+kHvW9dXUbtXcoadjJXGZj1Zaa9vslumZ0/WdBNFjwz/RxBz8s2QVAkB2UX9MO8KeiL5CCoRxwfJDPpfccHzdGj0/dAU71vqvdzzv6P71pvZhmYncoV9jVdc+/B19X5W2JbM71oSPU89cSlwBT0RSQ7ufa42+Q6TJ9qhCFgCLy11Wxi3RlWy1e2Hp/YQwxOHWSTXXykS1rMNsFPWfRSQqmCfmQT+USkhIISwq66KrPX51qPPlXCWZJkty++8Hl3IxZdzg6dWphDb/Zj6ur2Dh7ccXOeSZP8+7RV+QMYNix10mK62v6JxwuVCCiSJwV9EekoqO59sj3sk8k1S72xsf02tfESAuZLL0HfvnDXXXDRRfDErR+wSUPX9u2dOjX1xUfbdsBt2+6m+0yZZscri16iKmgIoFJuGt4XKYGgYfqamqwz8ROnBlatMrvsMrPOnc3q682efTbFudINs2eSsJg4vZBP9r5IEaA5fREpqlRz5JkuwUsSMD/6yGzvvf1pDj3UbPHiNK+rq0t98ZEuYVEZ9lKGUgV95x+vXP3797eWlpZSN0Ok+jQ3+7nyZEPnDQ2r59Mz9MQTcPTR8PnncOWVflS+3U60bUP18cP5Xbr4Jy1f3vGEtbWw1lqwaFHHx5yDHj38Nr719X5YPtOpDZESc87NNLP+yR7TnL6IhCOxkh74ZLlk5s1rfz9FFb4VK+Css2DQID/d/9JLcMIJSbaeT5Y8uGIFrL128q12256bbO598mT49NP2W/qKVAAFfZFyVeh90zM9f3MzrLceHHVU+yz5kSN9bzkZ51afr62Hnvja5mbeeQd22gkuu8wfeukl+MEPAtoblCS4eHHwxcfixfklLIqUm6Bx/0q5aU5fKlKhd2vLp/Jc4rr+oHnztsS4gGS629c7ybp3N1tnHbO77sqgzakK4mgjGqkiaE5fc/pSYXr16jhEDjnNled1/qDnZcI53wPv1MmH4Jj/UcsvuZqJHMeOO8KUKatnC1JKNqdfW+t77hD8mHr1UmE0py9SaQq9W1um58/n/drW3cetv3+FH9KfFm5hOOd2v5ann84w4EPq2gL51h0QqRAK+iLlqNAV3/KtPJdOfKGa8eOxtWr5M2MYwIt8zjr8fY39GX/dunTpkuV5Gxv9SESyBLxUj4lUCQV9kXKUbcW3xKS8MWNSJ+nlU3kulSS97MX7NnLINv/HSfyZPXiSOZvtz+43NyooixRC0GR/pdyUyCcVJ35f9pqa1QlpqSrDpUq2S5Wkl0vluaCCOEmS5v7xD7PNNjPr0sXsiit8tT0RyQ9K5FMin1SIVMlqQT3jTJPtwkoCzKCNq1b5evm//S1ssQXccQf065f/W4uIEvlEKkcuu9dlmmwXVhJgmqS599+H3XeHceP8oZdfVsAXKRYFfZFykkvWfqbJdm3Py7XoT/zrxo5tv31tLOA/8AD07g0zZ8Jtt/nb2mtndnoRyZ+Cvkg5ySVrP5Nku7YkvRTV8VJK87qvv4aTT4aDD/bXBbNm+Tr6oSt0lUKRchc02V8pNyXySUWIT95LrHCXy651o0cnT9LLtXJditf95z9mvXv7u6edZvb112nalmtVwUJXKRQpE+SSyOecmwqMMbO5xbwICZsS+aTsJUuMc86HtYaGcHeAS6iO1+79gurXB7zOgFsYwcm1N1NbC5MmweDBCa/LJTExSKGrFIqUiVwT+W4FHnfOjXXOZVsiQ0TCkix5ry3gh11kJteiPwmPf053jmQKx3EzO/T6iDlzkgR8yC0xMUihqxSKVIDAoG9mdwF9ge5Ai3PuDOfcr9puRWuhSLUrZjDLtuhPkte9yPb0ZRZ3czjjOZcn3t2KTaYHzK2H+dmyvWDR/L9UoXSJfCuA/wFrAGsn3ESkGApdcjdesuV2O+4Iw4b5+507+2p+SV7XesMELnNn8ROeZRU1/INdOJeLqVn6VXDPPczPls0FS64JiyLlLmiyH9gH+DdwCVAb9Lyo35TIJ2UlWVJbKRPURo9OnqA3enS7p330kdmgQf6hQ7nbFvPt9s93Lvn5w/5smSYFaqtdqWCkSORLFfT/CWwb9Hi53BT0pWSyzUpPFQDDynDPVluZ38RbTc03T3nsMbMNNjBbc02zG3qcY63ZBtNCf7Zk509cAZHu4kSkjKQK+irDK1IIYZbLLWX2uXOBDy1fZpx3Hlx+OWy7rS+l+4M5IWbjhyHo77DWWrBoUcfnK9NfKoDK8IqEJdPkrzDL5ZYy+7ymJunhdzp9h5139gH/hBPgxRfhBz8gevvWB/0dILeERZEyp6Avkqlskr/CLJcbf7yYGefNzb5HnOAOjqBv51d44w245x644YaE+Bm0b30psuWDvu/Fi6N1cSJSLEHj/pVy05y+hCab5K9cEsXSJbUVM6EvyXt9Ra2N4GYDs4EDzebOze98RUlGVMKeVCFySeSL6g2/quB14C3g7HTPV9CX0GST/JVrkAtKamtqCk6qK0QASwiWs+htW/Mfc6yysWPNVqzI73wd2l+o4K/SvFKFKiboAzXA28AWQFdgDrBNqtco6Etosu01FrKmfOItk3Nk05bYBU4r2DWcaF352jbmA3uS3XM7d9AFU66BOJvPU6qVDyIlUklBf0fgsbj75wDnpHqNgr6EJmpD1PEjDemCXrppg8Sg2NBgn9LDDuJ+A7PBPGSfsF7HC5xMv5N0nyGbEQv13kVSqqSgfxhwU9z9o4FrkzxvJNACtNTX14f4VUrVK0WvMV0vOV3ATDVCkSyAOmdPNxxtmzHfurDM/sQpq9fef+tbvj11df6WaXsyGa3IdI285ulFUkoV9Mtqnb5z7nBgbzP7Rez+0cAAMzs56DVapy9lL2j9frxUu+Cl2jmvvr7duVdSw0Wcx+/4DVvyNncwhO2YlX2bk7WnudkvoQv6LJmukc91J0CRKlFJ6/TfB3rG3d8MWFCitojkL5NlbMlqyidKVas+1VLAuCVt77EZuzONCxnHUTQxk365Bfyg92xbytfUlN8a+WLuRSBSYcot6L8EbOWc29w51xUYAjxY4jaJ5CbTdf/xBW+gY5W8dAEz1UY0sUD5AAfSh9nMoi+TOYpJHMvafJXb50rXnnwL+OS6E6CIlNecfmwqYjDwBj6Lf2y65yuRTyIr17npXPIKAl6zdOIUO5FrDcy2o8Xe4Dvp8wfS5RYUI89BGfkigaiURL5cbgr6ElmZrPtPF9xSretPExT//W+zH/3Iv+Wv+KN9Tdfcg72y50UiI1XQ71zCQQaR6paQRNfuOHTcLKZt+B/8UHjQ488+C5MmBb7ODCZOhF/+0o+KP/wwDP5sQxi7sZ/j79QJVq3K/HPU1cFVV6mErUgZKKvs/Vwoe18iK91OfOl23Qt6vKYmedBuaODzOXM54QS4807YfXeYPBk22SSDdqVS4f+HiJSbSsreF6kc6RLa0m3aE/R4QC99xryN6NvXb5Izfjw8/niSgB/Urk4p/qsoxsY5IhIK9fRFoiqknn4rjss5k/O4iE0bujBlCgwcmGVbElcMxIsfnRCRklNPX6QcpVuaFvT4yJHfHP+IDdmHRzmbSzl4wAJmz4aB7+awxW3bcsFklizxRXcyVYotdkXEC8rwq5SbsvelrOWRvf/oBkfbBnxka7qlduOIF6y11fLb/S9VGd2gnQYT26a6+SIFR6WU4c2Fhvel2ixfDuedB5dfDttu65P2tt029mC6KYNUmpth2LDgzP6GBj/6kGxlAfjRh7XWgkWLcnt/EclIquF9LdkTqSBvvw1Dh8JLL8GoUXDFFT7OfiNdcmAqbXP2QZn98UsDx47t+JwlS4JXBGTy/iKSN83pi+SikPPSOZ779tuhb194802foX/99QkBH/KvW59YEjhR2/x+tkFcdfNFikJBXyRbmdbMD+vcRx8NY8YEvuR//4MRI+DII+GHP4TZs+HQQwOeHEbd+raNc4Iy+ufPDw7idXWqmy9SQgr6ItkKGrpOl8GeSQ8+2bnN4IYbkj5/9mzo1w9uvdXP4z/9dOpE+7w3u4mXatQg6OLiqqvCe38RyV5Qhl+l3JS9L6HLpGZ+okyz1oPOnbART2ur2dVXm3XtarbJJmbTphXmo6aU7jNpUxyRkiBF9r56+iLZymVePGh04Kij2vf6U50jNk++aBEc3P89fvlLGLT8b8zp1JefLijBWvfEUYO6Op9EcPTR/jOBnwZobfU/1ZsXKTkFfalchUq2y2VePFViW3xOwPjxwXPlZjy90RH0bviMR1/egCs5hQc5kPXenx1eTkG22ub3J0+GpUv9FUnYeQ4iEp6gIYBKuWl4v0oVughMtkPXDQ3Bw/aJw/ejR3d4bAU1dj7jrBMrbStet5n0TTn8X3RBny+fNml6QCQnpBjeL3lQLvRNQb9KFSII5SNdRbvEnIC6um+Oz2cz25mnDcyGcYt9wbfSvz6fduYSaHPJc0jXDlXuE8lJqqCv4X2pTPkUoSmEdOvbof18/uLFAPyVg+jNHGbRl8kcxa0MZ22+Sv/6XOSzFDHf9f+Jcl0hISIpKehLZQo7CIWhbf67qSltTsDXPbfiRK7lEP7KFrzDy2zHUaQIvmGsdc8n0Iax/j9e1C7aRCqEgr5UprCDUJjSrJX/z39ggL3AdZzIr/gjzzGQrXgr+HxhrXXPt0RvmOvvo3jRJlIBFPSlMoUdhDIxZgx07uzfr3PnlFX0vun1xy1nM4ObbvLFdj76el2mnjmdPzZcQ1dWBJ/HufCWw4VRojesJXpRvmgTKWMK+lK5wgxC6YwZ44vdt+1At2qVv58q8Mf5/HO/Uc7xx8PAgTBnDux72U99u838GvhkggJyLssVoxRoS3HRJlIFFPRFwjBhQvLj11+fNvDOmOE3yrnnHvj97+Hxx2HjjROe1KdP8vMPHtzxWK4JeVELtMW8aBOpEs5n91eu/v37W0tLS6mbIZUuqKBOvNradkG0tdXveX/eebDppn6XvB13TPK65mZf5S7Zv9Vk+9D36uUDfSbPFZGK45ybaWb9kz6moC8Sgs6dVw/tpxILvB995OP43/8Ohx/urwW+/e2A1wQFcfAXG62t7Y916pT8AiHZc0Wk4qQK+hrel/KVbN66kPvcp3rv3XbL7HXz5/Poo/CjH8Gzz/pgf+edKQJ+7DWBks3pK/NdRAIo6Et5SjZvPWIEDB9emH3u073388/DHntATU3gy5bThTPXvp5994UNN4SWFp+4l3ZmIChYO+eT7BIvQAYPjk5CnohEioK+lKdkhWSWL4cVCcvbgorLZDIiEPScoCI2b70FK1cmjeJvswU78Qx/+OIERo+GF1+EbbbJ8LMmy6p3DkaN8r8nXoBMmgTDhkUnIU9EIqNzqRsgkpNsKrMlPretp94WuNtGBGB1YGxu9iMHy5evfs6IEanfu+14fX27OfgpDGUUN1DjWrn3HvjZzzJvers2jR3r36O+3l8INDb6i5FkFyBTpyppT0Q6iFwin3PucuAAYDnwNjDczP7rnOsF/Ad4PfbUF8xsVLrzKZGvQqVKbkuUmLWeSXb7euv5bWIT1dXBt76V+vWxi4qvljhO5hpuZTg/6fQcU674mPpTDsmszZlS0p6IJCi3RL4ngB+Y2Y+AN4Bz4h5728z6xG5pA75UsGRD3l27Qpcu7Y8lm8vOpNxssoDfdjxdEZvGRmaPvZv+necwiWH8Zp2reOqWueEHfFDSnohkJXJB38weN7OVsbsvAJuVsj0SUckKyUycCLfckn4uO4xyswFFbMzg6qthhwsH8+UGW/LktE789r+n0PmYI9Oft9yr6IlI5EV9Tn8EcGfc/c2dc7OAL4DzzOyfyV7knBsJjASoV4+ncjU2Jk9OC0pYa2728+Lz5vlgHT8snhgo6+qCe/sBwfjTT/20/9/+Bvvv768/1lsvw8+SSZ5BMqnm+0VEEpRkTt8593dgoyQPjTWzB2LPGQv0B35mZuacWwP4lpktcs71A/4KbGtmX6R6L83pC9AxqMLqwN/Q0DFQNjfDUUclP1e3bv48cf92nlpjbxpr7+PT/9Vy+eVw8smZFen7hqroiUhIyq4in3NuGDAK2MPMlgQ85yngDDNLGdEV9AXILahmELVXUsNvOZ+LOI/vdJ7LnS9uQd++ObRPCXkiEpKySuRzzu0DnAUcGB/wnXPrO+dqYr9vAWwFvFOaVkrZyWWv+IaG1KekJz9lOr/jfI7hNl5e2Tu3gA9KyBORoohc0AeuBdYGnnDOzXbO3RA7vgvwinNuDnAPMMrMFpeqkVJmcgmqyZLkYu7nYPowm9n0oYlGbmU432oI2P42E0rIE5EiiFzQN7PvmFnPxKV5ZnavmW1rZr3NbDsz+1up21q1ilXfPkzpgmqyz5QkS39pj005kWv5GfezBe8wi740MmV1SdxUUn1vUdvWVkQqk5lV9K1fv34mIWpqMqutNfMz0P5WW+uPR0FTk1lDg5lz/md8u4Iey/Azvfaa2Q97LjYwO53LbRld/HOdMxs9On27ovy9iUjFAFosICZGMpEvTErkC1mUs8yTZegn7GGfVJrPZAY33wy//KUvxjfp2Onse9fw7JbIRfl7E5GKUnbZ+2FS0A9ZlLPMcw2sKT7Tfxe3csIJcNddsOeecNttsPHGObQtjO+trc6A1uOLSApllb0vEVfqLPNU8+KZZOgne31A21/Y8CD69oV774WLL4bHHssx4EP+31uy7XwLsW2wiFQ0BX3JTimzzNMFvnSBNdnrhw/3pfTitOK4pMtv2OmTewF45hk4+2x/nZCzfL+3oO18k20bLCISJGiyv1JuSuQrgFTJcoXU0NA+Ea7t1tCwul2pkuWCXh93W8BGtmeX6QZmP/+52Wefhdj+fL4355K32bkQGygilQAl8mlOvyJkMi+eat476PUxj7I3x3AbX7m1uXrCWhx3XJaldAtJiYAikiHN6UtlyGRevLHRB8HWVv8zPtEt4PXL6cIZXM6+PMqGfEyL9ecXv4hQwAcV7xGRUCjoS/nIN/Alef1bbMlPeJY/cgZj+DMvMoBtGv4XUoNDpOI9IhKCqG+tK7JavtvItj3vlFNg0SKaOZJR3EBnVnIfh3AIf4127zloK2ERkQxpTl+qzlf123DSe79mEseyE/+kmUbqeQ9qamDSJAVWESlrqeb01dOXqjJrFgx5737eZCvO50J+w+/ozCr/YGurAr6IVDQFfakKZnD11fDrX8N6rjvTbHd24+n2T9I2tiJS4ZTIJ8VX5F36Pv0UDjwQTj0V9t72feZ07tcx4HfpEt25fBGRkCjoS3EVuZzsU09B797w+ONw1VXwwKKdWG/Fhx2f2L177kP75bjVsIhUJQV9Ka4ilZNduRLOPx92393vjPfCC36XPPdeQH3+xYtzeyPVxBeRMqKgL8WVyaY4IbzFbrvB734Hw4bBzJnQt2/swbA3DFJNfBEpIwr6UlwF3qXvvvv8cP4rr/jO9i23+J7+N8KubFeEixgRkbAo6EtxFaic7NKlMGYMHHoofOc7fmnekUcmeWLYle1KvdWwiEgWFPSluApQTva112DAALj+ejjjDHj2WdhyyzRtCKrPny3VxBeRMqJ1+lJ8IZWTNYObbvJVdb/1LXjkEdhnnxDal418SwOLiBSRgr6Upf/+1yfJ33037LknTJ4MG21UosaoJr6IlAkN70vZef556NMH7r8fLrkEHnushAFfRKSMKOhL2WhthYsvhp139ukA//wnnHWWr4kTKhXbEZEKpaAvxZNHMP3wQxg0CM4912foz5oFP/5x+O+jYjsiUsm0ta4Ux5gxcMMNPpC2qa3NKHP/kUd8kZ2vvoJrroERI3xPP6m2oB1fMCfD9wH8RcK8eR2P19X5Iv4iIhGXamtd9fQlM/n2nhMDPqStXLd8OZx+OgweDBtv7CvrHXdcioAP+VfICyqqs2iRevsiUvbU05f0CtV7Bh/BW1s7HH7rLRgyxAf6E0+EP/wB1lwzg7Z26tTx4iLF+2TV1oYGv65fRCTCyqqn75wb55z7wDk3O3YbHPfYOc65t5xzrzvn9i5lO6tKUO/5lFNgvfV8QHXO/56sN5yqJG2SynVNTb5W/jvv+LK6116bYcAPOF/K44lSFdVRaV0RKXORC/oxfzKzPrHbVADn3DbAEGBbYB/gOudcTSkbWTVSDXkvWtT+/vDhHQN/UMB1rl2Q/eorP3d/9NF+Sd7s2XDIIVm2Nd8KeY2Nfv4+GZXWFZEyF9Wgn8xBwB1mtszM3gXeAgaUuE3VIZtgt2JFx/nzZIHYORg16pvpgZdfhu22873888+H6dNzjLFhlPm96iqV1hWRihTVoH+Sc+4V59xE59y6sWObAu/FPef92LEOnHMjnXMtzrmWhQsXFrqtlS9Z0E4lcWQgWSCePBmuuw4zH2N33NHPGEybBhdeCJ3zqRWZb239AuwPICISBSVJ5HPO/R1IVkNtLPAC8ClgwO+Ajc1shHPuz8DzZtYUO8fNwFQzuzfVeymRLyTNze3ry3/1Vfuh/XgZJrwtXOhnAx5+GA44ACZO9GkBIiKSu1SJfCWpvW9me2byPOfcX4CHYnffB3rGPbwZsCDkpkmQxPryzc1+wfzy5e2f16VLRsPg06f70y1aBFdfDSedlGYpnoiI5C1yw/vOuY3j7h4CvBr7/UFgiHNuDefc5sBWwIvFbp/ENDb6RfPxNXC7dYNbbkk5DL5yJfzmN7DHHtC9O8yYASefHBfwm5szWxEgIiJZi1zQBy5zzv3LOfcK8FPgNAAzew24C/g38ChwopmtKl0zq1xzM0ya1H7te5qponnzYNdd4aKL4Nhj/Rr8Pn0Szjl8eMcVASNGtA/8qo0vIpITFeeR3AQVsQmYz7/vPj8wsGoV3HgjDB2axTnjz5tvoSARkQqXak5fQV9yk2Hlu6VL4Ve/8lV4t98ebr8dttwyy3PGnzfLiw0RkWpTVhX5pExkUPnutddgwAAf8M88E555JkXAT3XO+MeCCgWpWp6ISFoK+pLbHHmKyndmfrR9++3hk0/g0Ufhssuga9cMztmlS8fjXbuuXhGQT5ld5QKISLUzs4q+9evXzySFpiaz2lozP7Dub7W1/ngmr21oMHPO/2xqss8+Mzv8cH+avfYy+/DDHNpTV7e6LXV17duSa3vz+ZwiImUEaLGAmKg5/WoX4hz588/7BL0PPvAd8zPOaL+iLzSJhYLGj0+fxBfW58zlvUVEikiJfAr6wYIq4mS6FS0+I//SS33N/Pp6n6y3ww4htjEM+W65C1o5ICJlQYl8klxzc3DQz3C3mwULYNAg3/k97DCYNSuCAR/y33IXgrcYTtxgSEQkohT0q9kppwT3fjMopTt1KvTu7Yf1b7rJ9/DXWacA7QxDvlvuglYOiEjZU9DPVKVlfjc3B2+YY5ZyuHrZMr/2fr/9YJNNfGW9446LeO38MHbOC2O0QESkhBT0M9E2lztvng+I8+b5++Uc+FMNSTc0BD705pswcCD86U9w4om+dv73v1+A9hVCvlvuhjFaICJSQgr6majEudxUQ9IBQWzyZNhuO3j3Xbj/frj2WlhzzQK1L4rCGC0QESkhZe9nIozM76gJWsJWVwefftru0Jdf+l795Mmw885+gKNnz44vFRGR0lP2fr4qcS43aKj6qqvaHXr5ZejXzwf6Cy6AadMU8EVEypWCfiYqcS43zVC1GVx5Jfz4x34mY9o0GDcOOncuaatFRCQP+i88E21ztpVWia2xMelnWLjQ73c/dSoceCBMnOhH/UVEpLwp6GcqIEBWmunT/cdctAiuucbP5Ud6KZ6IiGRMw/sCwMqVcN55sMce0L07vPginHRSkQN+pdVCEBGJGAX9atfczLzNfsKuXZ5l/HgYvstbzJzpK+0Vux0VVwtBRCRiFPSrWXMz9454mD4fPMS/+CFTGMrNL/Wm219LEGgrsRaCiEjEKOhXqaVLYdQoOGz5FLbiTWbRl6HcUbpAq7r2IiIFp6BfhV57DbbfHm78qpFfcynPsBNb8s7qJ5Qi0FZiLQQRkYhR0C9nWSa+mcGNN0L//n5Z3qMbHMOlnE1XVrR/YikCbSXWQhARiRgF/XKVZeLbZ5/B4Yf7If2dd4Y5c2DvK/bOPNAWOrNede1FRArPzCr61q9fP6tIDQ1mPty3vzU0dHjqs8+a1debde5sdumlZqtWxT3Y1ORf45z/2dTU8b2amsxqa9u/T21t8ucGyeR9REQkb0CLBcREbbhTrjLYBGjVKrjkEl8zv74ebr8ddtghh/cK2pynocFvUZtO26hEfHZ+ba168iIiBaANdypRmsS3BQtgr718wZ3DD4dZs3IM+JB/Zr2W44mIRIKCfrlKkfj28MO+uM6MGXDzzTBlCqyzTh7vlW9mvZbjiYhEgoJ+uUqS+LbszzdxWksj++8Pm2wCLS0wYkQIpXTzzazXcjwRkUiIXNB3zt3pnJsdu811zs2OHe/lnFsa99gNJW5q6TU2+jn11lbefGIuA68ZypVX+pr5M2bA978f4vvkk1mv5XgiIpEQuV32zOyItt+dc38EPo97+G0z61P0RkXc5MkwZgx07Qp//SscdFAB3iSfXQYrdWtiEZEyE7mg38Y554CfA7uXui1R9eWXPtg3NcEuu/ifPXuWulUBqmRrYhGRKIvc8H6cnYGPzezNuGObO+dmOeeeds7tXKqGRcHMmbDddj5Jb9w4mDYtwgFfREQioSQ9fefc34GNkjw01sweiP0+FLg97rEPgXozW+Sc6wf81Tm3rZl9keT8I4GRAPUVlixmBldeCWedBRtuCNOn+16+iIhIOiUJ+ma2Z6rHnXOdgZ8B/eJeswxYFvt9pnPubeC7QIfKO2Y2AZgAvjhPeC0vrYUL4dhjYepUP29/881QV1fqVomISLmI6vD+nsD/mdn7bQecc+s752piv28BbAXxW8NVtmnT/Nr7J5+Ea6+F++9XwBcRkexENegPof3QPsAuwCvOuTnAPcAoM1tc9JYV2YoVPul9zz19gZ0ZM+DEE0NYey8iIlUnktn7ZnZskmP3AvcWvzWlM3cuHHkkPP88HHccXHUVdOtW6laJiEi5imTQF7jnHvjFL/zeObffDkOGlLpFIiJS7qI6vF+1liyBE07wm+RsvTXMnq2ALyIi4VDQj5BXX4UBA3yF21//Gp55BrbYotStEhGRSqGgHwFmcMMNsP328Omn8NhjcOml0KVLqVsmIiKVREG/xD77zA/ljx7ti+zMmQODBpW6VSIiUokU9Evo2WehTx944AG47DJ45BFfZU9ERKQQFPRLYNUquOgi2HVX6NzZB/8zz4RO+muIiEgBaclekS1YAEcd5WvmDx3q5/K7dy91q0REpBoo6BfRQw/52vlLl8LEif53VdYTEZFi0YByESxbBqeeCgccAJtt5rfFHT5cAV9ERIpLPf0Ce+MNX1xn1iw4+WSfsLfmmqVulYiIVCMF/QK67TYYMwbWWMNn6B94YKlbJCIi1UzD+wXw5Zdw9NEwbBj06+fX3ivgi4hIqSnoh2zmTNhuO5gyBS68EKZN8/P4IiIipaagH5LWVrjiCthxR/j6a3jqKTj/fKipKXXLREREPM3ph+CTT/zyu0cegYMPhptvhh49St0qERGR9tTTz9OTT0Lv3n4Y/9pr4b77FPBFRCSaFPRztGIFnHsu7LUXfPvbMGMGnHii1t6LiEh0KejnYO5cvyPexRfDiBHQ0uJ7+yXT3Ay9evni/b16+fsiIiIJNKefpbvvhuOPBzO44w444ogSN6i5GUaOhCVL/P158/x9gMbG0rVLREQiRz39DJnBqFHw85/D974Hs2dHIOADjB27OuC3WbLEHxcREYmjoJ8h56BbNzjrLPjnP2HzzUvdopj587M7LiIiVUvD+1n4wx8imKhXX++H9JMdFxERiaOefhYiF/ABxo+H2tr2x2pr/XEREZE4CvrlrrERJkyAhgZ/VdLQ4O8riU9ERBJoeL8SNDYqyIuISFrq6YuIiFQJBX0REZEqUZKg75w73Dn3mnOu1TnXP+Gxc5xzbznnXnfO7R13vJ9z7l+xx652LpJpdSIiIpFVqp7+q8DPgH/EH3TObQMMAbYF9gGuc861bU57PTAS2Cp226dorRUREakAJQn6ZvYfM3s9yUMHAXeY2TIzexd4CxjgnNsY6G5mz5uZAbcBBxevxSIiIuUvanP6mwLvxd1/P3Zs09jvicdFREQkQwVbsuec+zuwUZKHxprZA0EvS3LMUhwPeu+R+KkA6lWZTkREBChg0DezPXN42ftAz7j7mwELYsc3S3I86L0nABMA+vfvH3hxICIiUk2iNrz/IDDEObeGc25zfMLei2b2IfClc+7Hsaz9Y4Cg0QIRERFJolRL9g5xzr0P7Ag87Jx7DMDMXgPuAv4NPAqcaGarYi8bDdyET+57G3ik6A0XEREpY84nw1cu59xCIMk2dO2sB3xahOZIavo7RIP+DtGgv0M0lOPfocHM1k/2QMUH/Uw451rMrH/6Z0oh6e8QDfo7RIP+DtFQaX+HqM3pi4iISIEo6IuIiFQJBX1vQqkbIID+DlGhv0M06O8QDRX1d9CcvoiISJVQT19ERKRKVF3Q17a+0eOcG+ec+8A5Nzt2Gxz3WNK/iRSGc26f2Hf9lnPu7FK3p5o45+bG/p+Z7ZxriR3r4Zx7wjn3ZuznuqVuZ6Vxzk10zn3inHs17ljg917u/ydVXdBH2/pG1Z/MrE/sNhXS/k0kZLHv9s/AvsA2wNDY30CK56exfwNtHZKzgSfNbCvgydh9CdetdPw/Pen3Xgn/J1Vd0Ne2vmUl6d+kxG2qZAOAt8zsHTNbDtyB/xtI6RwETIr9Pgn93xM6M/sHsDjhcND3Xvb/J1Vd0E9B2/qW1knOuVdiQ21tQ2lBfxMpDH3fpWXA4865mbGdQgE2jO09QuznBiVrXXUJ+t7L/t9IwXbZK6VSbusryaX6m+CnT36H/15/B/wRGIG++2LT911aPzGzBc65DYAnnHP/V+oGSQdl/2+kIoN+Kbf1leQy/Zs45/4CPBS7G/Q3kcLQ911CZrYg9vMT59z9+GHjj51zG5vZh7Gpxk9K2sjqEfS9l/2/EQ3vr6ZtfUsk9o+qzSH4ZEsI+JsUu31V5CVgK+fc5s65rviEpQdL3Kaq4Jzr5pxbu+13YBD+38GDwLDY04ah/3uKJeh7L/v/kyqyp5+Kc+4Q4Bpgffy2vrPNbG8ze80517at70o6but7K7AWfktfbesbrsucc33ww2RzgRPAb7Wc4m8iITOzlc65k4DHgBpgYmy7aym8DYH7Y6uBOwNTzOxR59xLwF3OueOA+cDhJWxjRXLO3Q7sBqwX2/L9AuASknzvlfB/kiryiYiIVAkN74uIiFQJBX0REZEqoaAvIiJSJRT0RUREqoSCvoiISJVQ0BeR0Djnejrn3nXO9YjdXzd2v6HUbRMRBX0RCZGZvYcvq3xJ7NAlwAQzm1e6VolIG63TF5FQOee6ADOBicDxQN/Yrn0iUmJVV5FPRArLzFY4584EHgUGKeCLRIeG90WkEPYFPgR+UOqGiMhqCvoiEqrYPgp7AT8GTkvYUElESkhBX0RCE9uJ8nrgVDObD1wO/KG0rRKRNgr6IhKm44H5ZvZE7P51wPecc7uWsE0iEqPsfRERkSqhnr6IiEiVUNAXERGpEgr6IiIiVUJBX0REpEoo6IuIiFQJBX0REZEqoaAvIiJSJRT0RUREqsT/AwLJ67E0we+wAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "est_weight, est_bias = train_model(X_train, y_train, alpha, max_epoch)\n",
    "print(f\"Estimated Weight: {est_weight}\\nEstimated Bias: {est_bias}\")\n",
    "y_pred = (est_weight*X_test) + est_bias\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(y_test, y_pred, marker='o', color='red')\n",
    "\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_pred), max(y_pred)], color='blue', label=\"line1\")\n",
    "#plt.plot(y_test, y_test, color='orange', label=\"line2\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.59344190903729\n",
      "173.28609549341675\n",
      "0.8978521452417096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}