{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         f1        f2        f3        f4        f5   response\n0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075\n5 -0.699867  0.019159  1.103377 -0.671614 -0.119063 -18.597563\n6 -1.028250  0.962967  0.471027 -1.941219 -0.465591 -73.174734\n7  0.337585  1.352948 -1.789795 -0.885796 -0.846150 -25.865464\n8  0.295433 -0.907789  0.275980 -0.675526 -0.942592  -9.001596\n9  0.442269 -0.704559 -1.127342  1.030206  0.800113  57.076963",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.764216</td>\n      <td>-1.016209</td>\n      <td>0.149410</td>\n      <td>-0.050119</td>\n      <td>-0.578127</td>\n      <td>6.242514</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.763880</td>\n      <td>-1.159509</td>\n      <td>-0.721492</td>\n      <td>-0.654067</td>\n      <td>-0.431670</td>\n      <td>-8.118241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.519329</td>\n      <td>-0.664621</td>\n      <td>-1.694904</td>\n      <td>1.339779</td>\n      <td>0.182764</td>\n      <td>66.722455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.177388</td>\n      <td>0.515623</td>\n      <td>0.135144</td>\n      <td>-0.647634</td>\n      <td>-0.405631</td>\n      <td>-27.716793</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.104022</td>\n      <td>0.749665</td>\n      <td>-0.939338</td>\n      <td>-0.090725</td>\n      <td>-0.639963</td>\n      <td>8.192075</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.699867</td>\n      <td>0.019159</td>\n      <td>1.103377</td>\n      <td>-0.671614</td>\n      <td>-0.119063</td>\n      <td>-18.597563</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-1.028250</td>\n      <td>0.962967</td>\n      <td>0.471027</td>\n      <td>-1.941219</td>\n      <td>-0.465591</td>\n      <td>-73.174734</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.337585</td>\n      <td>1.352948</td>\n      <td>-1.789795</td>\n      <td>-0.885796</td>\n      <td>-0.846150</td>\n      <td>-25.865464</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.295433</td>\n      <td>-0.907789</td>\n      <td>0.275980</td>\n      <td>-0.675526</td>\n      <td>-0.942592</td>\n      <td>-9.001596</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.442269</td>\n      <td>-0.704559</td>\n      <td>-1.127342</td>\n      <td>1.030206</td>\n      <td>0.800113</td>\n      <td>57.076963</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset to be processed with pandas & displaying the top 10 result\n",
    "dt = pd.read_csv('assignment1_dataset.csv', sep=',')\n",
    "dt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                f1           f2           f3           f4           f5  \\\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \nmean      0.012255    -0.043030    -0.065785     0.039616     0.008074   \nstd       0.998816     1.042413     0.982640     1.023960     1.006679   \nmin      -3.174809    -3.381691    -3.158010    -2.764936    -2.946633   \n25%      -0.655282    -0.759477    -0.734505    -0.660802    -0.685371   \n50%      -0.001177    -0.038444    -0.049838    -0.006831    -0.000368   \n75%       0.697331     0.696343     0.591642     0.737806     0.710398   \nmax       3.092866     3.534175     3.406115     3.145835     3.007734   \n\n          response  \ncount  1000.000000  \nmean     11.229435  \nstd      40.028188  \nmin    -103.044475  \n25%     -16.580272  \n50%      10.554227  \n75%      38.485118  \nmax     157.890314  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.012255</td>\n      <td>-0.043030</td>\n      <td>-0.065785</td>\n      <td>0.039616</td>\n      <td>0.008074</td>\n      <td>11.229435</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.998816</td>\n      <td>1.042413</td>\n      <td>0.982640</td>\n      <td>1.023960</td>\n      <td>1.006679</td>\n      <td>40.028188</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-3.174809</td>\n      <td>-3.381691</td>\n      <td>-3.158010</td>\n      <td>-2.764936</td>\n      <td>-2.946633</td>\n      <td>-103.044475</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.655282</td>\n      <td>-0.759477</td>\n      <td>-0.734505</td>\n      <td>-0.660802</td>\n      <td>-0.685371</td>\n      <td>-16.580272</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.001177</td>\n      <td>-0.038444</td>\n      <td>-0.049838</td>\n      <td>-0.006831</td>\n      <td>-0.000368</td>\n      <td>10.554227</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.697331</td>\n      <td>0.696343</td>\n      <td>0.591642</td>\n      <td>0.737806</td>\n      <td>0.710398</td>\n      <td>38.485118</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.092866</td>\n      <td>3.534175</td>\n      <td>3.406115</td>\n      <td>3.145835</td>\n      <td>3.007734</td>\n      <td>157.890314</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying additional description\n",
    "dt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "f2         -0.031751\nf5         -0.028999\nf3          0.015218\nf1          0.308474\nf4          0.947255\nresponse    1.000000\nName: response, dtype: float64"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a correlation matrix between the columns/features and target in ascending order\n",
    "corr_matrix = dt.corr()\n",
    "corr_matrix['response'].sort_values(ascending=True)\n",
    "# Correlation between f4 and response are the closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'relationship between f4 & response')"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAwzUlEQVR4nO3de5xddX3v/9d7Jjuwg8KEH9GSAQQphkqRpKTUU/xZwUssXoggokdbz9Ee7Tn11+KvzTFYfyW0ehKbetT29LRia72AEBSMKNaoBfQcFCUxQUDJKZRLMgGJwiCQASaTz++PtfZkz5619l5z2ff38/GYR2avy97ftWdnffb39vkqIjAzMytioN0FMDOz7uGgYWZmhTlomJlZYQ4aZmZWmIOGmZkV5qBhZmaFOWgYkm6S9HuzPPc4SU9IGpzvclW9xjpJl9fZf6ekl83yuUPSL8+2bJ1O0hsk7Ur/RivaXR7rfg4aNiOS7pP0isrjiHggIp4VERPtKlNEnBIRN7X6dbsk4PwV8J70b7S9slHSSZKeqheM0+NOkPRtSY+nf/vfbXqJraM5aPQ4SQvaXQZrq+cBd2Zs/1vg1gLn/zfgPuBI4MXAj2fy4v789R4HjR6UfiN8n6QfAU9KWiDpxZK+K2lU0m15zTmSTpR0g6SfS/qZpCskDaX7PgccB3wlbe74r5KOT79xL0iPWSrpOkmPSLpb0n+qeu51kq6W9Nn0m+udklZW7X+fpJF0305JL68q2sI6503WftLX+KKkTemxP5R0WoO37BxJ/5Ze70ZJk/8vJL1D0k8kPSppi6Tnpdu/kx5yW/peXJh+Iz8/3f+S9H05J338Ckk7Gj1vuu9kSd9M38Odkt5Ute/Tkv5W0vXp9X1f0okZf8dDJD0BDKZlvKdq35uBUeBfGrwvAPuB3RExHhEPRcTWegdXfR7eKekB4IYG76MkfVTSw5Iek/QjSb9ada1/n74Xj6fvb/X79JuSbk3Pu1XSb1btu0nSX0i6OT33G5KOSvcdKuny9DM+mp773HTfEZL+UdKD6Wfxg2pi02tXigj/9NgPyTfDHcCxQBkYBn4OnEPyReGV6eMl6fE3Ab+X/v7L6f5DgCXAd4CP1Tz3K6oeHw8EsCB9/G3gfwKHAsuBvcDL033rgKfScgwC64Fb0n3LgF3A0qrnPbHRebVlSo8dB94IlIA/Ae4FSjnvVQA3knyTPg74P1XvxWrgbuBXgAXAB4Dv1pz7y1WP/xz4m/T39wP3AB+u2vfxRs8LHJa+D/8x3fdrwM+AU9L9nwYeAc5I918BXFXns1BbxsPTazw2fa8ub/BZ+n+Ap4FXF/zsVT4Pn02vpdzgelcB24AhQOkxR1dd6+PAS0k+jx8H/ne670jgUeB30ud8S/r4/6r6TN8DvCAtw03AhnTfu4GvAItIPk+nA4en+zYDn0jL/hzgB8C72/1/upN+2l4A/zThj5rcRN9R9fh9wOdqjtkCvD39/SbSG2XGc60Gttc8d2bQSG9EE8Czq/avBz6d/r4O+FbVvhcCY+nvvww8DLyCmht8vfNqy5QeWx1QBoAHgf875/qCqhsi8F+Af0l//2fgnTXPtQ94XtW51TfklwM/Sn//OvB7HAyK3wbOa/S8wIXA/6op4yeAS9LfPw38Q9W+c4C76nwWasv4ceB9Ve9VbtAAziQJuL8F7AZWpdtPIglkyjin8nl4ftW2etd7NkkQezEwUPNcn6YqIALPSj9fx5IEix/UHP894D9UfaY/UPN3/Xr6+zuA7wIvqjn/uSQBsly17S3Aja38/9vpP26e6l27qn5/HnBBWhUflTQKvAQ4uvYkSc+RdFVaNf8FcDlwVMHXXAo8EhGPV227n6SmU/FQ1e/7gEMlLYiIu4GLSG5kD6dlWNrovJxyTF57RBwgueEtzTl2yvFpeSvHPg/4eNV79gjJt+Fhsn0PeEHa1LGc5Nv2sWmzyBkktbZGz/s84Ddq/lZvBX6p6nVq34tn1bm2SZKWkwTljxY5HngPyZeNbwNvAD4naRXwmySBtV6209rPX+b1RsQNwP8g6WP5qaTLJB2e9TwR8UR67tL05/6a12z0Wau8T58j+dJ0laQ9kv5SUiktZwl4sKqsnyCpcVjKQaN3Vf+H3kXyn3+o6uewiNiQcd769NwXRcThwNtI/oNnPW+tPcCRkp5dte04YKRQgSM+HxEvIfnPG8CHi5yX4djKL2n/xDFp2RoeT1LeyrG7SJomqt+3ckR8N6f8+0iaWv4IuCMiniH5Rvv/AvdExM8KPO8u4Ns1+54VEf95xu/CdC8jqQk8IOkhkqa78yX9MOf4BSR9GkTErcCbgU0kgf2DDV6r9vOX+z5GxF9HxOnAKSTNSWuqzq3+Wz6LpFlqT/rzPKYq9FmLpH/m0oh4IUkAfC3wu2k5nwaOqirn4RFxSqPn7CcOGv3hcuB1klZJGkw7Al8m6ZiMY58NPAGMShpm6n9ggJ8Cz896kYjYRXKTXJ++xouAd5K0u9claZmksyUdQtJ/MUbSFDEbp0s6L62JXERyI7ilzvFrJC2WdCzJDX9Tuv3vgYslnZKW8QhJF1Sdl/VefJvkG/q308c31Txu9LxfJamt/I6kUvrz65J+pejF13EZcCJJLWh5Wo7rSfoVsnwB+ENJL02D74MkTYHPJflGXlTu9abX9hvpN/0nSf721X/3c5QMKlgI/AXw/fRz9jWS9+nfKxnocSFJs+VXGxVG0lmSTk07uH9B0gc2EREPAt8APiLpcEkDSgaG/NYMrrXnOWj0gfQ/2bkknbN7Sb5RrSH7738pSefrYyQ3lGtr9q8HPpBW3/8k4/y3kHyb3QN8iaQt/psFinkIsIGkrfwhkiaB9xc4L8uXSfoGKh2l50XEeIPjt5EMHrge+EeAiPgSSW3nqrSp7g7gt6vOWwd8Jn0vKiOcvk0SeL+T87ju86ZNe68i+Va/h+S9+DDJ+zMnEbEvkhFQD0XEQyRfDp6KiL05x18NrCUJNqPAlSRNW2uAr0o6ruDr1nsfDwc+SfK3up9kgMZfVZ3+eeASkmap00ma6oiIn5PUEP44Pee/Aq+tqs3V80vAF0kCxk9I/kaV+Sq/CywkGVr8aHrctGbcfqb6zZJm3UXSOpKO37e1uyw2N5I+TTLc9wPtLosd5JqGmZkV5qBhZmaFuXnKzMwKc03DzMwK6/lkYkcddVQcf/zx7S6GmVlX2bZt288iYknt9p4PGscffzxbt9bNsWZmZjUk1c64B9w8ZWZmM+CgYWZmhTlomJlZYQ4aZmZWmIOGmZkV1vOjp8zM+snm7SNs3LKTPaNjLB0qs2bVMlavyFsCZuYcNMzMesTm7SNcfO3tjI0n2eVHRse4+NrbAeYtcLh5ysysR2zcsnMyYFSMjU+wccvOeXsNBw0zsx6xZ3RsRttnw0HDzKxHLB0qz2j7bDhomJn1iDWrllEuDU7ZVi4NsmbVsnl7jbYGDUmfkvSwpDuqtq2TNCJpR/pzTtW+iyXdLWmnpLx1jc3M+tLqFcOsP+9UhofKCBgeKrP+vFN7avTUp4H/AXy2ZvtHI6J6nWAkvZBk3eRTgKXAtyS9ICImMDMzIAkc8xkkarW1phER3yFZML6Ic4GrIuLpiLgXuBs4o2mFMzOzaTq1T+M9kn6UNl8tTrcNA7uqjtmdbptG0rskbZW0de/evc0uq5lZ3+jEoPF3wInAcuBB4CPpdmUcm7lWbURcFhErI2LlkiXT1hAxM7NZ6rigERE/jYiJiDgAfJKDTVC7gWOrDj0G2NPq8pmZ9bOOCxqSjq56+AagMrLqOuDNkg6RdAJwEvCDVpfPzKyftXX0lKQrgZcBR0naDVwCvEzScpKmp/uAdwNExJ2SrgZ+DOwH/sAjp8zMWksRmd0CPWPlypXhNcLNzGZG0raIWFm7veOap8zMrHM5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYQ4aZmZWmIOGmZkV5qBhZmaFOWiYmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYW1dI9zMrBds3j7Cxi072TM6xtKhMmtWLWP1iuF2F6spHDTMzOZg8/YRLr72dsbGJwAYGR3j4mtvB+jJwOGgYWY2Bxu37JwMGBVj4xNs3LJzMmj0Uk3EQcPMek4rb9J7Rsfqbu+1mog7ws2sp1Ru0iOjYwQHb9Kbt4805fWWDpXrbq9XE+lGDhpm1lNadZPevH2EMzfcwMjoGKrZVy4NctbJSyb3Z8mroXQ6N0+ZWU9p1FxUbbbNWLVNTlG1b3iozFknL+GabSPTgle1vBpKp3NNw8x6SqPmooq5NGNl1WYABKxZtYwb79pbN2CUS4OsWbWs4et0IgcNM+spa1Yto1wanLIt6yY9l2asvNpMpM9br+lpeKjM+vNO7cpOcHDzlJn1mMrNuFGz00yasWodUS4xOjaee/7SoXJmX8bwUJmb157d8Pk7mWsaZtZzVq8Y5ua1Z/PRC5cD8N5NOzhzww1Tmp6KNmPV2rx9hCef2Z+7vxKkitR2ulFbg4akT0l6WNIdVduOlPRNSf+a/ru4at/Fku6WtFPSqvaU2sy6QaM+i9ne2Ddu2cn4RGTuq5y/esUw6887leGhMqL7m6SqKSL74lvy4tJLgSeAz0bEr6bb/hJ4JCI2SFoLLI6I90l6IXAlcAawFPgW8IKIyO9tAlauXBlbt25t6nWYWWfZvH2EP776NiYy7m/VTUSzGT11wtrrybtrfuzC5T0RGAAkbYuIlbXb29qnERHfkXR8zeZzgZelv38GuAl4X7r9qoh4GrhX0t0kAeR7LSmsmXWFSg0jK2DA1D6L1SuGZ3yTz+uvWLyo1JNpQ2p1Yp/GcyPiQYD03+ek24eBXVXH7U63TSPpXZK2Stq6d+/ephbWzDpL3nDYirnOj1izahmlwdrpfPDEU/vZvH2k5TPSW60Tg0ae6X8lsmuJEXFZRKyMiJVLlixpcrHMrJPUG/0kkpt4bad4PZWZ3yesvZ4zN9wAwGELpzfSjB8INm7Z2XNpQ2p14pDbn0o6OiIelHQ08HC6fTdwbNVxxwB7Wl46M+toec1HcPBbZtGkgVnJBt+7aUdun0a9gFW9r5ubrzqxpnEd8Pb097cDX67a/mZJh0g6ATgJ+EEbymdmHSxrVFRWM0WRb/9ZtYZ6Q4eWDpU5olzK3QetT6g439o95PZKko7sZZJ2S3onsAF4paR/BV6ZPiYi7gSuBn4MfB34g0Yjp8ysd9U2G1VuulnDXfNu9COjY3Vv1jNJKlhJUpg1h6M0oMmhvN3efNXu0VNvydn18pzjPwR8qHklMrNOk9WUAxRaoyKAhx57qu7z12umqtfUVW04LVfeHI5nHbpg8vnnMhO9E3Rin4aZGZC/gNEhCwYyv63/8dW3cdGmHYiDzUh5Q2+rz6teZa/amlXLprx+lup5Hxdt2pF5zOi+gylH8gJRt2S9ddAws45RW6vY98z+zOCQdxOvBIiZTlmujKiq7ZCu/J4XDIDJms/m7SNTglW16oCQFYi6KcWIg4aZdYSsWkUrjYyOseaLt7Huujt5bGx8SlPYoJRZYxkql6YkSMwKGJV06RVFEyp2KgcNM5t3MxlSWjl2JkFi8aISTzy9PzcH1GyNT8Rk9tpKECGym7jKpUHWvf6Uycf10qXXXvtsZqJ3CgcNM5tXef0QMP3muXn7CGu+cBvjB2Z2839033jmMNr5lheUBqVpCQjrpUPvJZ04T8PMuthMhpSuu+7OGQeMivalWoUDEdMCYC+nQ6/mmoaZzVi95qe8ZppKZ3P1OXkLGc2Xcmmw4bKr9fbnyRrp1O19FUW1NTV6Kzg1utn8qm1+guTmW2muOXPDDZnNNLUji2Z7w56pvBFNQ+US615/yuRNfmhRiSee2j+l5lMaFARTtlWuFXo7QOSlRnfQMLMZyQsKkLTfn3XyEq7ZNjIlIOTduFulNKCpwWBAbLzgtMw+lku/ciePpvMqhsolXnva0dx41966kwthauDsBXlBw30aZjYj9WYuj4yOcc22Ec4/fbhQGo/5UnmdLMNDZTZecNqU8lx4xrFs3LJzWgoSgKfGD0z+Pjo2zjXbRlizahn3bngNN689m9Urhrs+FchcuE/DzGakUWqNsfEJrvz+Lg5ETH4zzxtSmzf/YaYGpMznLw1qstmoeoGkvNFdecHgok072Lhl5+RzdXsqkLlw85SZzUhWn0Y95dIg558+nNtk1Yqmq8WLSkTAY2PjDOQEquGhMnvSzLN5Kk1QeUGwOqVIt3PzlJnNi+osskVUah6VJiuYGiha8bX10X3jjI6NE+TnohoZHWNA9Wd/VJqg+mV4bRY3T5kZMPuFgYrUFCYi2PSDXZPLpHZq+0aRprI9o2N9M7w2i4OGmc14Fnf1sUWbmMYPxKwn8rVavb6WyhyNbk4FMhdunjLrM1mLF81kNFDeanZDOSvWdarBOk1RExF87MLlfdsEVY+DhlkfyVpq9KJNO3JHQ2WNBsobITQ6Ns5hCwcz982XRaWByaGz9W76jZRLg3zkTafl9stUnrl2BcBemocxW26eMusjWbWEegKmrTNxRLmUm/7jyWeaN8O7NCj+23kvYvWKYTZvH6m7xgXAgCCrNawyE7xyPe/dtGNa01qQvFeVeRl2kGsaZn1kNvMIKinCl1/6DU5Yez2/eKq5+aKyDA+V2fjG0yYDRqW/pd7xR+Q0lx12yMGlV1evGM7ti+mHORez4ZqGWQ+rHRE1tKg0mSJjJqrXmWjl1K6hcokdl7xq8jreu2lH7jwLODgn5Ma79uZeZ20wGO7y5VdbzTUNsx6V1X/xxFP7J4e9drrKIke111FvWOyvHXcE12wbqTtjvTYY9POci9lwTcOsR9TWKp58evr62uMHAgELB8Uz87zq3Xw7tJR8p51JP8x373mk4Yzu2mDQz3MuZsNpRMx6wExTe3SaobRzvZnp04cdDGYkL42IaxpmHa7ITO2ZjorqNI8/tR+YPkFwbHxiXpIaLl5U6pmcUO3moGHWwYrO1O72kT71gsJExJxrHD3eoNJS7gg362BFZ2r38kifATHnWtRjTV5Wtp84aJh1sKLrNmSNAOoVeemqKjPCK/8OD5VZvCh7bkYvB9VWc/OUWQvNNJNs3oJH1TfB6txR7V5WtVXy1q3IW7/cw2fnj2saZi2SNW/i4mtvn7LUaK2zTl5Sd3v1c0ISMEoDYqA7pmLMWl4NrHqtD+eLao6OrWlIug94HJgA9kfESklHApuA44H7gDdFxKPtKqPZTNTrn8i6qW3ePsKV39+V+Vxfve1Bbrxrb2YtpFvSj89Fveamfk1Z3iqFahqSFkn6/yR9Mn18kqTXNrdoAJwVEcurxgqvBf4lIk4C/iV9bNYVZrKudKUGkTeqaHRsvO6s515WGpCbm9qoaPPUPwFPA/8ufbwb+GBTSlTfucBn0t8/A6xuQxnMZiXv23HW9rnOuyj1aMOzgI0XnOaaRBsV/WidGBF/CYwDRMQYB1PON0sA35C0TdK70m3PjYgH0zI8CDwn60RJ75K0VdLWvXv3NrmYZsXMJMfRXOddjB+Y0+lNNdcbhwNGexXt03hGUpl0YIakE0lqHs10ZkTskfQc4JuS7ip6YkRcBlwGSRqRZhXQbCaychyddfKSyeytlcc33rW37giobh0hNShxIIKhRSUikrkTS4fKPPrk0+wrGOU8dLb9igaNS4CvA8dKugI4E/gPzSoUQETsSf99WNKXgDOAn0o6OiIelHQ08HAzy2A236o7abNme19+ywMNn6MbAwYcnPX96L5xyqVBPnrhclavGOaEtdcXOt9DZztDoeapiPgmcB5JoLgSWBkRNzWrUJIOk/Tsyu/Aq4A7gOuAt6eHvR34crPKYNZs3Z4vqqisZVmrZ7Xn1R4WLyp56GwHKlTTkHQmsCMirpf0NuD9kj4eEfc3qVzPBb6k5MO2APh8RHxd0q3A1ZLeCTwAXNCk1zdrun4Y/VQv2WCl32bNqmWZE/Iued0pDhIdqGjz1N8Bp0k6DVgDfAr4LPBbzShURPwbcFrG9p8DL2/Ga5q10ubtI13bNzETExG511mpYVQCw7rr7pxcHfDQXh3+1QOK/mX2R7LwxrnAX0fEx4FnN69YZr1t45adPR8wKrKuM6t/4un9BzvDH9033nC2vLVH0ZrG45IuBt4GvFTSIJCdGczMGuaY6vZU5rMxVC5NjpiqfT9mOlve2qdo0LgQ+PfAOyPiIUnHARubVyyz7tVoDYzN20cYmIeFhbrNYYcsYMclr8rcN5PZ8tZehYJGRDwE/Peqxw+Q9GmYWY1Ga2DUSw/Sy+oFgCLZfK0zFM09dZ6kf5X0mKRfSHpc0i+aXTizTrJ5+whnbriBE9Zez5kbbshtb8+7OY6MjnHRph19Mcw2S70AMJPZ8tZeRZun/hJ4XUT8pJmFMetURZdd7aamp9IA7D/QmhFcIj/NO2TPlm+01oi1R9Gg8VMHDOtneU1Of3z1bcDBvopuanqaCPjohcsBuGjTjrrHlksDPLM/Gl5bJVVIuTQwJTVIANdsG2Hl847MDQROad4digaNrZI2AZupyjkVEdc2o1BmnSavyWkigjVfuI1Lv3Inj+7rrnWoD0TSv7L+vFMbHrv/QOOAAQdThWTlkvJoqN5QNGgcDuwjSedREYCDhvWFoUWl3KAwfiC6LmBUjI1PNKxlHLZwkCefmZ9+GI+G6n5FR0/9x2YXxKxTbd4+whNP7W93MdqiNMC8BQzwaKheUHT01DGSviTpYUk/lXSNpGOaXTizTrBxy86+WEI1S5GM5aVBsXhR47m+Hg3VG4o2T/0T8HkOJgh8W7rtlc0olFmrNJq5DW5SaWR8IohIgkL1YIHSoDhs4YLcWeDWnYoGjSUR8U9Vjz8t6aImlMesZfKG0W69/xFuvGvvZCA5olyaTKRn2R4bG+ejFy73kNk+UDRo/CxNiX5l+vgtwM+bUySz1sgbRnvFLQ9Mzl0YGR2jNChKA+rbJqoilg6VPWS2TxTNcvsO4E3AQ+nPG9NtZl0rr9mpNjSMTwSlwbmubN273FfRX4qOnnoAeH2Ty2I2K0X6JbLk5TvKUnQN637gvor+VnT01PMlfUXS3nQE1ZclPb/ZhTNrpNIvMTI6RnCwX6LIOgxZ+Y76tT4xoCR1eSPDQ2U2vvE0dlzyKu7d8BrWrFrGxi07G+bjst5RtHnq88DVwNHAUuALHOzfMGubRhllGzlkwdT/AuU+XTEuAta9/pRpQbSagJvXnj1Zq5hLwLbuVfR/iCLicxGxP/25nN5fqdK6wGzXYfjA5tu5aNOOaaOi+rUZqtKRvf68U1FOdeuImprIXAO2daeio6dulLQWuIokWFwIXC/pSICIeKRJ5TOrq+g6DNX9Hh5CO1V1R/bqFcO5ebRqg4kXTupPM1m5D+DdNdvfQRJE3L9hbbFm1bIpcy1g+miezdtHWPPF2xifSCrHDhhJU1OQ9FHUdmSP5uTRqt3uhZP6U9HRUyc0uyBms1FkHYY1X9hRKB1GvxiU+MibTssd8VQ0GBQJ2NZ7CgUNSRcAX4+IxyV9APg14C8iYntTS2eWqjestnZSWWWFvT2jYyxaONgXAWN4qMxZJy/hmm0jdVcGLJcGWX/eqXWHyBYNBl44qT8pCuTIl/SjiHiRpJcA64G/At4fEb/R7ALO1cqVK2Pr1q3tLobNQW26D8ieKwCw7ro7+7b5qVwa5PzTh6ekQDnr5CVTHhe9qc927ov1DknbImLltO0Fg8b2iFghaT1we0R8vrKtGYWdTw4a3e/MDTc0nIRXGhCIyX6LfjU8VObmtWe3uxjWA/KCRtEhtyOSPkGSSuRrkg6Zwblmc1JkNM74gej7gAEeuWTNV/TG/yZgC/DqiBgFjgTWNKtQZtU8Gqc4v1fWbIWCRkTsAx4GXpJu2g/8a7MKZVYtK91HvyqXBjlsYf574ZFL1mxFR09dAqwElpEsvlQCLgfObF7RzBK1o3SGFpV44qn9fZmqfGx8Ijc/Vrk04M5qa7qik/veAKwAfggQEXskPbtppapD0quBjwODwD9ExIZ2lMNaq3ZY7Vs/+T1uvqc/ExHkhcqn+mFssbVd0aDxTESEpACQdFgTy5RL0iDwtyTLzO4GbpV0XUT8uB3lsbnJG9a5efvIlKGzixeVuOR1p2Tu60eDEhMZox7dn2Gt0DBoSBLw1XT01JCk/0SSPuSTzS5chjOAuyPi39KyXQWcCzhodJl6S61u+sGuKU1Pj+4bZ80Xb2Pr/Y80nLzW68qlQX7tuCP47j2PTKlxeCa2tUrDjvBIJnKsBr4IXEPSr/FnEfE3zS1apmFgV9Xj3em2KSS9S9JWSVv37t3bssJZcXkZUq/8/q7MvorxieDyWx7o64ABcP7pw/zwgcemBAyl292fYa1QtHnqe8BoRLR7mG1WH+C0O0xEXAZcBsnkvmYXymYubz5BVrOLJYaHytx4195pgTOAG+/ylyNrjaLzNM4CvifpHkk/qvw0s2A5dgPHVj0+BtjThnLYHLn9fWYqzU9OR27tVjRo/DZwInA28Lqqn1a7FThJ0gmSFgJvBq5rQzlsjjz3orjFi0qTSQbzgq2DsLVK0dTo9ze7IEVExH5J7yGZnT4IfCoi7mxzsWwWqudeNMor1asqa1rkyUph7nTk1m5F+zQ6RkR8Dfhau8thc1e5GdbeBPuBgN888Uju+/kYI6Nj0wJIXgpzpyO3duu6oGHdp16a7axRVP0ggB8+8NhkYJhJKvLaiY5mrVQoNXo3c2r09spcC2NAPOvQBYzuG6/bPNMPnMrcOlVeanTXNKyp1l1357SaxPiB4NGcdaj7jUc9WbfxmhjWNJu3j/R1uo8iPOrJuo2DhjXNxi07212EjuZRT9aNHDSsadz0MlVpQCxeVEIkfRlZo6PMOp2DhjVNvze9DJVLDJVLk48nIunL8TBZ62YOGtY0/TzruzQgXnva0Ty9/+AaF5U8jJWMvpu3j7SpdGaz56Bh827z9hGWX/oNLtq0o+5Kc72gXBrkbS8+jsWLDtYohsolNl5wWmZywYqx8Qn3+VhX8pBbm1ebt4+w5gu3TUlv3stzMc4/fZgPrj6VD64+ddq+927aUfdc9/lYN3LQsFnJm8G8ccvOvlq7uzYlefX7MpCzwl5Fv/f5WHdy0LBCqm+GR5RLPPnMfsYnkhtipY0e+u/bc/X11s5+rxcwPNzWupWDRg+aSR6jIufU3gyzJuyNjU9w0aYduetX96rq2kJeHq3Ke1L5d9ijp6yLOWj0mKy1t9d84TYu/cqdjOYM98xbrxuYbHIqmlSwnwJGbW0hr5Z1IIL7NrymVcUyayqPnuoxWTf4Sq6nIHu4Z9563Zd+JVmqpN+anIoYlKZNzvMCSdYPHDR6TJEbfPVwz83bR3IXQXp03zjHr72eAfXyoNnZORAxrXkpa16K+y6s1zho9Jii32r3jI5NNks1ktfkNNhjsWQwDY6VfyspP7Jkvc+rVwyz/rxTGR4qO1WI9Sz3afSYrOVAsywdKs95AaSJHuq+yFspL2s9kHq1By+QZL3ONY0eU/ttd6hcolRTJRBw1slLeq6vYlCa/Ib/thcfN/ke1NOoRuDag9lUrmn0qCef3k+QDI9dWBM0Arhm2whDi0o9tRjSgQjuzRildOaGGzL7bYqumufag9lBDho9JiuNxzMZ7Uhj4xMcsmCA0qAmJ+l1uyAJEGedvIQb79o7OefkrJOXcM22kcJNTGaWz81TPWYmaTweGxvnsIW99b1hZHSMy295gJHRsckhxtdsG+H804fdxGQ2D3rrjmEz6qdYOlTuuX6NLGPjE9x4195CTVFmVp9rGj2m6JDbSvNMv0w864fgaNYKDho9Zs2qZZQGpo8ZGhCTI4kGJc4/Penc7faFkqpXxqun0t/hhY/M5sZBo8esXjHMxgtOm3IzPWzhIIPS5LoWExFcs22EzdtHJoeUDhaY9b14UamjPjDl0gDrXn9K4aDnFfPM5q6T7gE2T1avGGbHJa/ivg2v4b4Nr2Fo0cJpnePVqURWrxjmQJ1EgwNKAsbovnE6aRm+Q0uDk0GveuW8erxintncuCO8A80mtXm958gLB9VzF5YOlXNzUB0IJudztCKJbbk0yCELBjJTsFcbTctUmUfxgc23c8UtDzRcKdD9G2azp+jxVNYrV66MrVu3trsYhWWlrSgNiGcdumBaavO84JL1HHkWLypxyetOASh8znwrlwY48rBDplxHkfJkTc4rsnJe0Ul9Zv1M0raIWFm7veOapyStkzQiaUf6c07Vvosl3S1pp6RV7SxnsxRNbf6Bzbdz8bW3T5mPUGmvn0lOqUf3jU8mLayky5gv5VKxj5eAfc/sn7KtOn1H5Zipz52M/tq8fYQzN9zACWuv58wNNwBw89qzuXfDa/jIm05z1lmzedZxNQ1J64AnIuKvara/ELgSOANYCnwLeEFE1L07dltN44S11zdsXgFyV8gbTudezOavWllR7r2bdjQ8v8gKfYsXlXhq/MCMay9ZyQOzalUwvTZSe+58NPWZ9aO8mkY3BY2LASJiffp4C7AuIr5X7/m6JWhUbm55/QpFifr9E42UBpJgUG9SuYC3vvg4Nt26q24KEgEfvXD55HUJCgezIk1Ic80pZWb5uqZ5KvUeST+S9ClJi9Ntw8CuqmN2p9umkfQuSVslbd27d2+zyzpnlT6IuQYMYPLb9GznXowfqB8wABYMwOW3PNAwZ9XSoTKrVwxz89qzuW/Da/johcsLN38V6azOO8Yd3WbN05agIelbku7I+DkX+DvgRGA58CDwkcppGU+VedeKiMsiYmVErFyyZEkzLmFe1euDGCqXyJirl6vSN1B07sVsjB9ofExW30ElgBQJHEVmqnt5VbPWa0vQiIhXRMSvZvx8OSJ+GhETEXEA+CRJHwYkNYtjq57mGGBPq8veDHnfjAWse/0pDM4gajy6b5z3btrB1vsfqTv3olkBBZLJhE+NT3DRph2cePHX+MDmqasDNqoJlQbEvmf2T3Zu503G8/KqZq3Xcc1Tko6uevgG4I709+uAN0s6RNIJwEnAD1pdvmYYypmYNrSolGStnWHq8gCuuOWB3OddvKjEPevP4WMXLp/XFCLl0iBnnngkTz4zMWX2+eW3PDAlcGQtFFVZWnWoXAIxbbRYVuDwAklmrdeJHeGfI2maCuA+4N0R8WC670+BdwD7gYsi4p8bPV83dIQvv/QbmRPZhsolHhsbn9VIqMr5Tz6zPzPoVOZnbL3/ES6/5YFZvsJBw+m6FXnPNShxz/pzGo5mcue2WWfI6wjvuBnhEfE7dfZ9CPhQC4vTEo/lzHx+bGy84UioeiOSHhsb54hyKTMgVZqxFsyxrlkZ4gpMzvfIMhExbdJhpRYBTAYOd26bdbaOa57qR3kdtwMSZ528ZFoTUqU3YniozFtffFzu8x6R1lTyBMU6teHg+tvVTUnVzUGNJhRK2R3+tbmg3Llt1tk6rqbRj9asWpaZMqOSjfb804enLF9a26Rz7bbd7Mu4+zfK3VRU1mS7Wo1qAoLcGlP1uVnvhTu3zTqHg0abNUr7UWTVubGi1YVZqM5NdeaGG3IDV6NmtAORP4u8uhZReU7P4jbrTA4abVQ0sWCjb/FDi0qTWWjn2/Y/e1Whvoi82lK1rICRN5/DQcKsM7lPY57VJtCrt+BP0cSCA1Lu823ePsITT+3PObOxcmkwN7FgZRJekb6I2gSDRSxeVPIQWbMu46Axj6rTgTSaYwD5bfy1JiJyn2/jlp3TFlgqqtKRvf68F9WdJFd0RFNlxnfR+R+LFi5wwDDrMm6emkf1vpFn3RzrZYoVZK4HMZbOtN64ZSdrVi2b9VDUrHkPef0Ief0VeSOaavsl8kKah9GadR8HjXk00zkG9VKL37vhNZyw9vrc/ZVaR948jHpm2o8wmxFN1c+XN2HPw2jNuo+bp+bRTOcY5LX/V7Y3uqmOjU/w2FMzCxiz6UeYa7oO54gy6x2uacyjmX4jzzv+rJOXTH47b7QGRaMsMJXzh8olpGRd7UoH9kwDx2z7HzyM1qx3OGhkmO1qbzO9OWYdf9bJS6YsbjSXzGCDEh9502kADYfMVjRrpTsPozXrDR2XsHC+zTRhYdbciSIzoufLij//xrzNuRBJ30jRJIDtvnYz6xzdtnJf2xSZk9BM8zlJr9InUrSDvt3Xbmadz0GjRjdkWRVw5olH1p0LUd2XUrSDvhuu3czay0GjRjOyrM5klvhQOXvhpGoB3PfzsdyFjGpHNxUdveQMs2bWiDvCa8x3ltUieZuqrXv9Kaz5wm0NZ3mPjI7NqcM963hnmDWzRhw0asz38NCZzhKvff2sWeEVlc7tRoGosr16X6X2k3WNHhprZnkcNDLM5/DQRv0EeUNcK6+/efsI7920o+HQ23qBqFaj2o+DhJnlcZ9Gk+X1BwTJ8No1X7itboLD1SuGC8/VKNph7VFSZjZbDhpNtmbVMkqDytz36L7xaX0XWTfvounGi3ZYe5SUmc2Wg0aTrV4xzGELZ9YKWHvzzhr9VGsmHdYeJWVms+Wg0QKPzTALbe3NOyth4NtefJwTCJpZy7kjvAUarZ9dLe/mPZ8d1B4lZWaz5aDRAvXWzy4NisMWLuCxsfGW3rw9SsrMZsNBowWqv9mPjI5Nrtg3PE9BolmZac3MajlotEizvtnPdMa5mdlcuCO8y3nOhZm1kmsaM9CJzUCec2FmrdSWmoakCyTdKemApJU1+y6WdLeknZJWVW0/XdLt6b6/lpQ9Y65JKs1A9WZvt4PnXJhZK7WreeoO4DzgO9UbJb0QeDNwCvBq4H9Kqkwo+DvgXcBJ6c+rW1ZaOrcZyHMuzKyV2tI8FRE/AcioLJwLXBURTwP3SrobOEPSfcDhEfG99LzPAquBf25VmTu1GchzLsyslTqtT2MYuKXq8e5023j6e+32pqv0Y+QlDeyEZiDPuTCzVmla0JD0LeCXMnb9aUR8Oe+0jG1RZ3vea7+LpCmL4447rkFJ89UOZ63lZiAz6zdNCxoR8YpZnLYbOLbq8THAnnT7MRnb8177MuAygJUrVxbNLD5NVj9GxXxNzDMz6yadNk/jOuDNkg6RdAJJh/cPIuJB4HFJL05HTf0ukFdbmTd5/RUCbl57tgOGmfWddg25fYOk3cC/A66XtAUgIu4ErgZ+DHwd+IOIqHzV/8/APwB3A/fQgk5wD2c1M5uqXaOnvgR8KWffh4APZWzfCvxqk4s2RVaiQfdjmFk/67TRUx3Fw1nNzKZy0GjAw1nNzA7qtI5wMzPrYA4aZmZWmIOGmZkV5qBhZmaFOWiYmVlhiph1lo2uIGkvcH/68CjgZ20sznzqpWuB3rqeXroW6K3r6aVrgeZez/MiYkntxp4PGtUkbY2IlY2P7Hy9dC3QW9fTS9cCvXU9vXQt0J7rcfOUmZkV5qBhZmaF9VvQuKzdBZhHvXQt0FvX00vXAr11Pb10LdCG6+mrPg0zM5ubfqtpmJnZHDhomJlZYX0XNCT9haQfSdoh6RuSlra7TLMlaaOku9Lr+ZKkoXaXaS4kXSDpTkkHJHXlsEhJr5a0U9Ldkta2uzxzIelTkh6WdEe7yzJXko6VdKOkn6SfsT9qd5lmS9Khkn4g6bb0Wi5t6ev3W5+GpMMj4hfp738IvDAifr/NxZoVSa8CboiI/ZI+DBAR72tzsWZN0q8AB4BPAH+SLrzVNSQNAv8HeCXJuva3Am+JiB+3tWCzJOmlwBPAZyOipQugzTdJRwNHR8QPJT0b2Aas7sa/Tbrk9WER8YSkEvC/gT+KiFta8fp9V9OoBIzUYUDXRs2I+EZE7E8f3gIc087yzFVE/CQidra7HHNwBnB3RPxbRDwDXAWc2+YyzVpEfAd4pN3lmA8R8WBE/DD9/XHgJ0BXLpQTiSfSh6X0p2X3sb4LGgCSPiRpF/BW4M/aXZ558g5asG661TUM7Kp6vJsuvTH1MknHAyuA77e5KLMmaVDSDuBh4JsR0bJr6cmgIelbku7I+DkXICL+NCKOBa4A3tPe0tbX6FrSY/4U2E9yPR2tyPV0MWVs69qabC+S9CzgGuCimlaHrhIRExGxnKR14QxJLWs+7MnlXiPiFQUP/TxwPXBJE4szJ42uRdLbgdcCL48u6KCawd+mG+0Gjq16fAywp01lsRpp+/81wBURcW27yzMfImJU0k3Aq4GWDFjoyZpGPZJOqnr4euCudpVlriS9Gngf8PqI2Nfu8hi3AidJOkHSQuDNwHVtLpMx2Xn8j8BPIuK/t7s8cyFpSWWkpKQy8ApaeB/rx9FT1wDLSEbp3A/8fkSMtLdUsyPpbuAQ4Ofpplu6dSQYgKQ3AH8DLAFGgR0RsaqthZohSecAHwMGgU9FxIfaW6LZk3Ql8DKS9Ns/BS6JiH9sa6FmSdJLgP8F3E7yfx/g/RHxtfaVanYkvQj4DMlnbAC4OiL+vGWv329Bw8zMZq/vmqfMzGz2HDTMzKwwBw0zMyvMQcPMzApz0DAzs8IcNMxaRNIfpllWr0gf/7qkCUlvbHfZzIrqyRnhZh3qvwC/HRH3phlxPwxsaXOZzGbEQcOsBST9PfB84DpJnyLJSXUN8OttLZjZDDlomLVARPx+mvblLJJZ/J8HzsZBw7qM+zTMWu9jwPsiYqLdBTGbKdc0zFpvJXBVkkOPo4BzJO2PiM1tLZVZAQ4aZi0WESdUfpf0aeCrDhjWLdw8ZWZmhTnLrZmZFeaahpmZFeagYWZmhTlomJlZYQ4aZmZWmIOGmZkV5qBhZmaFOWiYmVlh/z8arwpfbRUBHAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's plot f4 & response, cuz f4 corr value is close to 1\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(dt.f4, dt.response)\n",
    "plt.xlabel('f4')\n",
    "plt.ylabel('response')\n",
    "plt.title('relationship between f4 & response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         f1        f2        f3        f4        f5   response\n0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.764216</td>\n      <td>-1.016209</td>\n      <td>0.149410</td>\n      <td>-0.050119</td>\n      <td>-0.578127</td>\n      <td>6.242514</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.763880</td>\n      <td>-1.159509</td>\n      <td>-0.721492</td>\n      <td>-0.654067</td>\n      <td>-0.431670</td>\n      <td>-8.118241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.519329</td>\n      <td>-0.664621</td>\n      <td>-1.694904</td>\n      <td>1.339779</td>\n      <td>0.182764</td>\n      <td>66.722455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.177388</td>\n      <td>0.515623</td>\n      <td>0.135144</td>\n      <td>-0.647634</td>\n      <td>-0.405631</td>\n      <td>-27.716793</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.104022</td>\n      <td>0.749665</td>\n      <td>-0.939338</td>\n      <td>-0.090725</td>\n      <td>-0.639963</td>\n      <td>8.192075</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine each column to be processed\n",
    "columns = ['f1','f2','f3','f4','f5','response']\n",
    "dt = dt.loc[:, columns]\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the training and test set with the ratio of 8:2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "features = ['f1','f2','f3','f4','f5'] # Data that we want to utilize as training & test\n",
    "#X_data = dt.loc[:, features] # X are the data we want to use from 'features' = independent variable\n",
    "#y_data = dt.loc[:, ['response']] # y is the data we want to use as target = dependent variable\n",
    "\n",
    "X_data = np.array(dt.iloc[:, 3])\n",
    "y_data = np.array(dt.iloc[:, -1])\n",
    "\n",
    "\"\"\"\n",
    "X = dt[['f1','f2','f3','f4','f5']]\n",
    "y = dt['response']\n",
    "X_data = np.array(X)\n",
    "y_data = np.array(y)\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=42, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.15 # Set learning rate to 0.1\n",
    "max_epoch = 1500 # Set max iteration to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) (800,)\n",
      "(200,) (200,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def loss_fn(y, yhat):\n",
    "    loss = np.sum((y-yhat)**2)/len(y)\n",
    "    return loss\n",
    "    #loss = mean_squared_error(y, yhat)\n",
    "    #return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def train_model(X, y, alpha, max_epoch):\n",
    "    w = b = 0\n",
    "    n = len(X)\n",
    "    losses = []\n",
    "    weights = []\n",
    "\n",
    "    def prediction(w, X):\n",
    "            yhat = (w * X) + b\n",
    "            return yhat;\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        y_predict = prediction(w, X)\n",
    "        loss = loss_fn(y, y_predict)\n",
    "\n",
    "        losses.append(loss)\n",
    "        weights.append(w)\n",
    "\n",
    "        loss_fn(y, y_predict)\n",
    "\n",
    "        wd = -(2/n)*sum(X*(y-y_predict))\n",
    "        bd = -(2/n)*sum(y-y_predict)\n",
    "\n",
    "        w = w - alpha * wd\n",
    "        b = b - alpha * bd\n",
    "\n",
    "        print(f\"Iteration {i+1}: Loss {loss}, Weight {w}, Bias {b}\");\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(weights, losses)\n",
    "    plt.scatter(weights, losses, marker='o', color='red')\n",
    "    plt.title(\"Loss vs Weights\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Weight\")\n",
    "    plt.show()\n",
    "\n",
    "    return w, b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss 1721.3069867699319, Weight 11.696102077957205, Bias 3.669599330511377\n",
      "Iteration 2: Loss 880.8238696125039, Weight 19.679364962567043, Bias 5.987867267407561\n",
      "Iteration 3: Loss 494.1386394965996, Weight 25.13239378048727, Bias 7.43970719426145\n",
      "Iteration 4: Loss 315.8860583169484, Weight 28.859946351231205, Bias 8.339228056640865\n",
      "Iteration 5: Loss 233.539957145164, Weight 31.40999672721068, Bias 8.889073633543482\n",
      "Iteration 6: Loss 195.4104133768699, Weight 33.15591093800564, Bias 9.219360663173202\n",
      "Iteration 7: Loss 177.7102979496032, Weight 34.35225495610511, Bias 9.413175880970186\n",
      "Iteration 8: Loss 169.47134217695225, Weight 35.17271580655756, Bias 9.523228916134661\n",
      "Iteration 9: Loss 165.62511199497047, Weight 35.73588308015578, Bias 9.582697304873543\n",
      "Iteration 10: Loss 163.82396516109745, Weight 36.12278728137225, Bias 9.612265933605121\n",
      "Iteration 11: Loss 162.97772420092355, Weight 36.38883785102429, Bias 9.624679096118454\n",
      "Iteration 12: Loss 162.57874964772344, Weight 36.57195428288922, Bias 9.627671301683716\n",
      "Iteration 13: Loss 162.3899632328638, Weight 36.69810794805331, Bias 9.625844727071108\n",
      "Iteration 14: Loss 162.30029677019212, Weight 36.78510173836004, Bias 9.6218647644765\n",
      "Iteration 15: Loss 162.25754323694943, Weight 36.845149682868595, Bias 9.61721597060746\n",
      "Iteration 16: Loss 162.23707720287345, Weight 36.88663882790536, Bias 9.612675993063268\n",
      "Iteration 17: Loss 162.22724073700977, Weight 36.91533351016493, Bias 9.6086095912174\n",
      "Iteration 18: Loss 162.2224939643714, Weight 36.93519913715687, Bias 9.605148663430505\n",
      "Iteration 19: Loss 162.22019407025476, Weight 36.94896614422218, Bias 9.60230062628026\n",
      "Iteration 20: Loss 162.21907527795017, Weight 36.958516395744944, Bias 9.600012203867918\n",
      "Iteration 21: Loss 162.21852890140693, Weight 36.96514815624741, Bias 9.59820580622594\n",
      "Iteration 22: Loss 162.21826105128002, Weight 36.96975794959221, Bias 9.5967993203102\n",
      "Iteration 23: Loss 162.21812925776024, Weight 36.972965486352784, Bias 9.595716069497401\n",
      "Iteration 24: Loss 162.21806417975338, Weight 36.9751995599702, Bias 9.594889110131854\n",
      "Iteration 25: Loss 162.21803193640054, Weight 36.97675716089835, Bias 9.594262399792084\n",
      "Iteration 26: Loss 162.21801591008477, Weight 36.97784419754151, Bias 9.593790349251291\n",
      "Iteration 27: Loss 162.21800792035035, Weight 36.97860357380796, Bias 9.59343663688193\n",
      "Iteration 28: Loss 162.21800392595296, Weight 36.97913456767773, Bias 9.593172777507164\n",
      "Iteration 29: Loss 162.21800192376546, Weight 36.97950621902726, Bias 9.59297670563868\n",
      "Iteration 30: Loss 162.2180009177433, Weight 36.97976658787136, Bias 9.592831497066317\n",
      "Iteration 31: Loss 162.21800041113045, Weight 36.97994916322101, Bias 9.592724275721697\n",
      "Iteration 32: Loss 162.2180001554898, Weight 36.980077303961906, Bias 9.592645311248285\n",
      "Iteration 33: Loss 162.21800002625147, Weight 36.98016731916892, Bias 9.592587292206774\n",
      "Iteration 34: Loss 162.21799996080486, Weight 36.980230606855535, Bias 9.592544751359304\n",
      "Iteration 35: Loss 162.21799992761171, Weight 36.98027514046877, Bias 9.592513617570821\n",
      "Iteration 36: Loss 162.21799991075352, Weight 36.98030650308951, Bias 9.59249087030935\n",
      "Iteration 37: Loss 162.21799990218094, Weight 36.98032860768009, Bias 9.592474275650583\n",
      "Iteration 38: Loss 162.2179998978167, Weight 36.98034419918664, Bias 9.592462186058253\n",
      "Iteration 39: Loss 162.21799989559273, Weight 36.980355204913394, Bias 9.592453389478745\n",
      "Iteration 40: Loss 162.21799989445836, Weight 36.98036297925867, Bias 9.592446996204677\n",
      "Iteration 41: Loss 162.21799989387932, Weight 36.980368474828076, Bias 9.592442354438806\n",
      "Iteration 42: Loss 162.21799989358348, Weight 36.98037236218677, Bias 9.592438987524677\n",
      "Iteration 43: Loss 162.2179998934323, Weight 36.98037511374591, Bias 9.592436547443791\n",
      "Iteration 44: Loss 162.21799989335497, Weight 36.980377062579294, Bias 9.59243478046734\n",
      "Iteration 45: Loss 162.2179998933154, Weight 36.980378443699834, Bias 9.59243350185296\n",
      "Iteration 46: Loss 162.21799989329517, Weight 36.98037942305219, Bias 9.592432577248612\n",
      "Iteration 47: Loss 162.21799989328477, Weight 36.98038011789493, Bias 9.592431909054447\n",
      "Iteration 48: Loss 162.21799989327945, Weight 36.98038061114148, Bias 9.592431426439688\n",
      "Iteration 49: Loss 162.21799989327673, Weight 36.98038096145877, Bias 9.592431078047344\n",
      "Iteration 50: Loss 162.21799989327533, Weight 36.98038121038424, Bias 9.59243082667127\n",
      "Iteration 51: Loss 162.21799989327462, Weight 36.98038138734534, Bias 9.592430645377714\n",
      "Iteration 52: Loss 162.21799989327425, Weight 36.98038151320244, Bias 9.592430514682912\n",
      "Iteration 53: Loss 162.21799989327408, Weight 36.9803816027513, Bias 9.592430420501541\n",
      "Iteration 54: Loss 162.217999893274, Weight 36.9803816664919, Bias 9.592430352657049\n",
      "Iteration 55: Loss 162.21799989327394, Weight 36.98038171187953, Bias 9.592430303801011\n",
      "Iteration 56: Loss 162.2179998932739, Weight 36.9803817442103, Bias 9.592430268629888\n",
      "Iteration 57: Loss 162.2179998932739, Weight 36.98038176724824, Bias 9.592430243317795\n",
      "Iteration 58: Loss 162.21799989327388, Weight 36.980381783669756, Bias 9.592430225106012\n",
      "Iteration 59: Loss 162.21799989327388, Weight 36.98038179537868, Bias 9.592430212006127\n",
      "Iteration 60: Loss 162.21799989327388, Weight 36.98038180372988, Bias 9.59243020258548\n",
      "Iteration 61: Loss 162.21799989327388, Weight 36.98038180968788, Bias 9.5924301958122\n",
      "Iteration 62: Loss 162.21799989327388, Weight 36.98038181393962, Bias 9.592430190943325\n",
      "Iteration 63: Loss 162.21799989327388, Weight 36.98038181697451, Bias 9.59243018744407\n",
      "Iteration 64: Loss 162.21799989327386, Weight 36.98038181914131, Bias 9.592430184929603\n",
      "Iteration 65: Loss 162.21799989327386, Weight 36.98038182068867, Bias 9.592430183123078\n",
      "Iteration 66: Loss 162.21799989327388, Weight 36.98038182179392, Bias 9.592430181825376\n",
      "Iteration 67: Loss 162.21799989327388, Weight 36.980381822583524, Bias 9.592430180893318\n",
      "Iteration 68: Loss 162.21799989327388, Weight 36.98038182314774, Bias 9.592430180223971\n",
      "Iteration 69: Loss 162.21799989327388, Weight 36.98038182355097, Bias 9.592430179743346\n",
      "Iteration 70: Loss 162.21799989327388, Weight 36.9803818238392, Bias 9.592430179398272\n",
      "Iteration 71: Loss 162.21799989327388, Weight 36.98038182404526, Bias 9.59243017915055\n",
      "Iteration 72: Loss 162.21799989327388, Weight 36.9803818241926, Bias 9.592430178972732\n",
      "Iteration 73: Loss 162.21799989327388, Weight 36.980381824297964, Bias 9.592430178845104\n",
      "Iteration 74: Loss 162.21799989327386, Weight 36.980381824373325, Bias 9.592430178753508\n",
      "Iteration 75: Loss 162.21799989327388, Weight 36.980381824427226, Bias 9.592430178687778\n",
      "Iteration 76: Loss 162.21799989327388, Weight 36.98038182446579, Bias 9.592430178640612\n",
      "Iteration 77: Loss 162.21799989327388, Weight 36.98038182449338, Bias 9.59243017860677\n",
      "Iteration 78: Loss 162.21799989327388, Weight 36.98038182451312, Bias 9.592430178582491\n",
      "Iteration 79: Loss 162.21799989327388, Weight 36.98038182452724, Bias 9.592430178565072\n",
      "Iteration 80: Loss 162.21799989327388, Weight 36.98038182453735, Bias 9.592430178552577\n",
      "Iteration 81: Loss 162.2179998932739, Weight 36.980381824544594, Bias 9.592430178543614\n",
      "Iteration 82: Loss 162.21799989327388, Weight 36.980381824549774, Bias 9.592430178537183\n",
      "Iteration 83: Loss 162.21799989327388, Weight 36.98038182455348, Bias 9.592430178532572\n",
      "Iteration 84: Loss 162.21799989327386, Weight 36.98038182455614, Bias 9.592430178529264\n",
      "Iteration 85: Loss 162.21799989327386, Weight 36.98038182455804, Bias 9.592430178526891\n",
      "Iteration 86: Loss 162.21799989327386, Weight 36.980381824559394, Bias 9.592430178525191\n",
      "Iteration 87: Loss 162.21799989327386, Weight 36.98038182456037, Bias 9.592430178523971\n",
      "Iteration 88: Loss 162.21799989327388, Weight 36.980381824561064, Bias 9.592430178523097\n",
      "Iteration 89: Loss 162.21799989327388, Weight 36.98038182456156, Bias 9.59243017852247\n",
      "Iteration 90: Loss 162.21799989327388, Weight 36.980381824561924, Bias 9.59243017852202\n",
      "Iteration 91: Loss 162.21799989327388, Weight 36.98038182456218, Bias 9.592430178521697\n",
      "Iteration 92: Loss 162.21799989327388, Weight 36.980381824562365, Bias 9.592430178521466\n",
      "Iteration 93: Loss 162.21799989327388, Weight 36.9803818245625, Bias 9.5924301785213\n",
      "Iteration 94: Loss 162.21799989327388, Weight 36.98038182456259, Bias 9.59243017852118\n",
      "Iteration 95: Loss 162.21799989327386, Weight 36.980381824562656, Bias 9.592430178521095\n",
      "Iteration 96: Loss 162.21799989327388, Weight 36.980381824562706, Bias 9.592430178521035\n",
      "Iteration 97: Loss 162.21799989327388, Weight 36.98038182456274, Bias 9.59243017852099\n",
      "Iteration 98: Loss 162.21799989327388, Weight 36.98038182456277, Bias 9.592430178520958\n",
      "Iteration 99: Loss 162.21799989327388, Weight 36.980381824562784, Bias 9.592430178520935\n",
      "Iteration 100: Loss 162.21799989327388, Weight 36.9803818245628, Bias 9.59243017852092\n",
      "Iteration 101: Loss 162.21799989327386, Weight 36.980381824562805, Bias 9.592430178520909\n",
      "Iteration 102: Loss 162.21799989327388, Weight 36.98038182456281, Bias 9.5924301785209\n",
      "Iteration 103: Loss 162.21799989327388, Weight 36.98038182456282, Bias 9.592430178520894\n",
      "Iteration 104: Loss 162.21799989327388, Weight 36.98038182456282, Bias 9.59243017852089\n",
      "Iteration 105: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520887\n",
      "Iteration 106: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520885\n",
      "Iteration 107: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520884\n",
      "Iteration 108: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 109: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 110: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 111: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 112: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 113: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 114: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 115: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 116: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 117: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 118: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 119: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 120: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 121: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 122: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 123: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 124: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 125: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 126: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 127: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 128: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 129: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 130: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 131: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 132: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 133: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 134: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 135: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 136: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 137: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 138: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 139: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 140: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 141: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 142: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 143: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 144: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 145: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 146: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 147: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 148: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 149: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 150: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 151: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 152: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 153: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 154: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 155: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 156: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 157: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 158: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 159: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 160: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 161: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 162: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 163: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 164: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 165: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 166: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 167: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 168: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 169: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 170: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 171: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 172: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 173: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 174: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 175: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 176: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 177: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 178: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 179: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 180: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 181: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 182: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 183: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 184: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 185: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 186: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 187: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 188: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 189: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 190: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 191: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 192: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 193: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 194: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 195: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 196: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 197: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 198: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 199: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 200: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 201: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 202: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 203: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 204: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 205: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 206: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 207: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 208: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 209: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 210: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 211: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 212: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 213: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 214: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 215: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 216: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 217: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 218: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 219: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 220: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 221: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 222: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 223: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 224: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 225: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 226: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 227: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 228: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 229: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 230: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 231: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 232: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 233: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 234: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 235: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 236: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 237: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 238: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 239: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 240: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 241: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 242: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 243: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 244: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 245: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 246: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 247: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 248: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 249: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 250: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 251: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 252: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 253: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 254: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 255: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 256: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 257: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 258: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 259: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 260: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 261: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 262: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 263: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 264: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 265: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 266: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 267: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 268: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 269: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 270: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 271: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 272: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 273: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 274: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 275: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 276: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 277: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 278: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 279: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 280: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 281: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 282: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 283: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 284: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 285: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 286: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 287: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 288: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 289: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 290: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 291: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 292: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 293: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 294: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 295: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 296: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 297: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 298: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 299: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 300: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 301: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 302: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 303: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 304: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 305: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 306: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 307: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 308: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 309: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 310: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 311: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 312: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 313: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 314: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 315: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 316: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 317: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 318: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 319: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 320: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 321: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 322: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 323: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 324: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 325: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 326: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 327: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 328: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 329: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 330: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 331: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 332: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 333: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 334: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 335: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 336: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 337: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 338: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 339: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 340: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 341: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 342: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 343: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 344: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 345: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 346: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 347: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 348: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 349: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 350: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 351: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 352: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 353: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 354: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 355: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 356: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 357: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 358: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 359: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 360: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 361: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 362: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 363: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 364: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 365: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 366: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 367: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 368: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 369: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 370: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 371: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 372: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 373: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 374: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 375: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 376: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 377: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 378: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 379: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 380: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 381: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 382: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 383: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 384: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 385: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 386: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 387: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 388: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 389: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 390: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 391: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 392: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 393: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 394: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 395: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 396: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 397: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 398: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 399: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 400: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 401: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 402: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 403: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 404: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 405: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 406: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 407: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 408: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 409: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 410: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 411: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 412: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 413: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 414: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 415: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 416: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 417: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 418: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 419: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 420: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 421: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 422: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 423: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 424: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 425: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 426: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 427: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 428: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 429: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 430: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 431: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 432: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 433: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 434: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 435: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 436: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 437: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 438: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 439: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 440: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 441: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 442: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 443: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 444: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 445: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 446: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 447: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 448: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 449: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 450: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 451: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 452: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 453: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 454: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 455: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 456: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 457: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 458: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 459: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 460: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 461: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 462: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 463: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 464: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 465: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 466: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 467: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 468: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 469: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 470: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 471: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 472: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 473: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 474: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 475: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 476: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 477: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 478: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 479: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 480: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 481: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 482: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 483: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 484: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 485: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 486: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 487: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 488: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 489: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 490: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 491: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 492: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 493: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 494: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 495: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 496: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 497: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 498: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 499: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 500: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 501: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 502: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 503: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 504: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 505: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 506: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 507: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 508: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 509: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 510: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 511: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 512: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 513: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 514: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 515: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 516: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 517: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 518: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 519: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 520: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 521: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 522: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 523: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 524: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 525: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 526: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 527: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 528: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 529: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 530: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 531: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 532: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 533: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 534: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 535: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 536: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 537: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 538: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 539: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 540: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 541: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 542: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 543: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 544: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 545: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 546: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 547: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 548: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 549: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 550: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 551: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 552: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 553: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 554: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 555: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 556: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 557: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 558: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 559: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 560: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 561: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 562: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 563: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 564: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 565: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 566: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 567: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 568: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 569: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 570: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 571: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 572: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 573: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 574: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 575: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 576: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 577: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 578: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 579: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 580: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 581: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 582: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 583: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 584: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 585: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 586: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 587: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 588: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 589: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 590: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 591: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 592: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 593: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 594: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 595: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 596: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 597: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 598: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 599: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 600: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 601: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 602: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 603: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 604: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 605: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 606: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 607: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 608: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 609: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 610: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 611: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 612: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 613: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 614: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 615: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 616: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 617: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 618: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 619: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 620: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 621: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 622: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 623: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 624: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 625: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 626: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 627: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 628: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 629: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 630: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 631: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 632: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 633: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 634: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 635: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 636: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 637: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 638: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 639: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 640: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 641: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 642: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 643: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 644: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 645: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 646: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 647: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 648: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 649: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 650: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 651: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 652: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 653: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 654: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 655: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 656: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 657: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 658: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 659: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 660: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 661: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 662: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 663: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 664: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 665: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 666: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 667: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 668: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 669: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 670: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 671: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 672: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 673: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 674: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 675: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 676: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 677: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 678: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 679: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 680: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 681: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 682: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 683: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 684: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 685: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 686: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 687: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 688: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 689: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 690: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 691: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 692: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 693: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 694: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 695: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 696: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 697: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 698: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 699: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 700: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 701: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 702: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 703: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 704: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 705: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 706: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 707: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 708: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 709: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 710: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 711: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 712: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 713: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 714: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 715: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 716: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 717: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 718: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 719: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 720: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 721: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 722: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 723: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 724: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 725: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 726: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 727: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 728: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 729: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 730: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 731: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 732: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 733: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 734: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 735: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 736: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 737: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 738: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 739: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 740: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 741: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 742: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 743: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 744: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 745: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 746: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 747: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 748: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 749: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 750: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 751: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 752: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 753: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 754: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 755: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 756: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 757: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 758: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 759: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 760: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 761: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 762: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 763: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 764: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 765: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 766: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 767: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 768: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 769: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 770: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 771: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 772: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 773: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 774: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 775: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 776: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 777: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 778: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 779: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 780: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 781: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 782: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 783: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 784: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 785: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 786: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 787: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 788: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 789: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 790: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 791: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 792: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 793: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 794: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 795: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 796: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 797: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 798: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 799: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 800: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 801: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 802: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 803: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 804: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 805: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 806: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 807: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 808: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 809: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 810: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 811: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 812: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 813: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 814: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 815: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 816: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 817: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 818: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 819: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 820: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 821: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 822: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 823: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 824: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 825: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 826: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 827: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 828: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 829: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 830: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 831: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 832: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 833: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 834: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 835: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 836: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 837: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 838: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 839: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 840: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 841: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 842: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 843: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 844: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 845: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 846: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 847: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 848: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 849: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 850: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 851: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 852: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 853: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 854: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 855: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 856: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 857: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 858: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 859: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 860: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 861: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 862: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 863: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 864: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 865: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 866: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 867: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 868: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 869: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 870: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 871: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 872: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 873: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 874: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 875: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 876: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 877: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 878: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 879: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 880: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 881: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 882: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 883: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 884: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 885: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 886: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 887: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 888: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 889: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 890: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 891: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 892: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 893: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 894: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 895: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 896: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 897: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 898: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 899: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 900: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 901: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 902: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 903: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 904: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 905: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 906: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 907: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 908: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 909: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 910: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 911: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 912: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 913: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 914: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 915: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 916: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 917: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 918: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 919: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 920: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 921: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 922: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 923: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 924: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 925: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 926: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 927: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 928: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 929: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 930: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 931: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 932: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 933: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 934: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 935: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 936: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 937: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 938: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 939: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 940: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 941: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 942: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 943: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 944: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 945: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 946: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 947: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 948: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 949: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 950: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 951: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 952: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 953: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 954: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 955: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 956: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 957: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 958: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 959: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 960: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 961: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 962: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 963: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 964: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 965: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 966: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 967: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 968: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 969: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 970: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 971: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 972: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 973: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 974: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 975: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 976: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 977: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 978: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 979: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 980: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 981: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 982: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 983: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 984: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 985: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 986: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 987: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 988: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 989: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 990: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 991: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 992: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 993: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 994: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 995: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 996: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 997: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 998: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 999: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1000: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1001: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1002: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1003: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1004: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1005: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1006: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1007: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1008: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1009: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1010: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1011: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1012: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1013: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1014: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1015: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1016: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1017: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1018: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1019: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1020: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1021: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1022: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1023: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1024: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1025: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1026: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1027: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1028: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1029: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1030: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1031: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1032: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1033: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1034: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1035: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1036: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1037: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1038: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1039: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1040: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1041: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1042: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1043: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1044: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1045: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1046: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1047: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1048: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1049: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1050: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1051: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1052: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1053: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1054: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1055: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1056: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1057: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1058: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1059: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1060: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1061: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1062: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1063: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1064: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1065: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1066: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1067: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1068: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1069: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1070: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1071: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1072: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1073: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1074: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1075: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1076: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1077: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1078: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1079: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1080: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1081: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1082: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1083: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1084: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1085: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1086: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1087: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1088: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1089: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1090: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1091: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1092: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1093: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1094: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1095: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1096: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1097: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1098: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1099: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1100: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1101: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1102: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1103: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1104: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1105: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1106: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1107: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1108: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1109: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1110: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1111: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1112: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1113: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1114: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1115: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1116: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1117: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1118: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1119: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1120: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1121: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1122: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1123: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1124: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1125: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1126: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1127: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1128: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1129: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1130: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1131: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1132: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1133: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1134: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1135: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1136: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1137: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1138: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1139: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1140: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1141: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1142: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1143: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1144: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1145: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1146: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1147: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1148: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1149: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1150: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1151: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1152: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1153: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1154: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1155: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1156: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1157: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1158: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1159: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1160: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1161: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1162: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1163: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1164: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1165: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1166: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1167: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1168: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1169: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1170: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1171: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1172: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1173: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1174: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1175: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1176: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1177: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1178: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1179: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1180: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1181: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1182: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1183: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1184: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1185: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1186: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1187: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1188: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1189: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1190: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1191: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1192: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1193: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1194: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1195: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1196: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1197: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1198: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1199: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1200: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1201: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1202: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1203: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1204: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1205: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1206: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1207: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1208: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1209: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1210: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1211: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1212: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1213: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1214: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1215: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1216: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1217: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1218: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1219: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1220: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1221: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1222: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1223: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1224: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1225: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1226: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1227: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1228: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1229: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1230: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1231: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1232: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1233: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1234: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1235: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1236: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1237: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1238: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1239: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1240: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1241: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1242: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1243: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1244: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1245: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1246: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1247: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1248: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1249: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1250: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1251: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1252: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1253: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1254: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1255: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1256: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1257: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1258: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1259: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1260: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1261: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1262: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1263: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1264: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1265: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1266: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1267: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1268: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1269: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1270: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1271: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1272: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1273: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1274: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1275: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1276: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1277: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1278: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1279: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1280: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1281: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1282: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1283: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1284: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1285: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1286: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1287: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1288: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1289: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1290: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1291: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1292: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1293: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1294: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1295: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1296: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1297: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1298: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1299: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1300: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1301: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1302: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1303: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1304: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1305: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1306: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1307: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1308: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1309: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1310: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1311: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1312: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1313: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1314: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1315: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1316: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1317: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1318: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1319: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1320: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1321: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1322: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1323: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1324: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1325: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1326: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1327: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1328: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1329: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1330: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1331: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1332: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1333: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1334: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1335: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1336: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1337: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1338: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1339: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1340: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1341: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1342: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1343: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1344: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1345: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1346: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1347: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1348: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1349: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1350: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1351: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1352: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1353: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1354: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1355: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1356: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1357: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1358: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1359: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1360: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1361: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1362: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1363: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1364: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1365: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1366: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1367: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1368: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1369: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1370: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1371: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1372: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1373: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1374: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1375: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1376: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1377: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1378: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1379: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1380: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1381: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1382: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1383: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1384: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1385: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1386: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1387: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1388: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1389: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1390: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1391: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1392: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1393: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1394: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1395: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1396: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1397: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1398: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1399: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1400: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1401: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1402: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1403: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1404: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1405: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1406: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1407: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1408: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1409: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1410: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1411: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1412: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1413: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1414: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1415: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1416: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1417: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1418: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1419: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1420: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1421: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1422: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1423: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1424: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1425: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1426: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1427: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1428: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1429: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1430: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1431: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1432: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1433: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1434: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1435: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1436: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1437: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1438: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1439: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1440: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1441: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1442: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1443: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1444: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1445: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1446: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1447: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1448: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1449: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1450: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1451: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1452: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1453: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1454: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1455: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1456: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1457: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1458: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1459: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1460: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1461: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1462: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1463: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1464: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1465: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1466: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1467: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1468: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1469: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1470: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1471: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1472: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1473: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1474: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1475: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1476: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1477: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1478: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1479: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1480: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1481: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1482: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1483: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1484: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1485: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1486: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1487: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1488: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1489: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1490: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1491: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1492: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1493: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1494: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1495: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1496: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1497: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1498: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1499: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1500: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Weight: 36.98038182456282\n",
      "Estimated Bias: 9.592430178520882\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAFzCAYAAAA0dtAgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABBN0lEQVR4nO3deZxT1fnH8c9hAHWoWBl3ZQa11lbbAoJYqVtdUHGvWsFREawIqFWr1gWr2ErdWutWF6ooMoO7Vqu4VdDWDR0ErLY/d0DFBcG6FGSb5/fHyUgmk5v1JrlJvu/XK6+Z3CQ3J5kXPPec85znODNDREREKl+nUjdAREREikNBX0REpEoo6IuIiFQJBX0REZEqoaAvIiJSJRT0RUREqkTnUjeg0NZbbz3r1atXqZshIiJSFDNnzvzUzNZP9ljFB/1evXrR0tJS6maIiIgUhXNuXtBjGt4XERGpEgr6IiIiVUJBX0REpEoo6IuIiFQJBX0REZEqoaAvIiJSJRT0RUREqoSCvoiISJVQ0BcREakSCvoiIiKl0NwMvXpBp07+Z3Nzwd+y4svwioiIRE5zM4wcCUuW+Pvz5vn7AI2NBXtb9fRFRESKbexYWLKEp9iV99nUH1uyxB8vIAV9ERGRIls57wN+w2/ZnWmcz29XPzB/fkHfV8P7IiIiRTR/Phy5xnM8u2x7hjORazh59YP19QV9b/X0RUREiuS++6B3b3jF9WFK12OZyHF0IzavX1sL48cX9P0V9EVERAps6VIYPRoOPRS22gpmvdqFoRP3goYGcM7/nDChoEl8oOF9ERGRgnrtNRgyBF59Fc48Ey66CLp2BbZsLHiQT6SgLyIiUgBm8Je/wKmnwtprw6OPwt57l7ZNGt4XEREJ2X//C0ccASecADvtBHPmlD7gQ4mDvnNuonPuE+fcq3HHejjnnnDOvRn7uW7cY+c4595yzr3unIvA1yciItLe889Dnz5w//1w6aW+h7/RRqVulVfqnv6twD4Jx84GnjSzrYAnY/dxzm0DDAG2jb3mOudcTfGaKiIiEqy1FS6+GHbe2VfWfeYZ+PWv/e9RUdKmmNk/gMUJhw8CJsV+nwQcHHf8DjNbZmbvAm8BA4rRThERkVQ+/BAGDYJzz4XDDoNZs2CHHUrdqo4idP3xjQ3N7EOA2M8NYsc3Bd6Le977sWMdOOdGOudanHMtCxcuLGhjRUSkuj3yiF97/9xzcNNNcPvtsM46pW5VclEM+kFckmOW7IlmNsHM+ptZ//XXX7/AzRIRkWq0fDmcfjoMHgwbbwwzZ8Jxx/ll91EVxaD/sXNuY4DYz09ix98HesY9bzNgQZHbJiIiwltvwcCBcMUVcOKJMGMGfP/7pW5VelEM+g8Cw2K/DwMeiDs+xDm3hnNuc2Ar4MUStE9ERKpYUxP07QvvvOMz9K+9FtZcs9StykxJi/M4524HdgPWc869D1wAXALc5Zw7DpgPHA5gZq855+4C/g2sBE40s1UlabiIiFSdr77yvfrbbvMZ+s3N0LNn+tdFSUmDvpkNDXhoj4DnjwcKuxuBiIhIgpdf9qV0334bLrgAzjsPOpdhTdsoDu+LiIhEghlceSXsuCMsWQLTpsG4ceUZ8EG190VERJJauBCGD4eHH4YDD4SJE6GurtStyo96+iIiEr7mZujVy5ej69XL3y8j06f7tfdPPAHXXAN//Wv5B3xQ0BcRkbA1N8PIkTBvnh8fnzfP3y+DwL9ypZ+v32MP6N4dXnwRTjop2mvvs6GgLyIi4Ro71k+Ax1uyxB+PsHnzYNddYfx4P6w/c6bv7VcSzemLiEi45s/P7ngE3Hsv/OIXsGoVTJkCQ4PWlpU59fRFRCRc9fXZHS+hpUth1Ci/Sc5WW/mNcio14IOCvoiIhG38eKitbX+sttYfj5DXXoMBA+DGG/0WuM88A1tuWepWFZaCvoiIhKuxESZMgIYGnwHX0ODvNzaWumWAzy2cMAG23x4++QQefRQuvRS6di11ywpPc/oiIhK+xsbIBPl4//0vHH883HMP7LWXL6m70UalblXxqKcvIiLlJccaAM89B336+DX3l17qe/jVFPBBQV9ERMpJDjUAVq2C3/8edtnFXyc884yfw+9UhRGwCj+yiIgUTdiV+bKsAbBgAQwa5B8+/HCfnb/DDvk1oZxpTl9ERAqjrVfeFqTbeuWQ+3x/FjUApk6FYcP82998sy+4UymV9XKlnr6IiBRGISrzZVADYNky+NWvYL/9YJNNoKUFRoxQwAcFfRERKZRCVOZLUwPgzTdh4ED40598zfwZM+D738/97SqNgr6IiBRGISrzpagBMHkybLcdzJ3rM/SvuQbWXDP3t6pECvoiIlIYharM19joI3trK8ydy5cHNnLMMXDMMT7oz54NBx2U31tUKgV9EREpjCJU5ps5E/r18zmD48bBtGnQs2dop684yt4XEZHCKVBlPjO48ko46yzYcEOYPt2vw5fU1NMXEZGysnAh7L+/z9AfPNgP54cW8MOuK1Ds86ehnr6IiJSNadPgqKNg8WK49loYMybEpXiFqCtQzPNnwJlZUd6oVPr3728tLS2lboaIiORh5Uq44AK4+GLYemu44w7o3TvkN+nVywfiRA0NPnEw6uePcc7NNLP+yR7T8L6IiETavHmw666+fv6IEb7YTu/ehD9UXoi6AsU8fwYU9EVEJLLuvdfvjPevf8Htt8NNN0G3buS08U5ahagrUMzzZ0BBX0REImfpUhg1Cg47DL77XZ+sN2RI3BMKUeK3UHUFinX+DCjoi4hIpLz6Kmy/Pdx4o98C95lnYIstEp5UiKHyQtcVKELdgnSUyCciIpFg5mPgqafCOuvAbbf5bXGTKlJSXDkqu0Q+59zWzrnZcbcvnHOnOufGOec+iDs+uNRtFRGR/H32md/vftQov+Z+zpwUAR+Ch8oHDy7pOvioi+Q6fTN7HegD4JyrAT4A7geGA38ysz+UrnUiIhKm556DoUNhwQK47DI4/XQfs1NqGxIfO9YP6dfX+4A/aVJJ18FHXSR7+gn2AN42syTjOCIiUq5WrfId9l12gc6d4dln4cwzMwj4bRI23mHq1PCT+ypMOQT9IcDtcfdPcs694pyb6JxbN9kLnHMjnXMtzrmWhQsXFqeVIiLloMRlYNssWAB77QXnnQc//znMmgUDBuR50gisg4+6SAd951xX4EDg7tih64Et8UP/HwJ/TPY6M5tgZv3NrP/6669fjKaKiERfIda25+Dhh31xnRkzYOJE//bdu4dw4gisg4+6SAd9YF/gZTP7GMDMPjazVWbWCvwFyPe6UESkegStbR82rCiBf9kyOO00v1nOppv6bXGHDw+xdn4E1sFHXdSD/lDihvadcxvHPXYI8GrRWyQiUq6ChrlXrSp4j/+NN2DgQL8d7sknwwsvwPe+F/KbRGAdfNRFdp2+c64WeA/Ywsw+jx2bjB/aN2AucIKZfZjqPFqnLyISE7S2vU2B1rjfdpvfDW+NNeCWW+DAA0N/C4lTduv0AcxsiZnVtQX82LGjzeyHZvYjMzswXcAXEZE4yYa/44Wc8Pbll3D00X72oF8/v/ZeAb+0Ihv0RUQkZG3D3zU1yR8PMeFt5kzYbjuYMgUuvBCmTYPNNgvt9JIjBX0RkWrS2OgL2BQo4a21Fa64AnbcEb7+Gp56Cs4/P/g6Q4pLQV9EpNqEkfCWZL3/J5/AAQf4inr77eeH83feuVAfQnKhoC8iUo0Sq9klC/hBhXySrPd/8rgp9P7uUp58Eq69Fu67D3r0KEC7I1JcqFxFsva+iIiUWFtgT1bHPm69/wo6M45xXLzsHLZe9Q6PtnyH3r1L0CYty8uIevoiItJRUCGftg1ugLk0sCtP83vGMoKJtKzsTe+DehWu952qTZIRBX0RkXJVyKHuVHXs6+u5m8Pow2xeY1vu4Ahu4ni6saSwpX1VWz9vCvoiIuWo0HX0A5bvLdnsu5zQbTI/526+x/8xmz4cwV0JTypQ71u19fOmoC8iUo4KPdQ9fjx06dLu0Kud+zDAXmDCv3fmLC7hn+zM5sxN/vpC9L5VWz9vCvoiIuWoGEPdsZ1wDLiBE9h+5XN8+uUaPM4gLuEcurAy+LWF6H2rtn7eFPRFRMpRNkPducz9jx0Ly5fzGd/mcO5mNDewK08zZ+2d2avhjdSvLWTvO5OlhhJIQV9EpBxlOtSdau4/1cXA/Pk8y0D6MJsHOIjLOYOpDGbD92f6cyTuh9t2X73vSNM6fRGRctQWVNuW0NXX+4CfGGyD5v5POQWWLk265n3VkEYu7n4Z4z4/lQbm8RwD2Z6E3UrNfKA384E+2XtL5ER2a92waGtdEalqnTr5wJyhDzYdwNHfncH06XBkzZ1cv+p4uvNl8AsKtB2v5K4st9YVEZEQZJFQ9xD70fuDh5kxw+9733TrSro39Og4lB9Pa+TLioK+iEg5yjQ5L2juv67um7vL6Mqp/IkDeIieXT7m5Zfh2GPBHRWXNNfQkPz8WiNfVhT0RUTKTTaFeYKWuV11FdTW8gZbsSPPcxWn8kuu5vljb2TrrZO8Z6rEQW2CUz7MrKJv/fr1MxGRQE1NZg0NZs75n01NpW5Reg0NZj7ct781NGR1mkm732rd+NLqWGgPsr8/R21t8HeQ7LtqavKviW9HqnNIwQEtFhATlcgnItUrcdc28L3XTJacNTenz5wvlKDkPOf8UHwaX34JY8ZAUxPsylM008imLFj9hGyS83r18iMNiZTgVzJK5BMRSSbXUraFrnufTrrCPCmG21taoG9fmDIFLuQCnmSP9gEfskvO0yY4ZUVBX0SqV64Bq9RbvKaaXx8zBo4+usMFSevkZq64AgYOhOXL4amn4PyGSdSQZGQgm+Q8bYJTVhT0RaR65RqwSt27DUrOA7jhhg5D/58s6cb+Izfh9NNhv/1g9mzYeWfC2cBGm+CUFQV9EaleuQasKPRuk9WgHzu2Q8B/kt3pzRymfb0jf/4z3Hcf9OgRd458N7DRJjhlRUFfRKpXrgErqr3buJGGFXTmXMazF0+wLp/x4sYHM2adZtzmvdrP9YexgU0m59CyvmgISuuvlJuW7IlIQURxqV9sKd+7NNiPec7A7BdMsK/oZjZ6dOmW1mlZX1GRYsmeevoiIrmI4hav48dzd9dG+jCbf7MNd3AEf3En0G30MTB1au4rFfLtoZc68VG+oV32REQqwJIlcOrTjfxleSM7dJ3F7csPZfOGVhg/2V+QdAro46VKPkysYxC3E19WFzmlTnyUb6inLyJS5v71L9h+e7jpJjj7bPjnV33Z3N5ZXRynV6/gnfZSJR+G1UOPQuKjABEO+s65uc65fznnZjvnWmLHejjnnnDOvRn7uW6p2ykiUipmcP31MGAALF4Mjz0GF18MXbrEnhBfRCiZdMmHYfXQo5r4WIUiG/RjfmpmfWx1OcGzgSfNbCvgydh9EZGqs3gxHHaYr8Wz224wZw7stVfCk5L11NtkslIhrB66lvVFRmRr7zvn5gL9zezTuGOvA7uZ2YfOuY2Bp8ws2X5Q31DtfRGpNM88A0ceCR9+CJdcAqedFjBln2eN/rz2JpCSKdfa+wY87pyb6ZyLZY6woZl9CBD7uUHJWiciUmSrVsFFF8Guu0LXrvDcc3D66cE5enn31NVDrzhRDvo/MbPtgH2BE51zu2T6QufcSOdci3OuZeHChYVroYhIOiEVpfngA9hzT/jNb2DIEHj5ZZ+8l1IYc+lRXJooOYts0DezBbGfnwD3AwOAj2PD+sR+fhLw2glm1t/M+q+//vrFarKISHsh7cb30EPQuze89BLceqvfErd79wxeqJ66JIhk0HfOdXPOrd32OzAIeBV4EBgWe9ow4IHStFBEJAOZLnkLGA1YtgxOOQUOOAB69oSZM2HYMB+/M6aeusSJZNAHNgSecc7NAV4EHjazR4FLgL2cc28Ce8Xui4hEUyZL3gJGA16//EF+/GO4+mr45S/hhRdg6/i05ULUsld9/MoXVJ+3Um6qvS8iWQmzpn6sFn6HW0ND4HNawW7lGOvmvrK6OrMHHwxoY9i17FUfv2Kg2vsiIhkIaQ7+G5kk0sX1+r9gbY5mMscyif72EnPm+KH9DtJNG+TSY1d9/KqgoC8i0ibswJdJIl1s+VwL/diOl7mdofyW3/Bk/Qg23TTgvKmmDXK9cFF9/KoQ2eI8YVFxHhHJWL7FbHLQOrmZPx33KuesuJCN+IgpHMlOtbNSZ9n36pW8tG5Dg/8Z9FhbLf5sz5nqdRI55VqcR0SkuIq8Mcwnn8B+Uxo5Y8XF7L/WNGbTl50a3k+/rC7VtEGuPXbVx68KCvoiIm2KGPj+/ne/9n76dLjuOrj3f/vQwxZltqwu1bRBrhcuWtNfFRT0RUTahB34kiTUrVgB55wDgwbBuuv6gjujR2e59r6trcnW3+dz4aI1/RVPc/oiIoWQZLOad9f8Pkdu9jQvvLU+xx8PV17ZMT6H9t5jx/oh/fp6H/AVwKuG5vRFpPyVW+GYhJUAd3E4fb5+nv+8vQZ33ukHEBTwpdgU9EUk+sJeP18MscS5JazF8UzgCO5iG/7NbOvNz39eoPcsx+9JikpBX0SiL8z188UaMaiv51/8gP60cDPHcQ6/5x/sQq+GAk6pqsCOpNG51A0QEUkrrMIxifPsbT1hCHUI3Axu+OldnHbrj1iXz3icQezJk4VfAqcCO5KGevoiEn1hrZ8vQk948WI49FAYc+sAdu+9iDmb7c+eblpxlsAVuc6AlB8FfREpvXRD7mGtn0/XE25rh3PQuXP7nxlMBTzzDPTpAw89BH/8Izz08qZs8N7M4i2BU4EdSSdoJ55KuWmXPZGIy3R3tzB2v0u1612ydiTeAnadW7nS7MILzTp1MttyS7OXXsrhewhLmLsESlkixS57WqcvIqVVzJrvSdbOU1vrh93Hjk3ejjTtev99OOooePpp35G/7jro3j3cZotkQ+v0RSS6ipl8lqriXqbvF/e8v/3ND+e3tMCtt8LkyQr4Em0K+iJSWsVOPgsqNZvp+9XX8/XXcMopcOCB/mUvvwzDhuVQSlekyBT0RaS0Mkk+y3VtfTavS9aORLW1vH7i1ey4I1x9tQ/8zz8P3/1uZs3Juk0iYQua7K+UmxL5RMpAquSzTBP9kp0z29e1tQPMamra/Wytb7BbRj5n3bqZ1dWZ/e1vGbQ9jDaJZAkl8imRT6Rs5ZroF2KC4Bdf+J3wpkyB3XaDpibYdFNSJwYmW55XzKRFqVqpEvkU9EUk2jp18n3iRM75efmwX5fgpZdg6FAfk8eN89vi1tTEHsw2iIfUJpFUlL0vIuWpudkHymTSJd7lmSDY2gp/+AMMHAgrVvgleeedFxfwIfuVB0Hv3aOH5vmlKBT0RSSa2obOV63q+FiqKnNtiXLz5nVMp8+wOt3HH8PgwXDmmT5Df/Zs+MlPkjwx2wuLZMmCXbv6+QPtjCdFoKAvIuEJMzM9WZ188F3toDnz+K1lwQfRtsBfU7O6zn6Kdj3xBPTu7Xv2118P99wD664b8ORsy94mqxOw9tp+KCGedsaTAlHQF5FwhL2Xe9AQedvcd7KLi2QXCm2Bv23EIKBdK1bA2WfD3ntDXZ2fyx81Ks3a+7YgXle3+thaa6X+XIl1AhYvTv68efM03C+hU9AXkXCEvYNdqvnvoIuLoAuFxOS5+HY1N/Pupjuxc9cXuPRSOH63N3npJfjBD7Jo69Klq39ftCi7i51UOQYa7peQKeiLSDjCLqcbNHQOwRcX2VTxmz8fmpu5c8Rj9FnwMP/H97iLw7lxRh9q788iwOZ7sZNJUSAN90tIFPRFJBxhl9MNqpMfNBw+f35mATTmf5ttzfHHtTJk+W1sy2vMpg+Hc0/2ATbfi53Ez5nt+4hkIZJB3znX0zk33Tn3H+fca865U2LHxznnPnDOzY7dBpe6rSISU4i93JPVyU91cdEWQNutq+volTUH0P/L6dy8rJFzGc/T7Eov4tbbZxNgw7jYif+cDQ35nU9lfiWVoFJ9pbwBGwPbxX5fG3gD2AYYB5yRzblUhlekiIqxl3smpWyda/947NYKdm2P82yNLitto5qP7e/snvR537Q9k88SdmndfM6nMr9iqcvwljzAZ3IDHgD2UtAXKaF8A3qYFwTpztVWPz/utoh17eC1HjUw23dfs4/ZIHnAB7PRo7MLntnW30/33Fy/qySf+5uLGKkaZR30gV7AfKB7LOjPBV4BJgLrpnu9gr5IHuI3oEnsPWfTgyx2DzTh/f7BTtbTzbcuNSvtj0e22Kr6XsEBv66ucMGz0N9DwAiHORfO+aUslG3QB74FzAR+Fru/IVCDz0UYD0wMeN1IoAVoqa+vD/nrFKkSyQJUrkGwFD3QpiZbWb+5Xcj51omV9p0NP7eW301N/ZnaAnChgmehvwf19MVSB/1IJvIBOOe6APcCzWZ2H4CZfWxmq8ysFfgLMCDZa81sgpn1N7P+66+/fvEaLVJJTjkleUW8eJkmvGWT4Z5rIlrC695ftBZ7bP4OF3AhRx5Vw8tvdqffTaODP1Pb6oB0yYL5CHtZY6JCJFNKZQm6GijlDXDAbcCVCcc3jvv9NOCOdOfS8L5IDpqaUvfwC9XTz3X4O+F1D3CA9eBT67bGcps0Ke55mfbgU7UjmwS/xOcVoydejGRKiTTKbXgf2Akw/Nz97NhtMDAZ+Ffs+IPxFwFBNwV9kRwEBadCz+kHvW9dXUbtXcoadjJXGZj1Zaa9vslumZ0/WdBNFjwz/RxBz8s2QVAkB2UX9MO8KeiL5CCoRxwfJDPpfccHzdGj0/dAU71vqvdzzv6P71pvZhmYncoV9jVdc+/B19X5W2JbM71oSPU89cSlwBT0RSQ7ufa42+Q6TJ9qhCFgCLy11Wxi3RlWy1e2Hp/YQwxOHWSTXXykS1rMNsFPWfRSQqmCfmQT+USkhIISwq66KrPX51qPPlXCWZJkty++8Hl3IxZdzg6dWphDb/Zj6ur2Dh7ccXOeSZP8+7RV+QMYNix10mK62v6JxwuVCCiSJwV9EekoqO59sj3sk8k1S72xsf02tfESAuZLL0HfvnDXXXDRRfDErR+wSUPX9u2dOjX1xUfbdsBt2+6m+0yZZscri16iKmgIoFJuGt4XKYGgYfqamqwz8ROnBlatMrvsMrPOnc3q682efTbFudINs2eSsJg4vZBP9r5IEaA5fREpqlRz5JkuwUsSMD/6yGzvvf1pDj3UbPHiNK+rq0t98ZEuYVEZ9lKGUgV95x+vXP3797eWlpZSN0Ok+jQ3+7nyZEPnDQ2r59Mz9MQTcPTR8PnncOWVflS+3U60bUP18cP5Xbr4Jy1f3vGEtbWw1lqwaFHHx5yDHj38Nr719X5YPtOpDZESc87NNLP+yR7TnL6IhCOxkh74ZLlk5s1rfz9FFb4VK+Css2DQID/d/9JLcMIJSbaeT5Y8uGIFrL128q12256bbO598mT49NP2W/qKVAAFfZFyVeh90zM9f3MzrLceHHVU+yz5kSN9bzkZ51afr62Hnvja5mbeeQd22gkuu8wfeukl+MEPAtoblCS4eHHwxcfixfklLIqUm6Bx/0q5aU5fKlKhd2vLp/Jc4rr+oHnztsS4gGS629c7ybp3N1tnHbO77sqgzakK4mgjGqkiaE5fc/pSYXr16jhEDjnNled1/qDnZcI53wPv1MmH4Jj/UcsvuZqJHMeOO8KUKatnC1JKNqdfW+t77hD8mHr1UmE0py9SaQq9W1um58/n/drW3cetv3+FH9KfFm5hOOd2v5ann84w4EPq2gL51h0QqRAK+iLlqNAV3/KtPJdOfKGa8eOxtWr5M2MYwIt8zjr8fY39GX/dunTpkuV5Gxv9SESyBLxUj4lUCQV9kXKUbcW3xKS8MWNSJ+nlU3kulSS97MX7NnLINv/HSfyZPXiSOZvtz+43NyooixRC0GR/pdyUyCcVJ35f9pqa1QlpqSrDpUq2S5Wkl0vluaCCOEmS5v7xD7PNNjPr0sXsiit8tT0RyQ9K5FMin1SIVMlqQT3jTJPtwkoCzKCNq1b5evm//S1ssQXccQf065f/W4uIEvlEKkcuu9dlmmwXVhJgmqS599+H3XeHceP8oZdfVsAXKRYFfZFykkvWfqbJdm3Py7XoT/zrxo5tv31tLOA/8AD07g0zZ8Jtt/nb2mtndnoRyZ+Cvkg5ySVrP5Nku7YkvRTV8VJK87qvv4aTT4aDD/bXBbNm+Tr6oSt0lUKRchc02V8pNyXySUWIT95LrHCXy651o0cnT9LLtXJditf95z9mvXv7u6edZvb112nalmtVwUJXKRQpE+SSyOecmwqMMbO5xbwICZsS+aTsJUuMc86HtYaGcHeAS6iO1+79gurXB7zOgFsYwcm1N1NbC5MmweDBCa/LJTExSKGrFIqUiVwT+W4FHnfOjXXOZVsiQ0TCkix5ry3gh11kJteiPwmPf053jmQKx3EzO/T6iDlzkgR8yC0xMUihqxSKVIDAoG9mdwF9ge5Ai3PuDOfcr9puRWuhSLUrZjDLtuhPkte9yPb0ZRZ3czjjOZcn3t2KTaYHzK2H+dmyvWDR/L9UoXSJfCuA/wFrAGsn3ESkGApdcjdesuV2O+4Iw4b5+507+2p+SV7XesMELnNn8ROeZRU1/INdOJeLqVn6VXDPPczPls0FS64JiyLlLmiyH9gH+DdwCVAb9Lyo35TIJ2UlWVJbKRPURo9OnqA3enS7p330kdmgQf6hQ7nbFvPt9s93Lvn5w/5smSYFaqtdqWCkSORLFfT/CWwb9Hi53BT0pWSyzUpPFQDDynDPVluZ38RbTc03T3nsMbMNNjBbc02zG3qcY63ZBtNCf7Zk509cAZHu4kSkjKQK+irDK1IIYZbLLWX2uXOBDy1fZpx3Hlx+OWy7rS+l+4M5IWbjhyHo77DWWrBoUcfnK9NfKoDK8IqEJdPkrzDL5ZYy+7ymJunhdzp9h5139gH/hBPgxRfhBz8gevvWB/0dILeERZEyp6Avkqlskr/CLJcbf7yYGefNzb5HnOAOjqBv51d44w245x644YaE+Bm0b30psuWDvu/Fi6N1cSJSLEHj/pVy05y+hCab5K9cEsXSJbUVM6EvyXt9Ra2N4GYDs4EDzebOze98RUlGVMKeVCFySeSL6g2/quB14C3g7HTPV9CX0GST/JVrkAtKamtqCk6qK0QASwiWs+htW/Mfc6yysWPNVqzI73wd2l+o4K/SvFKFKiboAzXA28AWQFdgDrBNqtco6Etosu01FrKmfOItk3Nk05bYBU4r2DWcaF352jbmA3uS3XM7d9AFU66BOJvPU6qVDyIlUklBf0fgsbj75wDnpHqNgr6EJmpD1PEjDemCXrppg8Sg2NBgn9LDDuJ+A7PBPGSfsF7HC5xMv5N0nyGbEQv13kVSqqSgfxhwU9z9o4FrkzxvJNACtNTX14f4VUrVK0WvMV0vOV3ATDVCkSyAOmdPNxxtmzHfurDM/sQpq9fef+tbvj11df6WaXsyGa3IdI285ulFUkoV9Mtqnb5z7nBgbzP7Rez+0cAAMzs56DVapy9lL2j9frxUu+Cl2jmvvr7duVdSw0Wcx+/4DVvyNncwhO2YlX2bk7WnudkvoQv6LJmukc91J0CRKlFJ6/TfB3rG3d8MWFCitojkL5NlbMlqyidKVas+1VLAuCVt77EZuzONCxnHUTQxk365Bfyg92xbytfUlN8a+WLuRSBSYcot6L8EbOWc29w51xUYAjxY4jaJ5CbTdf/xBW+gY5W8dAEz1UY0sUD5AAfSh9nMoi+TOYpJHMvafJXb50rXnnwL+OS6E6CIlNecfmwqYjDwBj6Lf2y65yuRTyIr17npXPIKAl6zdOIUO5FrDcy2o8Xe4Dvp8wfS5RYUI89BGfkigaiURL5cbgr6ElmZrPtPF9xSretPExT//W+zH/3Iv+Wv+KN9Tdfcg72y50UiI1XQ71zCQQaR6paQRNfuOHTcLKZt+B/8UHjQ488+C5MmBb7ODCZOhF/+0o+KP/wwDP5sQxi7sZ/j79QJVq3K/HPU1cFVV6mErUgZKKvs/Vwoe18iK91OfOl23Qt6vKYmedBuaODzOXM54QS4807YfXeYPBk22SSDdqVS4f+HiJSbSsreF6kc6RLa0m3aE/R4QC99xryN6NvXb5Izfjw8/niSgB/Urk4p/qsoxsY5IhIK9fRFoiqknn4rjss5k/O4iE0bujBlCgwcmGVbElcMxIsfnRCRklNPX6QcpVuaFvT4yJHfHP+IDdmHRzmbSzl4wAJmz4aB7+awxW3bcsFklizxRXcyVYotdkXEC8rwq5SbsvelrOWRvf/oBkfbBnxka7qlduOIF6y11fLb/S9VGd2gnQYT26a6+SIFR6WU4c2Fhvel2ixfDuedB5dfDttu65P2tt029mC6KYNUmpth2LDgzP6GBj/6kGxlAfjRh7XWgkWLcnt/EclIquF9LdkTqSBvvw1Dh8JLL8GoUXDFFT7OfiNdcmAqbXP2QZn98UsDx47t+JwlS4JXBGTy/iKSN83pi+SikPPSOZ779tuhb194802foX/99QkBH/KvW59YEjhR2/x+tkFcdfNFikJBXyRbmdbMD+vcRx8NY8YEvuR//4MRI+DII+GHP4TZs+HQQwOeHEbd+raNc4Iy+ufPDw7idXWqmy9SQgr6ItkKGrpOl8GeSQ8+2bnN4IYbkj5/9mzo1w9uvdXP4z/9dOpE+7w3u4mXatQg6OLiqqvCe38RyV5Qhl+l3JS9L6HLpGZ+okyz1oPOnbART2ur2dVXm3XtarbJJmbTphXmo6aU7jNpUxyRkiBF9r56+iLZymVePGh04Kij2vf6U50jNk++aBEc3P89fvlLGLT8b8zp1JefLijBWvfEUYO6Op9EcPTR/jOBnwZobfU/1ZsXKTkFfalchUq2y2VePFViW3xOwPjxwXPlZjy90RH0bviMR1/egCs5hQc5kPXenx1eTkG22ub3J0+GpUv9FUnYeQ4iEp6gIYBKuWl4v0oVughMtkPXDQ3Bw/aJw/ejR3d4bAU1dj7jrBMrbStet5n0TTn8X3RBny+fNml6QCQnpBjeL3lQLvRNQb9KFSII5SNdRbvEnIC6um+Oz2cz25mnDcyGcYt9wbfSvz6fduYSaHPJc0jXDlXuE8lJqqCv4X2pTPkUoSmEdOvbof18/uLFAPyVg+jNHGbRl8kcxa0MZ22+Sv/6XOSzFDHf9f+Jcl0hISIpKehLZQo7CIWhbf67qSltTsDXPbfiRK7lEP7KFrzDy2zHUaQIvmGsdc8n0Iax/j9e1C7aRCqEgr5UprCDUJjSrJX/z39ggL3AdZzIr/gjzzGQrXgr+HxhrXXPt0RvmOvvo3jRJlIBFPSlMoUdhDIxZgx07uzfr3PnlFX0vun1xy1nM4ObbvLFdj76el2mnjmdPzZcQ1dWBJ/HufCWw4VRojesJXpRvmgTKWMK+lK5wgxC6YwZ44vdt+1At2qVv58q8Mf5/HO/Uc7xx8PAgTBnDux72U99u838GvhkggJyLssVoxRoS3HRJlIFFPRFwjBhQvLj11+fNvDOmOE3yrnnHvj97+Hxx2HjjROe1KdP8vMPHtzxWK4JeVELtMW8aBOpEs5n91eu/v37W0tLS6mbIZUuqKBOvNradkG0tdXveX/eebDppn6XvB13TPK65mZf5S7Zv9Vk+9D36uUDfSbPFZGK45ybaWb9kz6moC8Sgs6dVw/tpxILvB995OP43/8Ohx/urwW+/e2A1wQFcfAXG62t7Y916pT8AiHZc0Wk4qQK+hrel/KVbN66kPvcp3rv3XbL7HXz5/Poo/CjH8Gzz/pgf+edKQJ+7DWBks3pK/NdRAIo6Et5SjZvPWIEDB9emH3u073388/DHntATU3gy5bThTPXvp5994UNN4SWFp+4l3ZmIChYO+eT7BIvQAYPjk5CnohEioK+lKdkhWSWL4cVCcvbgorLZDIiEPScoCI2b70FK1cmjeJvswU78Qx/+OIERo+GF1+EbbbJ8LMmy6p3DkaN8r8nXoBMmgTDhkUnIU9EIqNzqRsgkpNsKrMlPretp94WuNtGBGB1YGxu9iMHy5evfs6IEanfu+14fX27OfgpDGUUN1DjWrn3HvjZzzJvers2jR3r36O+3l8INDb6i5FkFyBTpyppT0Q6iFwin3PucuAAYDnwNjDczP7rnOsF/Ad4PfbUF8xsVLrzKZGvQqVKbkuUmLWeSXb7euv5bWIT1dXBt76V+vWxi4qvljhO5hpuZTg/6fQcU674mPpTDsmszZlS0p6IJCi3RL4ngB+Y2Y+AN4Bz4h5728z6xG5pA75UsGRD3l27Qpcu7Y8lm8vOpNxssoDfdjxdEZvGRmaPvZv+necwiWH8Zp2reOqWueEHfFDSnohkJXJB38weN7OVsbsvAJuVsj0SUckKyUycCLfckn4uO4xyswFFbMzg6qthhwsH8+UGW/LktE789r+n0PmYI9Oft9yr6IlI5EV9Tn8EcGfc/c2dc7OAL4DzzOyfyV7knBsJjASoV4+ncjU2Jk9OC0pYa2728+Lz5vlgHT8snhgo6+qCe/sBwfjTT/20/9/+Bvvv768/1lsvw8+SSZ5BMqnm+0VEEpRkTt8593dgoyQPjTWzB2LPGQv0B35mZuacWwP4lpktcs71A/4KbGtmX6R6L83pC9AxqMLqwN/Q0DFQNjfDUUclP1e3bv48cf92nlpjbxpr7+PT/9Vy+eVw8smZFen7hqroiUhIyq4in3NuGDAK2MPMlgQ85yngDDNLGdEV9AXILahmELVXUsNvOZ+LOI/vdJ7LnS9uQd++ObRPCXkiEpKySuRzzu0DnAUcGB/wnXPrO+dqYr9vAWwFvFOaVkrZyWWv+IaG1KekJz9lOr/jfI7hNl5e2Tu3gA9KyBORoohc0AeuBdYGnnDOzXbO3RA7vgvwinNuDnAPMMrMFpeqkVJmcgmqyZLkYu7nYPowm9n0oYlGbmU432oI2P42E0rIE5EiiFzQN7PvmFnPxKV5ZnavmW1rZr3NbDsz+1up21q1ilXfPkzpgmqyz5QkS39pj005kWv5GfezBe8wi740MmV1SdxUUn1vUdvWVkQqk5lV9K1fv34mIWpqMqutNfMz0P5WW+uPR0FTk1lDg5lz/md8u4Iey/Azvfaa2Q97LjYwO53LbRld/HOdMxs9On27ovy9iUjFAFosICZGMpEvTErkC1mUs8yTZegn7GGfVJrPZAY33wy//KUvxjfp2Onse9fw7JbIRfl7E5GKUnbZ+2FS0A9ZlLPMcw2sKT7Tfxe3csIJcNddsOeecNttsPHGObQtjO+trc6A1uOLSApllb0vEVfqLPNU8+KZZOgne31A21/Y8CD69oV774WLL4bHHssx4EP+31uy7XwLsW2wiFQ0BX3JTimzzNMFvnSBNdnrhw/3pfTitOK4pMtv2OmTewF45hk4+2x/nZCzfL+3oO18k20bLCISJGiyv1JuSuQrgFTJcoXU0NA+Ea7t1tCwul2pkuWCXh93W8BGtmeX6QZmP/+52Wefhdj+fL4355K32bkQGygilQAl8mlOvyJkMi+eat476PUxj7I3x3AbX7m1uXrCWhx3XJaldAtJiYAikiHN6UtlyGRevLHRB8HWVv8zPtEt4PXL6cIZXM6+PMqGfEyL9ecXv4hQwAcV7xGRUCjoS/nIN/Alef1bbMlPeJY/cgZj+DMvMoBtGv4XUoNDpOI9IhKCqG+tK7JavtvItj3vlFNg0SKaOZJR3EBnVnIfh3AIf4127zloK2ERkQxpTl+qzlf123DSe79mEseyE/+kmUbqeQ9qamDSJAVWESlrqeb01dOXqjJrFgx5737eZCvO50J+w+/ozCr/YGurAr6IVDQFfakKZnD11fDrX8N6rjvTbHd24+n2T9I2tiJS4ZTIJ8VX5F36Pv0UDjwQTj0V9t72feZ07tcx4HfpEt25fBGRkCjoS3EVuZzsU09B797w+ONw1VXwwKKdWG/Fhx2f2L177kP75bjVsIhUJQV9Ka4ilZNduRLOPx92393vjPfCC36XPPdeQH3+xYtzeyPVxBeRMqKgL8WVyaY4IbzFbrvB734Hw4bBzJnQt2/swbA3DFJNfBEpIwr6UlwF3qXvvvv8cP4rr/jO9i23+J7+N8KubFeEixgRkbAo6EtxFaic7NKlMGYMHHoofOc7fmnekUcmeWLYle1KvdWwiEgWFPSluApQTva112DAALj+ejjjDHj2WdhyyzRtCKrPny3VxBeRMqJ1+lJ8IZWTNYObbvJVdb/1LXjkEdhnnxDal418SwOLiBSRgr6Upf/+1yfJ33037LknTJ4MG21UosaoJr6IlAkN70vZef556NMH7r8fLrkEHnushAFfRKSMKOhL2WhthYsvhp139ukA//wnnHWWr4kTKhXbEZEKpaAvxZNHMP3wQxg0CM4912foz5oFP/5x+O+jYjsiUsm0ta4Ux5gxcMMNPpC2qa3NKHP/kUd8kZ2vvoJrroERI3xPP6m2oB1fMCfD9wH8RcK8eR2P19X5Iv4iIhGXamtd9fQlM/n2nhMDPqStXLd8OZx+OgweDBtv7CvrHXdcioAP+VfICyqqs2iRevsiUvbU05f0CtV7Bh/BW1s7HH7rLRgyxAf6E0+EP/wB1lwzg7Z26tTx4iLF+2TV1oYGv65fRCTCyqqn75wb55z7wDk3O3YbHPfYOc65t5xzrzvn9i5lO6tKUO/5lFNgvfV8QHXO/56sN5yqJG2SynVNTb5W/jvv+LK6116bYcAPOF/K44lSFdVRaV0RKXORC/oxfzKzPrHbVADn3DbAEGBbYB/gOudcTSkbWTVSDXkvWtT+/vDhHQN/UMB1rl2Q/eorP3d/9NF+Sd7s2XDIIVm2Nd8KeY2Nfv4+GZXWFZEyF9Wgn8xBwB1mtszM3gXeAgaUuE3VIZtgt2JFx/nzZIHYORg16pvpgZdfhu22873888+H6dNzjLFhlPm96iqV1hWRihTVoH+Sc+4V59xE59y6sWObAu/FPef92LEOnHMjnXMtzrmWhQsXFrqtlS9Z0E4lcWQgWSCePBmuuw4zH2N33NHPGEybBhdeCJ3zqRWZb239AuwPICISBSVJ5HPO/R1IVkNtLPAC8ClgwO+Ajc1shHPuz8DzZtYUO8fNwFQzuzfVeymRLyTNze3ry3/1Vfuh/XgZJrwtXOhnAx5+GA44ACZO9GkBIiKSu1SJfCWpvW9me2byPOfcX4CHYnffB3rGPbwZsCDkpkmQxPryzc1+wfzy5e2f16VLRsPg06f70y1aBFdfDSedlGYpnoiI5C1yw/vOuY3j7h4CvBr7/UFgiHNuDefc5sBWwIvFbp/ENDb6RfPxNXC7dYNbbkk5DL5yJfzmN7DHHtC9O8yYASefHBfwm5szWxEgIiJZi1zQBy5zzv3LOfcK8FPgNAAzew24C/g38ChwopmtKl0zq1xzM0ya1H7te5qponnzYNdd4aKL4Nhj/Rr8Pn0Szjl8eMcVASNGtA/8qo0vIpITFeeR3AQVsQmYz7/vPj8wsGoV3HgjDB2axTnjz5tvoSARkQqXak5fQV9yk2Hlu6VL4Ve/8lV4t98ebr8dttwyy3PGnzfLiw0RkWpTVhX5pExkUPnutddgwAAf8M88E555JkXAT3XO+MeCCgWpWp6ISFoK+pLbHHmKyndmfrR9++3hk0/g0Ufhssuga9cMztmlS8fjXbuuXhGQT5ld5QKISLUzs4q+9evXzySFpiaz2lozP7Dub7W1/ngmr21oMHPO/2xqss8+Mzv8cH+avfYy+/DDHNpTV7e6LXV17duSa3vz+ZwiImUEaLGAmKg5/WoX4hz588/7BL0PPvAd8zPOaL+iLzSJhYLGj0+fxBfW58zlvUVEikiJfAr6wYIq4mS6FS0+I//SS33N/Pp6n6y3ww4htjEM+W65C1o5ICJlQYl8klxzc3DQz3C3mwULYNAg3/k97DCYNSuCAR/y33IXgrcYTtxgSEQkohT0q9kppwT3fjMopTt1KvTu7Yf1b7rJ9/DXWacA7QxDvlvuglYOiEjZU9DPVKVlfjc3B2+YY5ZyuHrZMr/2fr/9YJNNfGW9446LeO38MHbOC2O0QESkhBT0M9E2lztvng+I8+b5++Uc+FMNSTc0BD705pswcCD86U9w4om+dv73v1+A9hVCvlvuhjFaICJSQgr6majEudxUQ9IBQWzyZNhuO3j3Xbj/frj2WlhzzQK1L4rCGC0QESkhZe9nIozM76gJWsJWVwefftru0Jdf+l795Mmw885+gKNnz44vFRGR0lP2fr4qcS43aKj6qqvaHXr5ZejXzwf6Cy6AadMU8EVEypWCfiYqcS43zVC1GVx5Jfz4x34mY9o0GDcOOncuaatFRCQP+i88E21ztpVWia2xMelnWLjQ73c/dSoceCBMnOhH/UVEpLwp6GcqIEBWmunT/cdctAiuucbP5Ud6KZ6IiGRMw/sCwMqVcN55sMce0L07vPginHRSkQN+pdVCEBGJGAX9atfczLzNfsKuXZ5l/HgYvstbzJzpK+0Vux0VVwtBRCRiFPSrWXMz9454mD4fPMS/+CFTGMrNL/Wm219LEGgrsRaCiEjEKOhXqaVLYdQoOGz5FLbiTWbRl6HcUbpAq7r2IiIFp6BfhV57DbbfHm78qpFfcynPsBNb8s7qJ5Qi0FZiLQQRkYhR0C9nWSa+mcGNN0L//n5Z3qMbHMOlnE1XVrR/YikCbSXWQhARiRgF/XKVZeLbZ5/B4Yf7If2dd4Y5c2DvK/bOPNAWOrNede1FRArPzCr61q9fP6tIDQ1mPty3vzU0dHjqs8+a1debde5sdumlZqtWxT3Y1ORf45z/2dTU8b2amsxqa9u/T21t8ucGyeR9REQkb0CLBcREbbhTrjLYBGjVKrjkEl8zv74ebr8ddtghh/cK2pynocFvUZtO26hEfHZ+ba168iIiBaANdypRmsS3BQtgr718wZ3DD4dZs3IM+JB/Zr2W44mIRIKCfrlKkfj28MO+uM6MGXDzzTBlCqyzTh7vlW9mvZbjiYhEgoJ+uUqS+LbszzdxWksj++8Pm2wCLS0wYkQIpXTzzazXcjwRkUiIXNB3zt3pnJsdu811zs2OHe/lnFsa99gNJW5q6TU2+jn11lbefGIuA68ZypVX+pr5M2bA978f4vvkk1mv5XgiIpEQuV32zOyItt+dc38EPo97+G0z61P0RkXc5MkwZgx07Qp//SscdFAB3iSfXQYrdWtiEZEyE7mg38Y554CfA7uXui1R9eWXPtg3NcEuu/ifPXuWulUBqmRrYhGRKIvc8H6cnYGPzezNuGObO+dmOeeeds7tXKqGRcHMmbDddj5Jb9w4mDYtwgFfREQioSQ9fefc34GNkjw01sweiP0+FLg97rEPgXozW+Sc6wf81Tm3rZl9keT8I4GRAPUVlixmBldeCWedBRtuCNOn+16+iIhIOiUJ+ma2Z6rHnXOdgZ8B/eJeswxYFvt9pnPubeC7QIfKO2Y2AZgAvjhPeC0vrYUL4dhjYepUP29/881QV1fqVomISLmI6vD+nsD/mdn7bQecc+s752piv28BbAXxW8NVtmnT/Nr7J5+Ea6+F++9XwBcRkexENegPof3QPsAuwCvOuTnAPcAoM1tc9JYV2YoVPul9zz19gZ0ZM+DEE0NYey8iIlUnktn7ZnZskmP3AvcWvzWlM3cuHHkkPP88HHccXHUVdOtW6laJiEi5imTQF7jnHvjFL/zeObffDkOGlLpFIiJS7qI6vF+1liyBE07wm+RsvTXMnq2ALyIi4VDQj5BXX4UBA3yF21//Gp55BrbYotStEhGRSqGgHwFmcMMNsP328Omn8NhjcOml0KVLqVsmIiKVREG/xD77zA/ljx7ti+zMmQODBpW6VSIiUokU9Evo2WehTx944AG47DJ45BFfZU9ERKQQFPRLYNUquOgi2HVX6NzZB/8zz4RO+muIiEgBaclekS1YAEcd5WvmDx3q5/K7dy91q0REpBoo6BfRQw/52vlLl8LEif53VdYTEZFi0YByESxbBqeeCgccAJtt5rfFHT5cAV9ERIpLPf0Ce+MNX1xn1iw4+WSfsLfmmqVulYiIVCMF/QK67TYYMwbWWMNn6B94YKlbJCIi1UzD+wXw5Zdw9NEwbBj06+fX3ivgi4hIqSnoh2zmTNhuO5gyBS68EKZN8/P4IiIipaagH5LWVrjiCthxR/j6a3jqKTj/fKipKXXLREREPM3ph+CTT/zyu0cegYMPhptvhh49St0qERGR9tTTz9OTT0Lv3n4Y/9pr4b77FPBFRCSaFPRztGIFnHsu7LUXfPvbMGMGnHii1t6LiEh0KejnYO5cvyPexRfDiBHQ0uJ7+yXT3Ay9evni/b16+fsiIiIJNKefpbvvhuOPBzO44w444ogSN6i5GUaOhCVL/P158/x9gMbG0rVLREQiRz39DJnBqFHw85/D974Hs2dHIOADjB27OuC3WbLEHxcREYmjoJ8h56BbNzjrLPjnP2HzzUvdopj587M7LiIiVUvD+1n4wx8imKhXX++H9JMdFxERiaOefhYiF/ABxo+H2tr2x2pr/XEREZE4CvrlrrERJkyAhgZ/VdLQ4O8riU9ERBJoeL8SNDYqyIuISFrq6YuIiFQJBX0REZEqUZKg75w73Dn3mnOu1TnXP+Gxc5xzbznnXnfO7R13vJ9z7l+xx652LpJpdSIiIpFVqp7+q8DPgH/EH3TObQMMAbYF9gGuc861bU57PTAS2Cp226dorRUREakAJQn6ZvYfM3s9yUMHAXeY2TIzexd4CxjgnNsY6G5mz5uZAbcBBxevxSIiIuUvanP6mwLvxd1/P3Zs09jvicdFREQkQwVbsuec+zuwUZKHxprZA0EvS3LMUhwPeu+R+KkA6lWZTkREBChg0DezPXN42ftAz7j7mwELYsc3S3I86L0nABMA+vfvH3hxICIiUk2iNrz/IDDEObeGc25zfMLei2b2IfClc+7Hsaz9Y4Cg0QIRERFJolRL9g5xzr0P7Ag87Jx7DMDMXgPuAv4NPAqcaGarYi8bDdyET+57G3ik6A0XEREpY84nw1cu59xCIMk2dO2sB3xahOZIavo7RIP+DtGgv0M0lOPfocHM1k/2QMUH/Uw451rMrH/6Z0oh6e8QDfo7RIP+DtFQaX+HqM3pi4iISIEo6IuIiFQJBX1vQqkbIID+DlGhv0M06O8QDRX1d9CcvoiISJVQT19ERKRKVF3Q17a+0eOcG+ec+8A5Nzt2Gxz3WNK/iRSGc26f2Hf9lnPu7FK3p5o45+bG/p+Z7ZxriR3r4Zx7wjn3ZuznuqVuZ6Vxzk10zn3inHs17ljg917u/ydVXdBH2/pG1Z/MrE/sNhXS/k0kZLHv9s/AvsA2wNDY30CK56exfwNtHZKzgSfNbCvgydh9CdetdPw/Pen3Xgn/J1Vd0Ne2vmUl6d+kxG2qZAOAt8zsHTNbDtyB/xtI6RwETIr9Pgn93xM6M/sHsDjhcND3Xvb/J1Vd0E9B2/qW1knOuVdiQ21tQ2lBfxMpDH3fpWXA4865mbGdQgE2jO09QuznBiVrXXUJ+t7L/t9IwXbZK6VSbusryaX6m+CnT36H/15/B/wRGIG++2LT911aPzGzBc65DYAnnHP/V+oGSQdl/2+kIoN+Kbf1leQy/Zs45/4CPBS7G/Q3kcLQ911CZrYg9vMT59z9+GHjj51zG5vZh7Gpxk9K2sjqEfS9l/2/EQ3vr6ZtfUsk9o+qzSH4ZEsI+JsUu31V5CVgK+fc5s65rviEpQdL3Kaq4Jzr5pxbu+13YBD+38GDwLDY04ah/3uKJeh7L/v/kyqyp5+Kc+4Q4Bpgffy2vrPNbG8ze80517at70o6but7K7AWfktfbesbrsucc33ww2RzgRPAb7Wc4m8iITOzlc65k4DHgBpgYmy7aym8DYH7Y6uBOwNTzOxR59xLwF3OueOA+cDhJWxjRXLO3Q7sBqwX2/L9AuASknzvlfB/kiryiYiIVAkN74uIiFQJBX0REZEqoaAvIiJSJRT0RUREqoSCvoiISJVQ0BeR0Djnejrn3nXO9YjdXzd2v6HUbRMRBX0RCZGZvYcvq3xJ7NAlwAQzm1e6VolIG63TF5FQOee6ADOBicDxQN/Yrn0iUmJVV5FPRArLzFY4584EHgUGKeCLRIeG90WkEPYFPgR+UOqGiMhqCvoiEqrYPgp7AT8GTkvYUElESkhBX0RCE9uJ8nrgVDObD1wO/KG0rRKRNgr6IhKm44H5ZvZE7P51wPecc7uWsE0iEqPsfRERkSqhnr6IiEiVUNAXERGpEgr6IiIiVUJBX0REpEoo6IuIiFQJBX0REZEqoaAvIiJSJRT0RUREqsT/AwLJ67E0we+wAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "est_weight, est_bias = train_model(X_train, y_train, alpha, max_epoch)\n",
    "print(f\"Estimated Weight: {est_weight}\\nEstimated Bias: {est_bias}\")\n",
    "y_pred = (est_weight*X_test) + est_bias\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(y_test, y_pred, marker='o', color='red')\n",
    "\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_pred), max(y_pred)], color='blue', label=\"line1\")\n",
    "#plt.plot(y_test, y_test, color='orange', label=\"line2\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.593441909037288\n",
      "173.2860954934168\n",
      "0.8978521452417096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.05011893, -0.65406671,  1.33977899, -0.6476337 , -0.09072538,\n       -0.67161437, -1.94121883, -0.88579565, -0.67552575,  1.0302058 ,\n       -0.02699786,  1.15802548, -1.08282817,  0.82975141, -0.33147203,\n        0.91027829, -1.56336414, -0.62343509, -1.48133953,  0.98156963,\n       -0.79753067,  1.24923471,  2.40415379,  0.50728827, -0.02855451,\n        1.32388265, -0.9804446 ,  0.35720381, -1.04077017, -0.33772038,\n        1.20249973,  0.90063358,  0.68769806,  0.74902782,  1.84003162,\n        1.22227647, -0.75886596,  0.15359862, -1.32772018, -0.27912728,\n       -0.69417972, -0.44222109,  0.59152436,  1.00172426, -1.13213405,\n        0.2077216 , -0.39042125, -0.09406684,  0.06301996, -0.05654816,\n        2.39979023, -0.69548917,  0.67978004, -0.42874012, -0.8588812 ,\n       -1.08518756,  0.33045577,  1.24238479, -0.56821753, -0.63905589,\n       -1.89271951,  1.35837615, -0.68461437, -0.96451243, -0.12612626,\n        1.55840307, -1.15611449,  1.55718631,  0.3000472 ,  0.1583928 ,\n        1.49860123,  0.40940439,  2.6222211 , -0.26250097,  1.43179396,\n        0.46717783, -1.80147978,  1.66286014, -1.31742813, -0.88088521,\n       -1.21200738, -1.41789081,  1.50112022,  1.26059639,  0.04277458,\n       -0.0948551 ,  1.71774984,  0.10808209, -0.14347251,  1.1975884 ,\n        0.50001918,  0.57758471, -0.70267944,  1.8004137 ,  0.47110629,\n       -0.20628578,  0.1893442 ,  1.22980304, -0.61119528, -0.08637639,\n        0.45856674,  1.52740172, -1.02069642,  1.62205858, -1.53623361,\n        1.57074855,  0.44384066, -1.59394142, -0.01205451,  0.27750461,\n       -0.39668958, -0.55743704,  1.14110597, -0.03287497,  0.17373096,\n       -1.16298025, -0.16786646,  0.59324966,  0.76943672, -1.73320504,\n       -0.66870787,  0.43313245, -1.38555149,  0.43854276, -0.94498022,\n        1.18729726, -0.36628192,  0.34369107,  0.34083209,  2.49399785,\n        0.71035309, -0.98088239, -0.1650212 ,  1.37602555, -1.51446674,\n        0.91054219,  1.26818776,  1.3071883 ,  0.96893859, -0.57136131,\n        2.11024266, -0.26157827,  1.29636387,  1.4026389 ,  0.77706833,\n        0.51417837, -0.2944796 , -0.53094855,  0.68132248,  0.16266966,\n       -0.73247403, -0.33735737, -0.0069448 ,  1.14738123,  0.18722836,\n        0.56867922,  0.15262386,  0.09422217,  1.25124132,  1.30067944,\n        0.66174751, -0.08387771,  1.55240676, -0.50709846, -0.90167292,\n       -1.24616548, -0.81158967,  0.44172141,  0.13250046, -0.52245294,\n        1.20022994, -0.67031172, -0.17796859,  2.01150585, -0.26627681,\n        0.4108713 ,  0.23558832,  0.70693451,  0.98527647, -0.41905727,\n        0.54658711,  1.06557164,  0.69027501, -1.07466648,  0.1353303 ,\n        0.65548886,  1.19758362, -0.51891723,  0.22663638,  0.02453217,\n       -0.43233655,  2.32207098,  1.47191568,  1.27050469, -0.31633117,\n        0.95715903, -0.38678663, -0.9934927 ,  0.63907599, -0.030957  ,\n       -0.6401502 ,  0.22414165,  0.94117739,  1.62405897, -1.0770742 ,\n        0.37090998,  1.46301864,  0.95937099, -2.1469858 , -0.90027112,\n        1.57173706,  0.61063343,  0.66457673, -0.80949345,  0.19018927,\n       -0.69742254,  1.50202338,  0.93193028,  0.38177893, -0.34421647,\n       -0.3659959 , -0.1752621 ,  0.52498946, -0.73267162, -0.82548875,\n        0.28139802, -0.9598318 ,  0.07513905, -0.06721433, -0.26884238,\n        2.42590239, -2.20348171,  0.44571149, -0.85964927,  0.19735248,\n       -1.07853391, -0.22492179, -1.91258748, -1.91563004, -0.4083475 ,\n        1.9948493 ,  1.30782569, -0.50471283,  0.84828096,  0.85371879,\n       -2.3500253 , -0.06649625,  1.33410893,  0.49690778, -1.3036227 ,\n        1.19975712, -0.89989747,  0.0050135 , -0.60065916,  1.68629104,\n       -1.17432852, -0.22548717,  0.35918016,  0.29117475,  0.78207405,\n        1.03841994, -0.45600606, -0.1171398 ,  0.65687919,  0.2879497 ,\n        0.09901023,  0.28360698,  0.29311905,  0.32334886, -1.65596129,\n        0.41732588,  0.95851395, -0.49280276,  0.04341237,  0.5163436 ,\n        0.51906584, -0.48612143,  0.56787491, -0.02355835, -1.2751233 ,\n        0.99275738,  0.02771261, -0.11857607, -1.10038673,  1.9426963 ,\n       -1.1963816 ,  0.83002224,  1.01126207, -1.43513253, -2.19202592,\n        2.34118738, -0.74032486,  0.41447613,  1.63447824, -0.31042167,\n       -0.06335349, -1.15708402, -1.31020663, -1.57193448,  0.15542279,\n       -0.36791361, -0.15076405,  0.48402326, -0.43644853,  1.40468837,\n        0.37797748,  0.50431083, -0.32053403, -0.76983848, -0.60069998,\n        0.4002507 , -0.0411855 , -0.497357  ,  0.92706142,  1.12433176,\n       -0.27282578,  1.65675635, -1.39194126, -1.70980318,  1.42093699,\n       -0.29919787,  0.28720399, -0.72573391, -0.28206249,  0.41097127,\n        0.32266173, -2.24077823,  0.15674114, -0.76679441, -0.88654755,\n       -0.00709525,  0.34227073,  1.15825898,  0.9191565 ,  1.50668706,\n       -0.82817913,  0.17471642, -0.78857505, -1.88202541,  0.3433022 ,\n        0.85694864, -0.28092037,  0.03117143,  0.00647191,  0.99596285,\n        0.42849756, -0.11354938,  0.25276745, -0.01086748, -0.29670041,\n        1.5134535 ,  0.32967971, -0.38346852,  1.12037321, -0.7890819 ,\n       -0.31546718, -0.81412092,  0.74540248,  1.0974743 ,  0.92959477,\n       -0.42001464,  0.55651023,  0.98266028, -0.4251015 , -0.24100843,\n       -0.22235577,  1.20689239,  0.53893163, -1.36039143, -0.19527367,\n       -0.71956267, -2.76493569, -0.53265514,  1.65084498,  0.70731388,\n       -0.92619026, -0.23391533,  0.47262998,  0.92182093, -0.6267199 ,\n        1.0998496 ,  0.33586519,  0.34653957,  0.25778227, -1.08250002,\n        1.80394475, -0.1497751 , -0.10043129, -0.35823585, -0.75175324,\n       -0.0768503 ,  0.46454546, -0.44587431, -0.3067073 , -0.94096349,\n        0.5824706 , -0.53630904, -2.38209385,  1.66396693, -0.98037626,\n        0.07905761,  1.3707572 , -0.58799183,  1.88405285,  1.01293789,\n        1.00349639, -1.45273292, -0.49428021,  0.95158815,  0.74181297,\n       -1.88619776,  0.81020706,  0.21719775,  1.26759125,  1.79184739,\n        0.7609248 ,  1.71311733, -0.65504397,  1.11765341, -0.08560918,\n       -0.87351316,  1.91583313,  0.66149825,  1.3683484 , -0.41515427,\n        0.98072142,  1.39046889,  0.40252081, -0.65495059, -0.96953537,\n        0.92659687,  0.55197264,  0.31528217, -0.39573967, -0.84683459,\n        0.46571921, -0.8353941 ,  0.38728749, -0.65513433,  0.5202482 ,\n        0.86056709, -1.13358554, -0.8039926 , -0.05353393, -0.22715644,\n       -0.26184891,  1.14218373,  0.30043805, -1.83136817,  0.75556845,\n        0.39851109,  1.91434452,  0.32137832, -0.11812741,  0.63019832,\n       -0.76245732,  1.25300505, -2.24470224, -0.48290973, -1.08230303,\n       -0.42215601,  1.52747761,  1.47678607,  0.0617582 , -2.04773533,\n       -0.25551779, -0.90199052, -1.56571375, -0.72858629, -0.35254178,\n       -1.40275314, -1.28638244,  1.94777419,  2.53152948,  0.19201901,\n       -0.23928335, -1.09512392, -1.43225754,  1.34376202,  0.22036052,\n        1.17168492,  1.01590859,  0.11837024, -1.16065223, -1.59979495,\n       -0.40932358,  0.53789333,  1.737317  ,  1.18867147,  0.58620396,\n       -1.70152393, -1.2041433 ,  1.08748343,  1.86706102,  0.04619126,\n       -0.32990412, -0.24441281,  1.38862711, -1.19687857, -0.29976554,\n        0.19632469,  0.12326735, -0.15947343,  0.19101691,  0.47616256,\n       -0.0612395 ,  0.6768725 ,  0.194504  , -0.48577343,  1.23130722,\n        1.45545871, -0.48158999, -2.13360154, -0.0447293 , -0.25060485,\n        0.12133296,  0.01384479,  0.57891566,  1.23640534, -0.17812505,\n       -0.4111477 , -0.13175398, -1.34406815,  2.51166827,  0.7602283 ,\n        0.16391832,  1.20328757, -0.3346257 , -1.83519948,  0.14061291,\n       -1.66755959, -0.89260829,  1.10377377, -0.42669732,  1.42418647,\n        2.69711269, -0.7521414 ,  0.46072378, -0.08940003, -0.29664763,\n        1.13834588, -0.88786246, -0.86206555, -0.21462513,  0.37187234,\n       -0.20550398, -1.6093388 ,  0.55439272,  2.06461083, -1.53406211,\n       -1.42083396, -0.80222256, -0.93586005, -0.08332923, -0.5061043 ,\n       -0.50595194, -0.15431815, -0.3390695 ,  0.39564255, -0.16154976,\n        1.82545766, -0.71524536,  0.43349976, -0.10923117,  1.18917601,\n       -0.13824947,  1.21498369, -0.25406098,  1.2664588 ,  0.35016605,\n       -1.72593214,  1.15101269,  1.19436487,  1.86181329, -0.53228834,\n        0.83978735, -0.59425153,  0.01836236, -0.76948268,  2.45996919,\n       -0.16918506,  1.05564567,  0.24519561,  0.72014025, -0.67236799,\n        2.14108348,  1.05837899, -1.53539713, -0.79137256, -0.28818089,\n        0.7146442 , -1.2370043 ,  0.43392075, -0.18086252,  0.73647048,\n       -2.08027625,  1.99405447,  0.85286874, -1.45410555,  0.25055865,\n        1.29936643, -1.54043737, -0.04124275,  1.0835539 , -1.23763222,\n        0.82139587,  1.24641636,  0.44403578,  2.2903707 ,  0.74864988,\n        0.48493134,  0.18505186, -0.23894875, -1.0056088 , -0.4245823 ,\n       -0.63704896, -0.42487359,  0.20268791, -0.7560595 , -1.97336614,\n       -0.86148916, -0.13334578,  0.4119908 , -0.85185079, -0.40120079,\n        0.13492598,  0.93463491,  0.2809414 ,  2.04830365,  0.13537969,\n        0.69926824, -0.17588149,  0.23803583,  1.50155763, -2.05710872,\n       -1.6001581 ,  0.63376657, -0.45351388, -0.31663674,  0.04252698,\n        0.55023378,  0.01563487, -1.34633934,  0.39438934, -1.55447059,\n        0.82077222,  0.2558472 ,  1.22482428,  2.55120707,  2.25752998,\n       -0.85171201,  0.11573018, -2.06091322,  1.29993567,  0.60219204,\n        0.05191566, -0.34952784, -1.08555208, -0.2465339 , -1.74825902,\n        0.34238813,  1.08357298, -1.80400388,  1.87720354,  0.80381837,\n        0.74432299, -0.14493969, -0.22958416, -0.98263119, -1.57262058,\n        1.40758879, -0.19177558,  0.38831722, -0.36190417, -0.79735485,\n       -0.53042453,  0.52590815,  1.32853917,  0.04217241, -1.52904165,\n       -1.1869355 ,  0.35103106,  2.40780656,  0.14611597,  0.49160991,\n        0.64295135, -0.12133305, -0.90891227,  0.07233209,  0.33488206,\n        1.19701938, -2.0555715 ,  0.37177511,  1.76777305, -0.98317231,\n       -0.5570386 , -1.1030988 , -0.63245154,  1.54454204,  0.85138717,\n       -0.03458629, -0.29772924,  2.21251877, -0.07897668, -1.81121508,\n       -1.21693783,  0.3750427 , -1.04108   , -0.88190972, -0.67600368,\n        0.24650874,  2.37836384, -0.63977123, -1.17405147, -0.87726384,\n       -2.31329687,  0.28754779, -0.79647872, -1.75197764, -1.10644699,\n        0.00809605, -1.06820346, -1.11807689, -1.83986892,  0.05650899,\n       -0.29622639, -1.06802476, -1.09589849,  0.85049016, -1.56075999,\n       -0.20774395, -0.06000961, -0.5132294 , -1.11358023, -0.33252339,\n       -0.25274685, -1.01989721, -0.63569572,  1.63347315,  0.42662324,\n       -0.19872476, -0.6581667 ,  0.59833573, -0.59840562,  0.56104394,\n       -0.14509859,  0.13782499,  0.66624358, -1.04371424,  1.55790557,\n       -0.11071417,  1.28589494, -0.67905456,  1.92258797,  1.03768817,\n        1.20540586,  1.15356115, -0.5710767 , -1.1423485 ,  0.26328824,\n       -0.46150498, -0.09230877,  0.50751551, -1.27722092, -0.78244303,\n       -0.70554603, -0.58206431, -0.39667478,  0.16488059, -0.31567732,\n        2.18310534,  1.57688601, -0.82763562,  0.14726512,  1.08574374,\n       -2.46619868,  0.27499716, -0.10975557,  1.76214994,  0.71470782,\n       -0.74640479,  0.27087544,  1.22843639,  0.53941995,  0.52557114,\n       -1.62587409,  0.84388566, -0.34648226,  0.1573338 ,  0.32621585,\n       -0.51086025,  3.14583535, -0.98756266, -1.12806606, -0.28101544,\n       -0.76456906, -1.26174237, -1.51676652,  1.51163995,  0.21837148,\n        0.81393839, -0.38457956, -0.26772459,  0.97871646,  2.13008918,\n        0.10763463, -0.9928588 ,  1.20871438, -0.41916108,  0.97850025,\n       -1.33371667,  0.51273522, -0.40502258,  0.97699755, -1.49935751,\n        1.21322079, -0.78769728,  0.91800161,  1.10709994, -0.57179542,\n       -0.89054495,  0.23638826,  0.61095662, -0.38250552, -2.04591718,\n        0.24963659, -0.26882883, -0.43331566, -0.16906238, -0.49749017,\n       -0.16295456, -0.83076576, -1.75029517,  0.59726717,  0.06967349,\n       -1.19575224, -0.72932402, -0.40733947, -0.29498877,  1.5321428 ,\n        0.01453888,  0.11658969, -0.03580243, -1.88671954, -1.44845488,\n       -0.65766411, -2.14722674, -0.84712177, -1.12189543, -0.06679097,\n       -1.2691671 , -0.04534198, -0.7794224 ,  0.02483706, -0.78216154,\n       -0.49199549, -0.73815091, -1.0486248 , -0.51089064,  0.28567653,\n       -0.30371472,  0.43768294, -1.77492928, -2.23747899,  0.48251703,\n        0.51100981, -0.57697862, -0.27110142,  0.68361095,  0.60544923,\n       -0.61152857, -0.6909976 ,  0.45323675, -0.52820293,  0.79426216,\n        0.03865558,  1.13652183, -1.08170302,  1.89927913, -1.07322652,\n       -0.13057259,  0.17038243,  0.97750569,  1.50664585,  0.43027592,\n        0.89140837,  0.52510895,  0.21727774,  1.18453973,  1.48616781,\n       -0.00671817,  0.93112096,  0.4013479 ,  0.44820144,  0.68571424,\n        1.99561759,  0.62871993,  0.48426893,  0.92105048, -1.70635171,\n        0.32069357,  0.98504189, -0.98178591,  0.97899306, -0.13447646,\n        1.94693493,  1.7145066 ,  0.62842654, -0.22728455, -0.87281962,\n       -0.4026924 ,  0.1042429 , -2.64685577,  1.90032373,  1.77911018,\n        0.79966306, -0.96407793,  0.30742398, -0.12055074, -0.59764581,\n       -2.69540398,  1.42085967, -0.24885802,  0.65689796, -1.70783159,\n        1.65774888,  1.24676403, -0.90949614, -0.83907134,  0.56884495,\n        1.05452807, -0.24646194,  1.14812109, -0.75959773, -0.34012266,\n       -1.72732998, -1.50575725, -0.22223624,  2.10480114,  1.95675275,\n       -0.75176942, -0.0668456 ,  0.27717016,  0.47629153, -0.28784473,\n        0.17190845, -0.07289224,  1.29163727,  0.48889548, -0.35479157,\n       -0.03182074, -0.20614051, -0.50934445,  1.28999412, -0.22768424,\n       -0.11192847, -0.48647882, -0.61725527, -1.26439825,  0.72274526,\n        0.2908074 , -0.1542079 ,  0.0435159 ,  2.27641195,  0.61774101,\n        0.84430341,  0.42887818, -1.37083467, -1.94472848,  0.77249236,\n       -1.24780721,  0.02944151, -0.6494101 , -0.81078795, -0.52458972,\n       -0.4661756 , -1.99984878, -1.8569638 , -0.55872887,  0.01941466,\n        0.06213873, -0.07018289, -0.81817459, -1.80545463,  1.08437217,\n       -2.16809382,  0.16653428, -0.15095713, -1.90399738,  0.63947378,\n        0.72447113,  0.61845781,  0.88531065, -0.82244879, -0.68462853,\n        0.7249741 , -2.03493393,  0.94542335, -0.65227822, -0.33424598])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 6.24251354e+00, -8.11824067e+00,  6.67224555e+01, -2.77167931e+01,\n        8.19207453e+00, -1.85975634e+01, -7.31747338e+01, -2.58654640e+01,\n       -9.00159641e+00,  5.70769631e+01,  8.25614691e+00,  3.60951293e+01,\n       -4.37178955e+01,  2.68389768e+01,  2.83097144e+00,  5.13804145e+01,\n       -5.87649462e+01, -3.97432142e+01, -4.81891618e+01,  2.01022126e+01,\n       -3.43441494e+01,  5.84652563e+01,  1.07747983e+02,  4.00757628e+01,\n       -1.46175832e+01,  7.28422887e+01, -9.01460749e+00,  2.91140591e+01,\n       -5.05497422e+01, -1.01154895e+01,  7.74266254e+01,  5.05943222e+01,\n        2.10375993e+01,  3.67073169e+01,  7.37582445e+01,  5.33909442e+01,\n       -1.80094381e+00,  1.40192079e+01, -3.25767518e+01, -1.12872425e+01,\n        3.49864024e+00, -1.25020675e+01,  1.84473688e+01,  4.23759296e+01,\n       -1.24324684e+01,  2.57110278e+01,  1.84618719e+00, -9.57440231e+00,\n       -1.14633584e+01,  3.61486031e+00,  9.81116238e+01, -8.79652015e+00,\n        4.79232433e+01, -9.96491864e+00, -4.02364195e+01, -4.03187395e+01,\n        1.07187013e+01,  3.95610120e+01,  7.70945199e+00, -1.87927915e+01,\n       -6.76123049e+01,  6.06806804e+01, -1.71094745e+00, -1.61838139e+01,\n        1.99118867e+01,  5.59032729e+01, -3.62142657e+01,  6.77994060e+01,\n        2.47613864e+01,  3.28285931e+01,  7.14600303e+01,  3.90008020e+01,\n        1.12615263e+02,  1.26608686e+00,  5.19882072e+01,  2.56097892e+01,\n       -6.64932435e+01,  5.51149761e+01, -3.57643755e+01, -3.54805136e+01,\n       -5.74350790e+01, -6.43344596e+01,  5.85368521e+01,  6.70699429e+01,\n        1.87837254e+01, -4.92681923e-01,  6.24041317e+01,  2.11279242e+01,\n       -1.58911895e+01,  5.62136795e+01,  2.44692170e+01,  5.21110452e+01,\n       -2.69313683e+01,  7.01453196e+01,  3.35513942e+01, -1.37069163e+01,\n        2.49034530e+01,  5.49495924e+01, -2.21693041e+01,  4.33891425e+00,\n        1.48328672e+01,  7.29964274e+01, -1.01328730e+00,  4.96267870e+01,\n       -7.49968715e+01,  7.22705585e+01,  2.84669741e+01, -5.85489560e+01,\n       -1.06006922e+00,  1.18119068e+01, -9.57252878e+00,  5.16008895e+00,\n        3.37197331e+01,  2.00587850e+01,  5.22831792e+01, -3.39097761e+01,\n        1.01659230e+01,  3.45116415e+01,  2.73120313e+01, -3.52311888e+01,\n       -1.26607168e+01,  2.52399489e+01, -3.82513780e+01,  2.74503972e+01,\n       -3.40152891e+01,  1.63506556e+01, -2.90396119e+00,  1.30467650e+01,\n        1.55439132e+01,  8.76450709e+01,  2.50557665e+01, -4.97490939e+01,\n        1.19980352e+01,  4.52276335e+01, -5.74952756e+01,  5.91392940e+01,\n        5.87350977e+01,  3.13347399e+01,  4.10360439e+01, -2.69265472e+01,\n        1.07367729e+02, -2.25245250e+00,  8.48277743e+01,  6.41905516e+01,\n        2.64851596e+01,  3.70290831e+00, -1.59991727e+01, -4.77134940e-01,\n        3.80980827e+01,  2.87628994e+01,  1.38146376e-01, -2.95092719e+00,\n        2.05701072e+01,  7.05788534e+01,  1.69255663e+01,  2.51531614e+01,\n        1.59328905e+01, -2.01920268e+01,  5.83150495e+01,  6.01178224e+01,\n        1.78778226e+01,  8.80682736e+00,  7.83552069e+01, -1.64207112e+01,\n       -1.40819164e+00, -6.01178218e+01, -8.96481781e+00,  2.33118445e+01,\n        7.14087432e+00, -2.57601197e+00,  5.36947907e+01, -2.33298721e+01,\n        1.83272739e+01,  9.46456710e+01,  1.34753847e+01,  4.45912694e+01,\n        4.04838720e+01,  6.28163526e+01,  1.53152397e+01,  5.18141992e+00,\n        3.12997825e+01,  6.44677772e+01,  2.81469461e+01, -2.79269528e+01,\n        1.76781362e+00,  3.21795880e+01,  5.13822211e+01,  7.41524934e+00,\n       -2.27669008e+00,  2.88958304e+01, -2.06367747e+00,  9.03096947e+01,\n        7.11626997e+01,  7.50780909e+01,  1.38886998e+01,  2.60402591e+01,\n        1.59633971e+01, -1.94821043e+01,  1.99742447e+01,  9.05924405e-01,\n       -2.49343857e+00,  1.34694993e+01,  2.21123290e+01,  8.28251263e+01,\n       -2.09265775e+01,  3.26869433e+01,  5.53631826e+01,  3.57329503e+01,\n       -7.55536720e+01, -3.07664732e+01,  7.12484672e+01,  6.04155578e+01,\n        3.91974974e+01, -1.30605544e+01,  4.99640367e+00,  5.47121675e-01,\n        6.70720647e+01,  6.00104899e+01,  2.93734628e+01,  7.75320619e+00,\n        7.19965009e+00,  2.73834545e+01,  2.20389111e+01, -2.16561248e+01,\n       -1.19091073e+01,  2.09430901e+01, -1.41944645e+01,  2.03364655e+01,\n        8.47327802e+00,  7.79832794e+00,  1.06361519e+02, -5.67108011e+01,\n        4.19806690e+01, -2.05535548e+01,  3.48083983e+01, -2.06449465e+01,\n       -1.65435512e+01, -9.45204012e+01, -6.61123638e+01, -3.05051850e-02,\n        7.37878016e+01,  5.40150014e+01, -5.04125144e+00,  5.83847912e+01,\n        4.33743077e+01, -8.61827666e+01,  1.19767527e+01,  5.50338458e+01,\n        2.62777896e+01, -3.67429356e+01,  4.75253462e+01, -2.46180051e+01,\n       -2.49121522e+01, -4.05310251e+01,  6.30958856e+01, -2.74317452e+01,\n        1.96873473e+00,  1.13885366e+01, -3.77138950e+00,  2.97110940e+01,\n        4.38106287e+01,  4.93976277e+00,  2.10705122e+01,  1.79261157e+01,\n        3.77870903e+01,  9.76951540e+00,  2.73948116e+01,  3.84826939e+01,\n        4.52414357e+01, -8.36651013e+01,  3.77750968e+01,  3.79989185e+01,\n        1.23422787e+01,  3.01049486e+01,  2.03118821e+01,  2.81470430e+01,\n        1.64889207e+00,  4.85433761e+01,  1.17399330e+01, -3.70915709e+01,\n        4.85510930e+01, -1.13666662e+01, -5.42544464e+00, -4.65662227e+01,\n        7.39772104e+01, -6.17320446e+01,  2.54198915e+01,  5.61666884e+01,\n       -5.44034807e+01, -5.36360799e+01,  9.39447979e+01, -1.75885871e+01,\n        3.43914987e+01,  7.43853579e+01,  4.12041900e+00,  3.50300344e+00,\n       -4.44135613e+01, -4.69785225e+01, -6.17476867e+01,  1.59789362e+01,\n        2.35757484e+01,  1.01420053e+01,  3.23350236e+01,  9.26841559e+00,\n        5.49278348e+01,  2.79306804e+01, -2.66336426e+00,  5.37815990e+00,\n       -1.72365218e+01, -2.06431536e+01,  3.75065671e+00, -1.44047256e+01,\n        1.32541273e+01,  5.70049462e+01,  6.02482617e+01, -2.68163335e+00,\n        8.34692701e+01, -5.46364257e+01, -4.80288137e+01,  6.98771303e+01,\n        1.42709532e+00,  3.38707905e+01, -1.92082188e+01,  4.21510723e+00,\n        1.23272097e+01,  1.61983466e+01, -8.51659416e+01,  8.18031460e+00,\n       -2.43921879e+01, -1.98467538e+01,  6.22380964e+00,  1.99546235e+01,\n        6.84264713e+01,  5.96056668e+01,  8.03783125e+01, -2.17997912e+01,\n        2.72057239e+01, -2.22762556e+01, -5.73206721e+01,  3.25137140e+00,\n        4.79389082e+01, -2.58454881e+00, -4.32141606e+00,  2.60373324e+01,\n        5.12447848e+01,  3.45176506e+01,  3.03149814e+01,  3.60751037e+01,\n        1.34530856e+01,  8.08795866e+00,  8.42922168e+01,  4.25688066e+01,\n       -1.50183831e+01,  3.32764209e+01, -5.97786007e+00, -5.84732206e-01,\n       -1.36642028e+01,  1.87957808e+01,  4.04962459e+01,  5.41849219e+01,\n       -8.80625759e+00,  1.85412997e+01,  3.98026908e+01,  4.03092306e+00,\n       -1.16790735e+01, -1.20816283e+01,  5.39319852e+01,  9.46071333e+00,\n       -4.39514021e+01, -1.07443481e+01, -1.24040109e+01, -7.48350973e+01,\n       -2.11741649e+01,  5.46780608e+01,  5.00525672e+01, -3.02699591e+01,\n        1.07664304e+01,  2.65911980e+01,  3.33694369e+01, -2.49105703e+01,\n        4.44393063e+01,  1.72625552e+01,  2.45603331e+01,  2.87697091e+01,\n       -3.82674165e+01,  8.50325642e+01,  2.34720415e+01,  2.65023336e+01,\n       -2.72067781e+00, -2.15410277e+01,  3.26451844e+00,  2.94245935e+01,\n       -1.24799365e+01, -6.33900041e+00, -2.40078021e+01,  2.24674255e+01,\n       -4.40419282e+00, -8.08776584e+01,  7.95330295e+01, -1.80384556e+01,\n        1.83732147e+01,  5.39085832e+01, -1.07464249e+01,  7.45508092e+01,\n        6.17818545e+01,  5.63608747e+01, -4.72532669e+01, -2.20076566e+01,\n        3.37479118e+01,  2.63209842e+01, -7.25827544e+01,  5.79226199e+01,\n        2.40158680e+01,  4.09809589e+01,  6.12135616e+01,  4.54968270e+01,\n        8.26731168e+01, -1.09004825e+01,  2.57969480e+01, -1.80979288e+00,\n       -1.22511781e+01,  9.00267197e+01,  3.03475728e+01,  6.28111923e+01,\n        1.44069604e+01,  5.50046426e+01,  7.08344155e+01,  1.18480216e+01,\n        1.44755698e+00, -2.99732784e+01,  6.74169603e+01,  4.49833729e+01,\n        3.77717236e+01, -1.62194018e+01, -2.24974724e+01,  4.15767371e+01,\n       -2.31442935e+01,  5.03476084e+01, -2.03823831e+01,  8.37804240e+00,\n        5.74505791e+01, -2.29559737e+01, -3.91930099e+01,  2.93960304e+01,\n        1.49653743e+01, -1.26310307e+01,  7.99164054e+01,  1.44079855e+01,\n       -3.68366715e+01,  2.89257701e+01,  3.00429981e+01,  1.11133340e+02,\n       -1.90512290e+00,  1.10532603e+00,  1.85469699e+01, -1.45994066e+01,\n        6.51325745e+01, -6.43063291e+01, -3.24393471e+01, -2.73060730e+01,\n        5.77004234e+00,  4.78123029e+01,  6.79241861e+01,  3.73257069e+01,\n       -8.18429577e+01, -3.82263087e-01, -5.68730702e+01, -5.53277454e+01,\n        7.95762526e+00,  1.54513347e+01, -3.34995967e+01, -2.72607269e+01,\n        8.89536640e+01,  9.69792224e+01,  1.08537768e+01,  1.81236357e+01,\n       -1.97100371e+01, -2.66373297e+01,  6.99315323e+01,  1.10641981e+01,\n        4.99996715e+01,  5.43701979e+01,  1.95077976e-01, -1.47374899e+01,\n       -3.76945200e+01, -2.83847753e+01,  2.53898373e+01,  8.00979947e+01,\n        4.76356185e+01,  3.03025377e+01, -6.13546801e+01, -5.64307015e+01,\n        5.40087416e+01,  8.80191156e+01, -4.68207131e+00,  7.98342312e-01,\n        9.60860218e+00,  8.01841166e+01, -2.60385675e+01, -9.94696525e-01,\n        1.42959243e+01, -4.21832823e+00,  1.27316146e+01,  3.02970984e+01,\n        3.37668095e+01,  2.43217647e+01,  2.91626125e+01, -2.23583378e-01,\n       -2.17531747e+01,  3.93767133e+01,  7.43449184e+01, -2.83687753e+00,\n       -4.82930720e+01,  2.50665876e+01,  1.48620835e+00, -2.05004717e+01,\n        8.72208178e+00,  3.31794699e+01,  6.58001679e+01,  9.56941365e+00,\n        7.09329532e+00,  3.09574896e+00, -2.58613293e+01,  9.27030159e+01,\n        3.29986045e+01,  1.44710089e+01,  6.18116411e+01,  4.38973803e+00,\n       -2.80096083e+01, -1.00957298e+01, -4.84681797e+01, -1.78712800e+01,\n        4.52941090e+01, -2.09570300e+01,  6.04377631e+01,  9.31711606e+01,\n       -2.80673581e+01,  1.96143061e+01, -6.21915756e+00, -3.35926662e+00,\n        6.74861337e+01, -2.03803000e+00, -4.29237935e+01,  1.21060346e+01,\n       -7.46443850e-02,  5.26703947e-01, -5.17715551e+01,  2.59680406e+01,\n        7.65367150e+01, -5.29957319e+01, -3.20184009e+01,  9.39394289e+00,\n       -6.48925309e+00, -1.41146759e+00,  1.02625424e+01, -6.58502270e+00,\n        3.33123314e+01, -2.49528560e+01,  3.27819296e+01, -2.24490186e+01,\n        7.48950932e+01, -3.32971936e+01,  3.61268363e+01,  1.14216504e+01,\n        5.64490868e+01,  1.62270737e+01,  4.49212715e+01,  1.02327506e+01,\n        7.19451806e+01,  3.57682997e+01, -4.41963925e+01,  4.20948740e+01,\n        4.29492734e+01,  7.23606640e+01, -4.34659390e+01,  2.51651709e+01,\n       -3.82394785e+01,  9.25208760e+00, -2.45838426e+01,  1.14173494e+02,\n       -1.30429391e+01,  4.22771785e+01,  1.59804706e+01,  1.59103270e+01,\n        1.82758061e+01,  6.84319982e+01,  2.98533513e+01, -4.00790051e+01,\n       -1.36458449e+01,  2.04895527e+00,  3.31751766e+01, -4.06678865e+01,\n        1.21895512e+01, -1.67939994e+01,  3.57683634e+01, -6.68726299e+01,\n        8.49213224e+01,  5.06948898e+01, -3.67866870e+01,  3.16476135e+01,\n        5.68607729e+01, -5.38996525e+01,  1.09789704e+01,  5.29242960e+01,\n       -3.54753592e+01,  4.04539985e+01,  6.47694491e+01,  3.84923885e+01,\n        8.56690294e+01,  6.09926019e+01,  1.54842406e+01,  1.25643026e+01,\n       -2.55314556e+01, -2.78711058e+01, -1.98147472e+01, -1.34730397e+01,\n       -1.76293345e+01,  2.85371234e+01, -1.70051048e+01, -8.22587392e+01,\n       -3.74459752e+00,  2.99824196e+00,  5.31439151e+01, -1.63400005e+01,\n        1.43707766e+01,  6.17495411e-01,  5.86040412e+01,  1.74622679e+00,\n        9.08712523e+01,  1.98452219e+01,  5.89093022e+01,  7.55339569e+00,\n        5.56639784e+00,  8.27670089e+01, -7.81238890e+01, -5.92967670e+01,\n        2.67537599e+01, -2.15283678e+00, -1.88319029e+01,  1.75976398e+01,\n        3.18037999e+01,  2.25715147e+01, -3.49097747e+01,  4.56143888e+01,\n       -3.10414868e+01,  6.06538406e+01,  2.06848344e+01,  5.20719123e+01,\n        1.08076610e+02,  7.56955207e+01, -2.70626461e+01,  3.33502391e+01,\n       -4.88638822e+01,  6.37556790e+01,  2.01771859e+01,  3.88784481e+00,\n       -1.81931660e+01, -3.13058121e+01,  6.06421100e-01, -6.45819332e+01,\n        1.71086323e+00,  4.75730265e+01, -3.69931439e+01,  8.22579786e+01,\n        6.86736278e+01,  4.16710130e+01,  2.60610570e-02,  1.27616376e+01,\n       -2.73163631e+01, -2.80588904e+01,  6.39193357e+01,  2.37848679e+00,\n        2.18350004e+01, -2.48209974e+00, -4.42135090e+01, -1.55060145e+01,\n        3.72423406e+01,  6.40901076e+01, -2.76557453e+00, -3.98418906e+01,\n       -2.35405403e+01,  2.01947729e+01,  7.85454867e+01,  8.17390703e+00,\n        2.80538724e+01,  4.58795870e+01,  1.01966641e+01, -3.04697975e+01,\n        3.06504973e+01,  1.73448996e+01,  5.65242799e+01, -5.50786198e+01,\n        2.47280539e+00,  6.17502543e+01, -4.17402218e+01, -5.12702425e+00,\n       -2.25551213e+01,  8.02582228e+00,  3.88664534e+01,  3.98066607e+01,\n        6.05045413e+00,  9.14359344e+00,  7.83298527e+01,  1.21436073e+01,\n       -4.78828478e+01, -2.23620593e+01,  2.21656262e+01, -2.42322886e+01,\n       -2.04128234e+01, -1.76389838e+01,  4.41841542e+01,  1.02151706e+02,\n       -1.03268697e+01, -3.42318045e+01, -9.12459188e+00, -8.60510386e+01,\n        3.26115700e+01, -3.12046372e+01, -5.84586012e+01, -4.43709616e+01,\n        6.24537182e+00, -3.30409804e+01, -4.23739045e+01, -6.36757574e+01,\n       -7.85115050e+00, -8.33503180e+00, -1.66904324e+01, -3.09368274e+01,\n        3.89234027e+01, -4.29058048e+01, -3.94028012e+00, -1.11572593e+01,\n       -1.18434845e+01, -6.55366371e+01, -1.81740417e+01, -7.96127193e+00,\n       -2.63031487e+01, -1.05814038e+01,  7.29691679e+01,  2.07003834e+01,\n       -2.83643656e+01, -1.42570220e+01,  2.44235966e+01, -1.85442295e+01,\n        2.24949455e+01,  2.65509265e+01,  1.70588385e+01,  2.51312068e+01,\n       -3.52976897e+01,  8.65870614e+01,  1.41751143e+00,  5.05125435e+01,\n       -6.05655800e-01,  9.90181951e+01,  2.50889440e+01,  5.53690795e+01,\n        3.86403049e+01, -1.25296086e+01, -4.22523026e+01,  2.06417358e+01,\n       -1.44128479e+01,  9.06194395e+00,  2.56259879e+01, -3.05431742e+01,\n       -6.82233407e+00, -2.07295521e+01,  7.51457249e-01, -6.92672925e+00,\n        8.91976858e+00, -2.81161723e+00,  8.50827974e+01,  6.00368079e+01,\n       -3.10847663e+01,  1.31313416e+01,  6.24852147e+01, -6.74678397e+01,\n        1.63008190e+01, -4.54394496e+00,  6.64257314e+01,  3.87156420e+01,\n       -2.57100364e+01,  2.54481316e+01,  3.01737547e+01,  3.52749710e+01,\n        4.68009449e+01, -3.16201961e+01,  6.67185630e+01,  1.61210335e+00,\n        4.65041703e+00,  9.47147380e-02, -2.69276395e+01,  1.57890314e+02,\n       -2.90219273e+01, -2.52523535e+01, -3.03511581e+00, -3.36698823e+01,\n       -4.19174502e+01, -3.45350145e+01,  5.30732357e+01,  1.83102807e+01,\n        2.00775529e+01, -4.38812756e+00, -4.89944738e+00,  5.98229846e+01,\n        7.28161971e+01,  7.28440241e+00, -2.89486012e+01,  6.01981509e+01,\n       -1.88287749e+01,  4.06455381e+01, -5.22764646e+01,  2.16521697e+01,\n       -4.71497922e+00,  4.15866009e+01, -4.72286748e+01,  4.83520353e+01,\n       -1.21697012e+01,  7.10768928e+01,  5.31735075e+01, -2.78721722e+01,\n       -3.53408379e+01,  1.25667527e+01,  3.98656487e+01,  4.15467500e+00,\n       -7.14302320e+01,  3.02403527e+01,  2.18025547e+01,  5.96454640e+00,\n        2.78864214e+00, -3.06485230e+00,  6.39964772e+00, -4.79305658e+00,\n       -2.89552636e+01,  3.18739467e+01,  2.05685314e+01, -2.31435885e+01,\n       -2.14315799e+01, -1.40097577e+01, -1.06078588e+01,  7.09479544e+01,\n       -6.66152753e+00,  8.72100914e+00, -9.30586272e+00, -6.06197326e+01,\n       -4.66408747e+01, -3.88762303e+01, -7.43169184e+01, -3.09360608e+01,\n       -2.94157675e+01,  1.53738934e+01, -3.86426278e+01,  1.83978464e+01,\n       -2.72206571e+01,  1.07099238e+01, -3.65558267e+01, -2.76151998e+01,\n       -3.06084740e+01, -1.38709412e+01,  1.03985310e+01,  1.02296628e+01,\n       -5.40974743e+00,  3.29573791e+01, -5.30386913e+01, -6.69262808e+01,\n        2.48666312e+01,  5.02466103e+01, -1.47376993e+01,  7.11251833e+00,\n        3.74710537e+01,  3.21755108e+01, -2.48540278e+01, -5.04601343e+01,\n        4.08762835e+01, -5.40936320e-02,  5.33886975e+01,  2.38798933e+01,\n        4.19430397e+01, -4.22301043e+01,  5.90024995e+01, -6.00797608e+01,\n        3.31905220e+00,  2.55801628e+01,  4.83860303e+01,  5.55991092e+01,\n        5.82968500e+01,  6.40129332e+01,  2.84779904e+01,  1.61180081e+01,\n        6.13350245e+01,  5.64862987e+01, -6.03238746e+00,  3.25309592e+01,\n        1.21992327e+01,  3.08764361e+01,  5.32956014e+01,  8.56060916e+01,\n        1.52029960e+01,  1.57862504e+01,  3.52775532e+01, -5.91865418e+01,\n        2.82262831e+01,  6.57421652e+01, -3.67693120e+01,  5.20818455e+01,\n        3.21378629e+00,  6.71646126e+01,  7.49774494e+01,  3.68347573e+01,\n        5.23130555e+00, -3.27806081e+01,  1.29908764e+01,  3.33051758e+01,\n       -9.80298639e+01,  6.34033431e+01,  9.09984865e+01,  4.75804139e+01,\n       -2.63768188e+01,  1.79901916e+01, -3.95844062e+00,  8.70339238e+00,\n       -1.03044475e+02,  7.16960372e+01,  1.93435791e+01,  5.87830311e+01,\n       -4.88234106e+01,  8.93640078e+01,  4.35443894e+01, -1.25137795e+01,\n       -2.22756311e+01,  4.00971885e+01,  3.80122566e+01, -2.28757010e+01,\n        4.70048925e+01, -2.07924888e+01, -1.37620428e+01, -7.38608549e+01,\n       -3.18595093e+01,  1.50072500e+00,  8.74348905e+01,  1.01892370e+02,\n       -1.45428832e+01, -1.28522570e+01,  2.72486734e+01,  2.55486714e+01,\n       -2.56725591e+01,  1.08055591e+01,  1.77978089e+01,  3.84826835e+01,\n        3.17491675e+01, -9.04467909e+00, -2.66029374e+00,  9.57831967e+00,\n       -1.53426409e+01,  5.53974226e+01,  5.45066361e+00,  1.34818891e+01,\n       -2.71216940e+00, -8.64324143e+00, -4.85020567e+01,  3.41607044e+01,\n        2.21576225e+01,  1.50625534e+01,  2.63169735e+01,  9.45637003e+01,\n        4.61619097e+01,  2.74901171e+01,  1.43650037e+01, -3.70178579e+01,\n       -4.95495703e+01,  2.79778839e+01, -4.28000532e+01,  8.32552438e+00,\n       -1.89854252e+01, -2.23932334e+01,  6.59960423e+00, -2.09932773e+00,\n       -5.39900362e+01, -6.71579908e+01, -1.52482992e+01,  3.08328605e+01,\n        1.26752069e+01, -4.98006109e+00, -1.90125135e+01, -6.41931317e+01,\n        5.63572455e+01, -6.17691719e+01,  1.14351364e+01,  1.83706237e+01,\n       -5.60139094e+01,  4.58082143e+01,  4.64218000e+01,  3.07505066e+01,\n        3.83916252e+01, -1.78518766e+01, -1.21889433e+01,  3.51818284e+01,\n       -7.01348761e+01,  4.29254779e+01, -1.32878621e+01, -6.82976673e+00])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}