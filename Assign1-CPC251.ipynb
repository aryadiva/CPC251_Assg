{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         f1        f2        f3        f4        f5   response\n0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075\n5 -0.699867  0.019159  1.103377 -0.671614 -0.119063 -18.597563\n6 -1.028250  0.962967  0.471027 -1.941219 -0.465591 -73.174734\n7  0.337585  1.352948 -1.789795 -0.885796 -0.846150 -25.865464\n8  0.295433 -0.907789  0.275980 -0.675526 -0.942592  -9.001596\n9  0.442269 -0.704559 -1.127342  1.030206  0.800113  57.076963",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.764216</td>\n      <td>-1.016209</td>\n      <td>0.149410</td>\n      <td>-0.050119</td>\n      <td>-0.578127</td>\n      <td>6.242514</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.763880</td>\n      <td>-1.159509</td>\n      <td>-0.721492</td>\n      <td>-0.654067</td>\n      <td>-0.431670</td>\n      <td>-8.118241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.519329</td>\n      <td>-0.664621</td>\n      <td>-1.694904</td>\n      <td>1.339779</td>\n      <td>0.182764</td>\n      <td>66.722455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.177388</td>\n      <td>0.515623</td>\n      <td>0.135144</td>\n      <td>-0.647634</td>\n      <td>-0.405631</td>\n      <td>-27.716793</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.104022</td>\n      <td>0.749665</td>\n      <td>-0.939338</td>\n      <td>-0.090725</td>\n      <td>-0.639963</td>\n      <td>8.192075</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.699867</td>\n      <td>0.019159</td>\n      <td>1.103377</td>\n      <td>-0.671614</td>\n      <td>-0.119063</td>\n      <td>-18.597563</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-1.028250</td>\n      <td>0.962967</td>\n      <td>0.471027</td>\n      <td>-1.941219</td>\n      <td>-0.465591</td>\n      <td>-73.174734</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.337585</td>\n      <td>1.352948</td>\n      <td>-1.789795</td>\n      <td>-0.885796</td>\n      <td>-0.846150</td>\n      <td>-25.865464</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.295433</td>\n      <td>-0.907789</td>\n      <td>0.275980</td>\n      <td>-0.675526</td>\n      <td>-0.942592</td>\n      <td>-9.001596</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.442269</td>\n      <td>-0.704559</td>\n      <td>-1.127342</td>\n      <td>1.030206</td>\n      <td>0.800113</td>\n      <td>57.076963</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset to be processed with pandas & displaying the top 10 result\n",
    "dt = pd.read_csv('assignment1_dataset.csv', sep=',')\n",
    "dt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                f1           f2           f3           f4           f5  \\\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \nmean      0.012255    -0.043030    -0.065785     0.039616     0.008074   \nstd       0.998816     1.042413     0.982640     1.023960     1.006679   \nmin      -3.174809    -3.381691    -3.158010    -2.764936    -2.946633   \n25%      -0.655282    -0.759477    -0.734505    -0.660802    -0.685371   \n50%      -0.001177    -0.038444    -0.049838    -0.006831    -0.000368   \n75%       0.697331     0.696343     0.591642     0.737806     0.710398   \nmax       3.092866     3.534175     3.406115     3.145835     3.007734   \n\n          response  \ncount  1000.000000  \nmean     11.229435  \nstd      40.028188  \nmin    -103.044475  \n25%     -16.580272  \n50%      10.554227  \n75%      38.485118  \nmax     157.890314  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.012255</td>\n      <td>-0.043030</td>\n      <td>-0.065785</td>\n      <td>0.039616</td>\n      <td>0.008074</td>\n      <td>11.229435</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.998816</td>\n      <td>1.042413</td>\n      <td>0.982640</td>\n      <td>1.023960</td>\n      <td>1.006679</td>\n      <td>40.028188</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-3.174809</td>\n      <td>-3.381691</td>\n      <td>-3.158010</td>\n      <td>-2.764936</td>\n      <td>-2.946633</td>\n      <td>-103.044475</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.655282</td>\n      <td>-0.759477</td>\n      <td>-0.734505</td>\n      <td>-0.660802</td>\n      <td>-0.685371</td>\n      <td>-16.580272</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.001177</td>\n      <td>-0.038444</td>\n      <td>-0.049838</td>\n      <td>-0.006831</td>\n      <td>-0.000368</td>\n      <td>10.554227</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.697331</td>\n      <td>0.696343</td>\n      <td>0.591642</td>\n      <td>0.737806</td>\n      <td>0.710398</td>\n      <td>38.485118</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.092866</td>\n      <td>3.534175</td>\n      <td>3.406115</td>\n      <td>3.145835</td>\n      <td>3.007734</td>\n      <td>157.890314</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying additional description\n",
    "dt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "f2         -0.031751\nf5         -0.028999\nf3          0.015218\nf1          0.308474\nf4          0.947255\nresponse    1.000000\nName: response, dtype: float64"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a correlation matrix between the columns/features and target in ascending order\n",
    "corr_matrix = dt.corr()\n",
    "corr_matrix['response'].sort_values(ascending=True)\n",
    "# Correlation between f4 and response are the closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'relationship between f4 & response')"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's plot f4 & response, because f4 corr value is close to 1\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(dt.f4, dt.response)\n",
    "plt.xlabel('f4')\n",
    "plt.ylabel('response')\n",
    "plt.title('relationship between f4 & response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'relationship between f1 & response')"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's also plot f1 & response, because f1 corr value is the second closest to 1\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(dt.f1, dt.response)\n",
    "plt.xlabel('f1')\n",
    "plt.ylabel('response')\n",
    "plt.title('relationship between f1 & response')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# our group has decided to compare the linear regression between f1 & f4 and shows the difference\n",
    "# Splitting the training and test set with the ratio of 8:2 for f4\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_f4 = np.array(dt.iloc[:, 3])\n",
    "y_f4 = np.array(dt.iloc[:, -1])\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X_f4, y_f4, random_state=42, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "# Splitting the training and test set with the ratio of 8:2 for f1\n",
    "X_f1 = np.array(dt.iloc[:, 0])\n",
    "y_f1 = np.array(dt.iloc[:, -1])\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_f1, y_f1, random_state=42, train_size=0.8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.15 # Set learning rate to 0.15\n",
    "max_epoch = 2000 # Set max iteration to 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "# the loss function accept y as y dataset and yhat as the predicted y value\n",
    "def loss_fn(y, yhat):\n",
    "    # loss = the sum of (y-yhat)^2/len(y)\n",
    "    # len(y) returns the number of elements in y\n",
    "    loss = np.sum((y-yhat)**2)/len(y)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "# train model accepts X as X dataset, y as y dataset, alpha as learning rate, max_epoch as max number of iterations\n",
    "def train_model(X, y, alpha, max_epoch):\n",
    "    # we initialize weight and bias as both 0\n",
    "    # assign len(X) to n\n",
    "    # creating an empty array to store losses and weights value to be displayed after each iteration later\n",
    "    w = b = 0\n",
    "    n = len(X)\n",
    "    losses = []\n",
    "    weights = []\n",
    "\n",
    "    # the prediction function accepts w as weight, and X as the X dataset\n",
    "    def prediction(w, X):\n",
    "        # yhat returns the predicted y value by using the general gradient formula\n",
    "        # y = mx + c\n",
    "        # but in this case m = w and b = c\n",
    "        yhat = (w * X) + b\n",
    "        return yhat;\n",
    "\n",
    "    # for loop will keep looping until max_epoch is reached\n",
    "    for i in range(max_epoch):\n",
    "        # assign prediction function to y_predict and loss_fn to loss\n",
    "        y_predict = prediction(w, X)\n",
    "        loss = loss_fn(y, y_predict)\n",
    "\n",
    "        # losses will append loss after each iteration\n",
    "        # weights will append w after each iteration\n",
    "        losses.append(loss)\n",
    "        weights.append(w)\n",
    "\n",
    "        # calling loss_fn to calculate the next loss value using y & y_predict\n",
    "        loss_fn(y, y_predict)\n",
    "\n",
    "        # calculate the derivatives of both w & b in wd & bd\n",
    "        wd = -(2/n)*sum(X*(y-y_predict))\n",
    "        bd = -(2/n)*sum(y-y_predict)\n",
    "\n",
    "        # w & b will update its own value using wb, bd and alpha\n",
    "        w = w - alpha * wd\n",
    "        b = b - alpha * bd\n",
    "\n",
    "        # will print the i number of iteration, loss, weight, and bias for that iteration\n",
    "        print(f\"Iteration {i+1}: Loss {loss}, Weight {w}, Bias {b}\");\n",
    "    # creating a plot to display the decreasing loss and increasing weight after each iteration\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(weights, losses)\n",
    "    plt.scatter(weights, losses, marker='o', color='red')\n",
    "    plt.title(\"Loss vs Weights\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Weight\")\n",
    "    plt.show()\n",
    "\n",
    "    # the function will return the estimated weight and bias\n",
    "    return w, b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss 1721.3069867699319, Weight 11.696102077957205, Bias 3.669599330511377\n",
      "Iteration 2: Loss 880.8238696125039, Weight 19.679364962567043, Bias 5.987867267407561\n",
      "Iteration 3: Loss 494.1386394965996, Weight 25.13239378048727, Bias 7.43970719426145\n",
      "Iteration 4: Loss 315.8860583169484, Weight 28.859946351231205, Bias 8.339228056640865\n",
      "Iteration 5: Loss 233.539957145164, Weight 31.40999672721068, Bias 8.889073633543482\n",
      "Iteration 6: Loss 195.4104133768699, Weight 33.15591093800564, Bias 9.219360663173202\n",
      "Iteration 7: Loss 177.7102979496032, Weight 34.35225495610511, Bias 9.413175880970186\n",
      "Iteration 8: Loss 169.47134217695225, Weight 35.17271580655756, Bias 9.523228916134661\n",
      "Iteration 9: Loss 165.62511199497047, Weight 35.73588308015578, Bias 9.582697304873543\n",
      "Iteration 10: Loss 163.82396516109745, Weight 36.12278728137225, Bias 9.612265933605121\n",
      "Iteration 11: Loss 162.97772420092355, Weight 36.38883785102429, Bias 9.624679096118454\n",
      "Iteration 12: Loss 162.57874964772344, Weight 36.57195428288922, Bias 9.627671301683716\n",
      "Iteration 13: Loss 162.3899632328638, Weight 36.69810794805331, Bias 9.625844727071108\n",
      "Iteration 14: Loss 162.30029677019212, Weight 36.78510173836004, Bias 9.6218647644765\n",
      "Iteration 15: Loss 162.25754323694943, Weight 36.845149682868595, Bias 9.61721597060746\n",
      "Iteration 16: Loss 162.23707720287345, Weight 36.88663882790536, Bias 9.612675993063268\n",
      "Iteration 17: Loss 162.22724073700977, Weight 36.91533351016493, Bias 9.6086095912174\n",
      "Iteration 18: Loss 162.2224939643714, Weight 36.93519913715687, Bias 9.605148663430505\n",
      "Iteration 19: Loss 162.22019407025476, Weight 36.94896614422218, Bias 9.60230062628026\n",
      "Iteration 20: Loss 162.21907527795017, Weight 36.958516395744944, Bias 9.600012203867918\n",
      "Iteration 21: Loss 162.21852890140693, Weight 36.96514815624741, Bias 9.59820580622594\n",
      "Iteration 22: Loss 162.21826105128002, Weight 36.96975794959221, Bias 9.5967993203102\n",
      "Iteration 23: Loss 162.21812925776024, Weight 36.972965486352784, Bias 9.595716069497401\n",
      "Iteration 24: Loss 162.21806417975338, Weight 36.9751995599702, Bias 9.594889110131854\n",
      "Iteration 25: Loss 162.21803193640054, Weight 36.97675716089835, Bias 9.594262399792084\n",
      "Iteration 26: Loss 162.21801591008477, Weight 36.97784419754151, Bias 9.593790349251291\n",
      "Iteration 27: Loss 162.21800792035035, Weight 36.97860357380796, Bias 9.59343663688193\n",
      "Iteration 28: Loss 162.21800392595296, Weight 36.97913456767773, Bias 9.593172777507164\n",
      "Iteration 29: Loss 162.21800192376546, Weight 36.97950621902726, Bias 9.59297670563868\n",
      "Iteration 30: Loss 162.2180009177433, Weight 36.97976658787136, Bias 9.592831497066317\n",
      "Iteration 31: Loss 162.21800041113045, Weight 36.97994916322101, Bias 9.592724275721697\n",
      "Iteration 32: Loss 162.2180001554898, Weight 36.980077303961906, Bias 9.592645311248285\n",
      "Iteration 33: Loss 162.21800002625147, Weight 36.98016731916892, Bias 9.592587292206774\n",
      "Iteration 34: Loss 162.21799996080486, Weight 36.980230606855535, Bias 9.592544751359304\n",
      "Iteration 35: Loss 162.21799992761171, Weight 36.98027514046877, Bias 9.592513617570821\n",
      "Iteration 36: Loss 162.21799991075352, Weight 36.98030650308951, Bias 9.59249087030935\n",
      "Iteration 37: Loss 162.21799990218094, Weight 36.98032860768009, Bias 9.592474275650583\n",
      "Iteration 38: Loss 162.2179998978167, Weight 36.98034419918664, Bias 9.592462186058253\n",
      "Iteration 39: Loss 162.21799989559273, Weight 36.980355204913394, Bias 9.592453389478745\n",
      "Iteration 40: Loss 162.21799989445836, Weight 36.98036297925867, Bias 9.592446996204677\n",
      "Iteration 41: Loss 162.21799989387932, Weight 36.980368474828076, Bias 9.592442354438806\n",
      "Iteration 42: Loss 162.21799989358348, Weight 36.98037236218677, Bias 9.592438987524677\n",
      "Iteration 43: Loss 162.2179998934323, Weight 36.98037511374591, Bias 9.592436547443791\n",
      "Iteration 44: Loss 162.21799989335497, Weight 36.980377062579294, Bias 9.59243478046734\n",
      "Iteration 45: Loss 162.2179998933154, Weight 36.980378443699834, Bias 9.59243350185296\n",
      "Iteration 46: Loss 162.21799989329517, Weight 36.98037942305219, Bias 9.592432577248612\n",
      "Iteration 47: Loss 162.21799989328477, Weight 36.98038011789493, Bias 9.592431909054447\n",
      "Iteration 48: Loss 162.21799989327945, Weight 36.98038061114148, Bias 9.592431426439688\n",
      "Iteration 49: Loss 162.21799989327673, Weight 36.98038096145877, Bias 9.592431078047344\n",
      "Iteration 50: Loss 162.21799989327533, Weight 36.98038121038424, Bias 9.59243082667127\n",
      "Iteration 51: Loss 162.21799989327462, Weight 36.98038138734534, Bias 9.592430645377714\n",
      "Iteration 52: Loss 162.21799989327425, Weight 36.98038151320244, Bias 9.592430514682912\n",
      "Iteration 53: Loss 162.21799989327408, Weight 36.9803816027513, Bias 9.592430420501541\n",
      "Iteration 54: Loss 162.217999893274, Weight 36.9803816664919, Bias 9.592430352657049\n",
      "Iteration 55: Loss 162.21799989327394, Weight 36.98038171187953, Bias 9.592430303801011\n",
      "Iteration 56: Loss 162.2179998932739, Weight 36.9803817442103, Bias 9.592430268629888\n",
      "Iteration 57: Loss 162.2179998932739, Weight 36.98038176724824, Bias 9.592430243317795\n",
      "Iteration 58: Loss 162.21799989327388, Weight 36.980381783669756, Bias 9.592430225106012\n",
      "Iteration 59: Loss 162.21799989327388, Weight 36.98038179537868, Bias 9.592430212006127\n",
      "Iteration 60: Loss 162.21799989327388, Weight 36.98038180372988, Bias 9.59243020258548\n",
      "Iteration 61: Loss 162.21799989327388, Weight 36.98038180968788, Bias 9.5924301958122\n",
      "Iteration 62: Loss 162.21799989327388, Weight 36.98038181393962, Bias 9.592430190943325\n",
      "Iteration 63: Loss 162.21799989327388, Weight 36.98038181697451, Bias 9.59243018744407\n",
      "Iteration 64: Loss 162.21799989327386, Weight 36.98038181914131, Bias 9.592430184929603\n",
      "Iteration 65: Loss 162.21799989327386, Weight 36.98038182068867, Bias 9.592430183123078\n",
      "Iteration 66: Loss 162.21799989327388, Weight 36.98038182179392, Bias 9.592430181825376\n",
      "Iteration 67: Loss 162.21799989327388, Weight 36.980381822583524, Bias 9.592430180893318\n",
      "Iteration 68: Loss 162.21799989327388, Weight 36.98038182314774, Bias 9.592430180223971\n",
      "Iteration 69: Loss 162.21799989327388, Weight 36.98038182355097, Bias 9.592430179743346\n",
      "Iteration 70: Loss 162.21799989327388, Weight 36.9803818238392, Bias 9.592430179398272\n",
      "Iteration 71: Loss 162.21799989327388, Weight 36.98038182404526, Bias 9.59243017915055\n",
      "Iteration 72: Loss 162.21799989327388, Weight 36.9803818241926, Bias 9.592430178972732\n",
      "Iteration 73: Loss 162.21799989327388, Weight 36.980381824297964, Bias 9.592430178845104\n",
      "Iteration 74: Loss 162.21799989327386, Weight 36.980381824373325, Bias 9.592430178753508\n",
      "Iteration 75: Loss 162.21799989327388, Weight 36.980381824427226, Bias 9.592430178687778\n",
      "Iteration 76: Loss 162.21799989327388, Weight 36.98038182446579, Bias 9.592430178640612\n",
      "Iteration 77: Loss 162.21799989327388, Weight 36.98038182449338, Bias 9.59243017860677\n",
      "Iteration 78: Loss 162.21799989327388, Weight 36.98038182451312, Bias 9.592430178582491\n",
      "Iteration 79: Loss 162.21799989327388, Weight 36.98038182452724, Bias 9.592430178565072\n",
      "Iteration 80: Loss 162.21799989327388, Weight 36.98038182453735, Bias 9.592430178552577\n",
      "Iteration 81: Loss 162.2179998932739, Weight 36.980381824544594, Bias 9.592430178543614\n",
      "Iteration 82: Loss 162.21799989327388, Weight 36.980381824549774, Bias 9.592430178537183\n",
      "Iteration 83: Loss 162.21799989327388, Weight 36.98038182455348, Bias 9.592430178532572\n",
      "Iteration 84: Loss 162.21799989327386, Weight 36.98038182455614, Bias 9.592430178529264\n",
      "Iteration 85: Loss 162.21799989327386, Weight 36.98038182455804, Bias 9.592430178526891\n",
      "Iteration 86: Loss 162.21799989327386, Weight 36.980381824559394, Bias 9.592430178525191\n",
      "Iteration 87: Loss 162.21799989327386, Weight 36.98038182456037, Bias 9.592430178523971\n",
      "Iteration 88: Loss 162.21799989327388, Weight 36.980381824561064, Bias 9.592430178523097\n",
      "Iteration 89: Loss 162.21799989327388, Weight 36.98038182456156, Bias 9.59243017852247\n",
      "Iteration 90: Loss 162.21799989327388, Weight 36.980381824561924, Bias 9.59243017852202\n",
      "Iteration 91: Loss 162.21799989327388, Weight 36.98038182456218, Bias 9.592430178521697\n",
      "Iteration 92: Loss 162.21799989327388, Weight 36.980381824562365, Bias 9.592430178521466\n",
      "Iteration 93: Loss 162.21799989327388, Weight 36.9803818245625, Bias 9.5924301785213\n",
      "Iteration 94: Loss 162.21799989327388, Weight 36.98038182456259, Bias 9.59243017852118\n",
      "Iteration 95: Loss 162.21799989327386, Weight 36.980381824562656, Bias 9.592430178521095\n",
      "Iteration 96: Loss 162.21799989327388, Weight 36.980381824562706, Bias 9.592430178521035\n",
      "Iteration 97: Loss 162.21799989327388, Weight 36.98038182456274, Bias 9.59243017852099\n",
      "Iteration 98: Loss 162.21799989327388, Weight 36.98038182456277, Bias 9.592430178520958\n",
      "Iteration 99: Loss 162.21799989327388, Weight 36.980381824562784, Bias 9.592430178520935\n",
      "Iteration 100: Loss 162.21799989327388, Weight 36.9803818245628, Bias 9.59243017852092\n",
      "Iteration 101: Loss 162.21799989327386, Weight 36.980381824562805, Bias 9.592430178520909\n",
      "Iteration 102: Loss 162.21799989327388, Weight 36.98038182456281, Bias 9.5924301785209\n",
      "Iteration 103: Loss 162.21799989327388, Weight 36.98038182456282, Bias 9.592430178520894\n",
      "Iteration 104: Loss 162.21799989327388, Weight 36.98038182456282, Bias 9.59243017852089\n",
      "Iteration 105: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520887\n",
      "Iteration 106: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520885\n",
      "Iteration 107: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520884\n",
      "Iteration 108: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 109: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 110: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 111: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 112: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 113: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 114: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 115: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 116: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 117: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 118: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 119: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 120: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 121: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 122: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 123: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 124: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 125: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 126: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 127: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 128: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 129: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 130: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 131: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 132: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 133: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 134: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 135: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 136: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 137: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 138: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 139: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 140: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 141: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 142: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 143: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 144: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 145: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 146: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 147: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 148: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 149: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 150: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 151: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 152: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 153: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 154: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 155: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 156: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 157: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 158: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 159: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 160: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 161: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 162: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 163: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 164: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 165: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 166: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 167: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 168: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 169: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 170: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 171: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 172: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 173: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 174: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 175: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 176: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 177: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 178: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 179: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 180: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 181: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 182: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 183: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 184: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 185: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 186: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 187: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 188: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 189: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 190: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 191: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 192: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 193: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 194: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 195: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 196: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 197: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 198: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 199: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 200: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 201: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 202: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 203: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 204: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 205: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 206: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 207: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 208: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 209: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 210: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 211: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 212: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 213: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 214: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 215: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 216: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 217: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 218: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 219: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 220: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 221: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 222: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 223: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 224: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 225: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 226: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 227: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 228: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 229: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 230: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 231: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 232: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 233: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 234: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 235: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 236: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 237: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 238: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 239: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 240: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 241: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 242: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 243: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 244: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 245: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 246: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 247: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 248: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 249: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 250: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 251: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 252: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 253: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 254: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 255: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 256: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 257: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 258: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 259: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 260: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 261: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 262: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 263: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 264: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 265: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 266: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 267: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 268: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 269: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 270: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 271: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 272: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 273: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 274: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 275: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 276: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 277: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 278: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 279: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 280: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 281: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 282: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 283: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 284: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 285: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 286: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 287: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 288: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 289: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 290: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 291: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 292: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 293: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 294: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 295: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 296: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 297: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 298: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 299: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 300: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 301: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 302: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 303: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 304: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 305: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 306: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 307: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 308: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 309: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 310: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 311: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 312: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 313: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 314: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 315: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 316: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 317: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 318: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 319: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 320: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 321: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 322: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 323: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 324: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 325: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 326: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 327: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 328: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 329: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 330: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 331: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 332: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 333: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 334: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 335: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 336: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 337: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 338: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 339: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 340: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 341: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 342: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 343: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 344: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 345: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 346: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 347: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 348: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 349: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 350: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 351: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 352: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 353: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 354: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 355: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 356: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 357: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 358: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 359: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 360: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 361: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 362: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 363: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 364: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 365: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 366: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 367: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 368: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 369: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 370: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 371: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 372: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 373: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 374: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 375: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 376: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 377: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 378: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 379: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 380: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 381: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 382: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 383: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 384: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 385: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 386: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 387: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 388: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 389: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 390: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 391: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 392: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 393: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 394: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 395: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 396: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 397: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 398: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 399: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 400: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 401: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 402: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 403: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 404: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 405: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 406: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 407: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 408: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 409: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 410: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 411: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 412: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 413: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 414: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 415: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 416: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 417: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 418: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 419: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 420: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 421: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 422: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 423: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 424: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 425: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 426: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 427: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 428: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 429: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 430: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 431: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 432: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 433: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 434: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 435: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 436: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 437: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 438: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 439: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 440: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 441: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 442: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 443: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 444: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 445: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 446: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 447: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 448: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 449: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 450: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 451: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 452: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 453: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 454: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 455: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 456: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 457: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 458: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 459: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 460: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 461: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 462: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 463: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 464: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 465: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 466: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 467: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 468: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 469: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 470: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 471: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 472: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 473: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 474: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 475: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 476: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 477: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 478: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 479: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 480: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 481: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 482: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 483: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 484: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 485: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 486: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 487: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 488: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 489: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 490: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 491: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 492: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 493: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 494: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 495: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 496: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 497: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 498: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 499: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 500: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 501: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 502: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 503: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 504: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 505: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 506: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 507: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 508: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 509: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 510: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 511: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 512: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 513: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 514: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 515: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 516: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 517: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 518: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 519: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 520: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 521: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 522: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 523: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 524: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 525: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 526: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 527: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 528: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 529: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 530: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 531: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 532: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 533: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 534: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 535: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 536: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 537: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 538: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 539: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 540: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 541: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 542: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 543: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 544: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 545: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 546: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 547: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 548: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 549: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 550: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 551: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 552: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 553: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 554: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 555: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 556: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 557: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 558: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 559: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 560: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 561: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 562: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 563: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 564: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 565: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 566: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 567: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 568: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 569: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 570: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 571: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 572: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 573: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 574: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 575: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 576: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 577: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 578: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 579: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 580: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 581: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 582: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 583: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 584: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 585: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 586: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 587: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 588: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 589: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 590: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 591: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 592: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 593: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 594: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 595: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 596: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 597: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 598: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 599: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 600: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 601: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 602: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 603: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 604: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 605: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 606: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 607: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 608: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 609: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 610: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 611: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 612: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 613: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 614: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 615: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 616: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 617: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 618: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 619: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 620: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 621: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 622: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 623: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 624: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 625: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 626: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 627: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 628: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 629: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 630: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 631: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 632: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 633: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 634: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 635: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 636: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 637: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 638: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 639: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 640: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 641: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 642: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 643: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 644: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 645: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 646: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 647: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 648: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 649: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 650: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 651: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 652: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 653: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 654: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 655: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 656: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 657: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 658: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 659: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 660: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 661: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 662: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 663: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 664: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 665: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 666: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 667: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 668: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 669: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 670: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 671: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 672: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 673: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 674: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 675: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 676: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 677: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 678: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 679: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 680: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 681: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 682: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 683: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 684: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 685: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 686: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 687: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 688: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 689: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 690: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 691: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 692: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 693: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 694: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 695: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 696: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 697: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 698: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 699: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 700: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 701: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 702: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 703: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 704: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 705: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 706: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 707: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 708: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 709: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 710: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 711: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 712: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 713: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 714: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 715: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 716: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 717: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 718: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 719: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 720: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 721: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 722: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 723: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 724: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 725: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 726: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 727: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 728: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 729: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 730: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 731: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 732: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 733: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 734: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 735: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 736: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 737: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 738: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 739: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 740: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 741: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 742: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 743: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 744: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 745: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 746: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 747: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 748: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 749: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 750: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 751: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 752: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 753: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 754: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 755: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 756: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 757: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 758: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 759: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 760: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 761: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 762: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 763: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 764: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 765: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 766: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 767: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 768: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 769: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 770: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 771: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 772: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 773: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 774: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 775: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 776: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 777: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 778: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 779: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 780: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 781: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 782: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 783: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 784: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 785: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 786: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 787: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 788: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 789: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 790: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 791: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 792: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 793: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 794: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 795: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 796: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 797: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 798: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 799: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 800: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 801: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 802: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 803: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 804: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 805: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 806: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 807: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 808: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 809: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 810: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 811: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 812: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 813: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 814: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 815: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 816: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 817: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 818: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 819: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 820: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 821: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 822: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 823: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 824: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 825: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 826: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 827: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 828: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 829: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 830: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 831: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 832: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 833: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 834: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 835: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 836: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 837: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 838: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 839: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 840: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 841: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 842: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 843: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 844: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 845: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 846: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 847: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 848: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 849: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 850: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 851: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 852: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 853: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 854: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 855: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 856: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 857: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 858: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 859: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 860: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 861: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 862: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 863: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 864: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 865: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 866: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 867: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 868: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 869: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 870: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 871: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 872: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 873: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 874: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 875: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 876: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 877: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 878: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 879: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 880: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 881: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 882: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 883: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 884: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 885: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 886: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 887: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 888: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 889: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 890: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 891: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 892: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 893: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 894: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 895: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 896: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 897: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 898: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 899: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 900: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 901: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 902: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 903: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 904: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 905: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 906: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 907: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 908: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 909: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 910: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 911: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 912: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 913: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 914: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 915: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 916: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 917: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 918: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 919: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 920: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 921: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 922: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 923: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 924: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 925: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 926: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 927: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 928: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 929: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 930: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 931: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 932: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 933: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 934: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 935: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 936: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 937: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 938: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 939: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 940: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 941: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 942: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 943: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 944: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 945: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 946: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 947: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 948: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 949: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 950: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 951: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 952: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 953: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 954: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 955: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 956: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 957: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 958: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 959: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 960: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 961: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 962: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 963: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 964: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 965: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 966: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 967: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 968: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 969: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 970: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 971: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 972: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 973: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 974: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 975: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 976: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 977: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 978: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 979: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 980: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 981: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 982: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 983: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 984: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 985: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 986: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 987: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 988: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 989: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 990: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 991: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 992: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 993: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 994: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 995: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 996: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 997: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 998: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 999: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1000: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1001: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1002: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1003: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1004: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1005: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1006: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1007: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1008: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1009: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1010: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1011: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1012: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1013: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1014: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1015: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1016: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1017: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1018: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1019: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1020: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1021: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1022: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1023: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1024: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1025: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1026: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1027: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1028: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1029: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1030: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1031: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1032: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1033: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1034: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1035: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1036: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1037: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1038: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1039: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1040: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1041: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1042: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1043: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1044: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1045: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1046: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1047: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1048: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1049: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1050: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1051: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1052: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1053: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1054: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1055: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1056: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1057: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1058: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1059: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1060: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1061: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1062: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1063: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1064: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1065: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1066: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1067: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1068: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1069: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1070: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1071: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1072: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1073: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1074: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1075: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1076: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1077: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1078: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1079: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1080: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1081: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1082: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1083: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1084: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1085: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1086: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1087: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1088: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1089: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1090: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1091: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1092: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1093: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1094: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1095: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1096: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1097: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1098: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1099: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1100: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1101: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1102: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1103: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1104: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1105: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1106: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1107: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1108: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1109: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1110: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1111: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1112: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1113: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1114: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1115: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1116: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1117: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1118: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1119: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1120: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1121: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1122: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1123: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1124: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1125: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1126: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1127: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1128: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1129: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1130: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1131: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1132: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1133: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1134: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1135: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1136: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1137: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1138: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1139: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1140: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1141: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1142: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1143: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1144: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1145: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1146: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1147: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1148: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1149: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1150: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1151: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1152: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1153: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1154: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1155: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1156: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1157: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1158: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1159: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1160: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1161: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1162: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1163: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1164: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1165: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1166: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1167: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1168: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1169: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1170: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1171: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1172: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1173: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1174: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1175: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1176: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1177: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1178: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1179: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1180: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1181: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1182: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1183: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1184: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1185: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1186: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1187: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1188: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1189: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1190: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1191: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1192: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1193: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1194: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1195: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1196: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1197: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1198: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1199: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1200: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1201: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1202: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1203: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1204: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1205: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1206: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1207: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1208: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1209: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1210: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1211: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1212: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1213: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1214: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1215: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1216: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1217: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1218: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1219: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1220: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1221: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1222: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1223: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1224: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1225: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1226: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1227: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1228: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1229: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1230: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1231: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1232: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1233: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1234: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1235: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1236: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1237: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1238: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1239: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1240: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1241: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1242: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1243: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1244: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1245: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1246: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1247: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1248: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1249: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1250: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1251: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1252: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1253: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1254: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1255: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1256: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1257: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1258: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1259: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1260: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1261: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1262: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1263: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1264: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1265: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1266: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1267: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1268: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1269: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1270: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1271: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1272: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1273: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1274: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1275: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1276: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1277: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1278: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1279: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1280: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1281: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1282: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1283: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1284: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1285: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1286: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1287: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1288: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1289: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1290: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1291: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1292: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1293: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1294: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1295: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1296: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1297: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1298: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1299: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1300: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1301: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1302: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1303: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1304: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1305: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1306: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1307: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1308: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1309: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1310: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1311: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1312: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1313: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1314: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1315: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1316: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1317: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1318: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1319: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1320: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1321: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1322: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1323: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1324: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1325: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1326: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1327: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1328: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1329: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1330: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1331: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1332: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1333: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1334: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1335: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1336: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1337: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1338: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1339: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1340: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1341: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1342: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1343: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1344: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1345: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1346: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1347: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1348: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1349: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1350: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1351: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1352: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1353: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1354: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1355: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1356: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1357: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1358: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1359: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1360: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1361: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1362: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1363: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1364: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1365: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1366: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1367: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1368: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1369: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1370: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1371: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1372: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1373: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1374: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1375: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1376: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1377: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1378: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1379: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1380: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1381: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1382: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1383: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1384: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1385: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1386: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1387: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1388: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1389: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1390: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1391: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1392: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1393: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1394: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1395: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1396: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1397: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1398: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1399: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1400: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1401: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1402: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1403: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1404: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1405: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1406: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1407: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1408: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1409: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1410: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1411: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1412: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1413: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1414: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1415: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1416: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1417: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1418: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1419: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1420: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1421: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1422: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1423: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1424: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1425: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1426: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1427: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1428: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1429: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1430: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1431: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1432: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1433: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1434: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1435: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1436: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1437: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1438: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1439: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1440: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1441: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1442: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1443: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1444: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1445: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1446: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1447: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1448: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1449: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1450: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1451: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1452: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1453: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1454: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1455: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1456: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1457: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1458: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1459: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1460: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1461: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1462: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1463: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1464: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1465: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1466: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1467: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1468: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1469: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1470: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1471: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1472: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1473: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1474: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1475: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1476: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1477: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1478: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1479: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1480: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1481: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1482: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1483: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1484: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1485: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1486: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1487: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1488: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1489: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1490: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1491: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1492: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1493: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1494: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1495: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1496: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1497: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1498: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1499: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1500: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1501: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1502: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1503: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1504: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1505: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1506: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1507: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1508: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1509: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1510: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1511: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1512: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1513: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1514: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1515: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1516: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1517: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1518: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1519: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1520: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1521: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1522: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1523: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1524: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1525: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1526: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1527: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1528: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1529: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1530: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1531: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1532: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1533: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1534: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1535: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1536: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1537: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1538: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1539: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1540: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1541: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1542: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1543: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1544: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1545: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1546: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1547: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1548: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1549: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1550: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1551: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1552: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1553: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1554: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1555: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1556: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1557: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1558: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1559: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1560: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1561: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1562: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1563: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1564: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1565: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1566: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1567: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1568: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1569: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1570: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1571: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1572: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1573: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1574: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1575: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1576: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1577: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1578: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1579: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1580: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1581: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1582: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1583: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1584: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1585: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1586: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1587: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1588: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1589: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1590: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1591: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1592: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1593: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1594: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1595: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1596: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1597: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1598: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1599: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1600: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1601: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1602: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1603: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1604: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1605: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1606: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1607: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1608: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1609: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1610: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1611: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1612: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1613: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1614: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1615: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1616: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1617: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1618: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1619: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1620: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1621: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1622: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1623: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1624: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1625: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1626: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1627: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1628: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1629: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1630: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1631: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1632: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1633: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1634: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1635: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1636: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1637: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1638: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1639: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1640: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1641: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1642: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1643: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1644: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1645: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1646: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1647: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1648: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1649: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1650: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1651: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1652: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1653: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1654: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1655: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1656: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1657: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1658: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1659: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1660: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1661: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1662: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1663: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1664: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1665: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1666: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1667: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1668: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1669: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1670: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1671: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1672: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1673: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1674: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1675: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1676: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1677: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1678: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1679: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1680: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1681: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1682: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1683: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1684: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1685: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1686: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1687: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1688: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1689: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1690: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1691: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1692: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1693: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1694: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1695: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1696: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1697: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1698: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1699: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1700: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1701: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1702: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1703: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1704: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1705: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1706: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1707: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1708: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1709: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1710: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1711: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1712: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1713: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1714: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1715: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1716: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1717: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1718: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1719: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1720: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1721: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1722: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1723: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1724: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1725: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1726: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1727: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1728: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1729: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1730: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1731: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1732: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1733: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1734: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1735: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1736: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1737: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1738: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1739: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1740: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1741: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1742: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1743: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1744: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1745: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1746: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1747: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1748: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1749: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1750: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1751: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1752: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1753: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1754: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1755: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1756: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1757: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1758: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1759: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1760: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1761: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1762: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1763: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1764: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1765: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1766: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1767: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1768: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1769: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1770: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1771: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1772: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1773: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1774: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1775: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1776: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1777: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1778: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1779: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1780: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1781: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1782: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1783: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1784: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1785: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1786: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1787: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1788: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1789: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1790: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1791: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1792: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1793: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1794: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1795: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1796: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1797: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1798: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1799: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1800: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1801: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1802: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1803: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1804: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1805: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1806: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1807: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1808: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1809: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1810: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1811: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1812: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1813: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1814: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1815: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1816: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1817: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1818: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1819: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1820: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1821: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1822: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1823: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1824: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1825: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1826: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1827: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1828: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1829: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1830: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1831: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1832: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1833: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1834: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1835: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1836: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1837: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1838: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1839: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1840: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1841: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1842: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1843: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1844: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1845: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1846: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1847: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1848: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1849: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1850: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1851: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1852: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1853: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1854: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1855: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1856: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1857: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1858: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1859: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1860: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1861: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1862: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1863: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1864: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1865: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1866: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1867: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1868: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1869: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1870: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1871: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1872: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1873: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1874: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1875: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1876: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1877: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1878: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1879: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1880: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1881: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1882: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1883: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1884: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1885: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1886: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1887: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1888: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1889: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1890: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1891: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1892: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1893: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1894: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1895: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1896: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1897: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1898: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1899: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1900: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1901: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1902: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1903: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1904: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1905: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1906: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1907: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1908: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1909: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1910: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1911: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1912: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1913: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1914: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1915: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1916: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1917: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1918: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1919: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1920: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1921: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1922: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1923: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1924: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1925: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1926: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1927: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1928: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1929: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1930: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1931: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1932: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1933: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1934: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1935: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1936: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1937: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1938: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1939: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1940: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1941: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1942: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1943: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1944: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1945: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1946: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1947: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1948: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1949: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1950: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1951: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1952: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1953: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1954: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1955: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1956: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1957: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1958: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1959: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1960: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1961: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1962: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1963: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1964: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1965: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1966: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1967: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1968: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1969: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1970: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1971: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1972: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1973: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1974: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1975: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1976: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1977: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1978: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1979: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1980: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1981: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1982: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1983: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1984: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1985: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1986: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1987: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1988: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1989: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1990: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1991: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1992: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1993: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1994: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1995: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1996: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1997: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1998: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 1999: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n",
      "Iteration 2000: Loss 162.21799989327386, Weight 36.98038182456282, Bias 9.592430178520882\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Weight: 36.98038182456282\n",
      "Estimated Bias: 9.592430178520882\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we'll input the training dataset from f4 to train_model\n",
    "# and have est_weight & est_bias as the final estimated value\n",
    "est_weight, est_bias = train_model(X_train4, y_train4, alpha, max_epoch)\n",
    "print(f\"Estimated Weight: {est_weight}\\nEstimated Bias: {est_bias}\")\n",
    "\n",
    "# y_pred4 is the predicted y value using both estimated weight & bias and the X test dataset\n",
    "y_pred4 = (est_weight*X_test4) + est_bias\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "# scatter the y test & y pred value and plot them\n",
    "plt.scatter(y_test4, y_pred4, marker='o', color='red')\n",
    "plt.plot([min(y_test4), max(y_test4)], [min(y_pred4), max(y_pred4)], color='blue', label=\"line1\")\n",
    "\n",
    "plt.title(\"LG: Gradient Descent f4\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.593441909037288\n",
      "173.2860954934168\n",
      "0.8978521452417096\n"
     ]
    }
   ],
   "source": [
    "# this is to measure the error using mean absolute error, mean squared error, and r2 score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "print(mean_absolute_error(y_test4, y_pred4))\n",
    "print(mean_squared_error(y_test4, y_pred4))\n",
    "print(r2_score(y_test4, y_pred4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss 1721.3069867699319, Weight 3.689212084502532, Bias 3.669599330511377\n",
      "Iteration 2: Loss 1567.7214115819804, Weight 6.285064632618403, Bias 6.237393813416817\n",
      "Iteration 3: Loss 1492.0972346050717, Weight 8.111596555002121, Bias 8.034199056536591\n",
      "Iteration 4: Loss 1454.8594169289017, Weight 9.396810295080039, Bias 9.29150473447451\n",
      "Iteration 5: Loss 1436.522719225072, Weight 10.301134790810663, Bias 10.17129644916792\n",
      "Iteration 6: Loss 1427.4930593949791, Weight 10.93745258429452, Bias 10.786923895350755\n",
      "Iteration 7: Loss 1423.046388073608, Weight 11.385191325817603, Bias 11.217703554696506\n",
      "Iteration 8: Loss 1420.8565497132304, Weight 11.70023887866654, Bias 11.519137048355717\n",
      "Iteration 9: Loss 1419.77809356029, Weight 11.921919827995715, Bias 11.730061497577417\n",
      "Iteration 10: Loss 1419.246956821867, Weight 12.077904332033048, Bias 11.877653026825135\n",
      "Iteration 11: Loss 1418.9853653264365, Weight 12.187662123075512, Bias 11.98092798509225\n",
      "Iteration 12: Loss 1418.8565242518182, Weight 12.26489284123329, Bias 12.053192934752111\n",
      "Iteration 13: Loss 1418.7930644799735, Weight 12.3192360900634, Bias 12.103759034361053\n",
      "Iteration 14: Loss 1418.7618068436577, Weight 12.35747468532845, Bias 12.139141677834042\n",
      "Iteration 15: Loss 1418.7464101584826, Weight 12.384381300216972, Bias 12.163899940159986\n",
      "Iteration 16: Loss 1418.738825923129, Weight 12.403314193807518, Bias 12.181223977111108\n",
      "Iteration 17: Loss 1418.7350898974144, Weight 12.416636386760086, Bias 12.193346055664048\n",
      "Iteration 18: Loss 1418.7332494585253, Weight 12.426010608783264, Bias 12.201828170188781\n",
      "Iteration 19: Loss 1418.7323427943816, Weight 12.432606834944929, Bias 12.207763299824498\n",
      "Iteration 20: Loss 1418.7318961262376, Weight 12.437248316433388, Bias 12.21191623660419\n",
      "Iteration 21: Loss 1418.7316760683084, Weight 12.440514333776544, Bias 12.214822128525512\n",
      "Iteration 22: Loss 1418.731567650014, Weight 12.442812498349392, Bias 12.216855433935603\n",
      "Iteration 23: Loss 1418.7315142327518, Weight 12.44442962686666, Bias 12.21827817147128\n",
      "Iteration 24: Loss 1418.7314879134606, Weight 12.445567538961278, Bias 12.219273682260692\n",
      "Iteration 25: Loss 1418.7314749452437, Weight 12.446368246016005, Bias 12.219970254488448\n",
      "Iteration 26: Loss 1418.7314685552585, Weight 12.446931675223315, Bias 12.220457654275245\n",
      "Iteration 27: Loss 1418.7314654065465, Weight 12.447328141100051, Bias 12.220798692849408\n",
      "Iteration 28: Loss 1418.7314638549474, Weight 12.447607121070842, Bias 12.221037320439812\n",
      "Iteration 29: Loss 1418.731463090338, Weight 12.447803430420212, Bias 12.221204289800491\n",
      "Iteration 30: Loss 1418.7314627135358, Weight 12.447941567314652, Bias 12.22132111912954\n",
      "Iteration 31: Loss 1418.7314625278407, Weight 12.448038770194945, Bias 12.221402865022853\n",
      "Iteration 32: Loss 1418.7314624363241, Weight 12.44810716912652, Bias 12.221460062775115\n",
      "Iteration 33: Loss 1418.73146239122, Weight 12.448155299611297, Bias 12.221500084051065\n",
      "Iteration 34: Loss 1418.7314623689902, Weight 12.448189167793284, Bias 12.221528086875791\n",
      "Iteration 35: Loss 1418.7314623580332, Weight 12.448213000000184, Bias 12.22154768036085\n",
      "Iteration 36: Loss 1418.7314623526327, Weight 12.44822977016612, Bias 12.221561389824604\n",
      "Iteration 37: Loss 1418.7314623499706, Weight 12.448241570958757, Bias 12.221570982244211\n",
      "Iteration 38: Loss 1418.7314623486586, Weight 12.448249874927669, Bias 12.221577693978956\n",
      "Iteration 39: Loss 1418.7314623480115, Weight 12.448255718265058, Bias 12.221582390111104\n",
      "Iteration 40: Loss 1418.731462347693, Weight 12.448259830111812, Bias 12.221585675938426\n",
      "Iteration 41: Loss 1418.7314623475354, Weight 12.448262723545836, Bias 12.22158797498653\n",
      "Iteration 42: Loss 1418.731462347458, Weight 12.448264759607607, Bias 12.22158958359469\n",
      "Iteration 43: Loss 1418.73146234742, Weight 12.448266192352978, Bias 12.221590709109872\n",
      "Iteration 44: Loss 1418.731462347401, Weight 12.448267200555486, Bias 12.221591496611246\n",
      "Iteration 45: Loss 1418.7314623473917, Weight 12.44826791001436, Bias 12.221592047609407\n",
      "Iteration 46: Loss 1418.7314623473874, Weight 12.448268409252051, Bias 12.221592433130228\n",
      "Iteration 47: Loss 1418.7314623473846, Weight 12.448268760560174, Bias 12.221592702869621\n",
      "Iteration 48: Loss 1418.7314623473837, Weight 12.448269007772259, Bias 12.221592891599109\n",
      "Iteration 49: Loss 1418.7314623473833, Weight 12.448269181733254, Bias 12.221593023647761\n",
      "Iteration 50: Loss 1418.7314623473828, Weight 12.448269304148283, Bias 12.2215931160382\n",
      "Iteration 51: Loss 1418.7314623473826, Weight 12.448269390290942, Bias 12.221593180680811\n",
      "Iteration 52: Loss 1418.7314623473826, Weight 12.448269450909065, Bias 12.22159322590904\n",
      "Iteration 53: Loss 1418.7314623473828, Weight 12.448269493565773, Bias 12.2215932575536\n",
      "Iteration 54: Loss 1418.7314623473826, Weight 12.448269523583159, Bias 12.221593279694096\n",
      "Iteration 55: Loss 1418.7314623473826, Weight 12.448269544706326, Bias 12.221593295184917\n",
      "Iteration 56: Loss 1418.7314623473828, Weight 12.448269559570674, Bias 12.221593306023195\n",
      "Iteration 57: Loss 1418.7314623473828, Weight 12.448269570030714, Bias 12.221593313606263\n",
      "Iteration 58: Loss 1418.7314623473826, Weight 12.448269577391455, Bias 12.221593318911788\n",
      "Iteration 59: Loss 1418.7314623473826, Weight 12.448269582571223, Bias 12.221593322623809\n",
      "Iteration 60: Loss 1418.7314623473826, Weight 12.448269586216242, Bias 12.221593325220924\n",
      "Iteration 61: Loss 1418.7314623473826, Weight 12.448269588781256, Bias 12.221593327037992\n",
      "Iteration 62: Loss 1418.7314623473826, Weight 12.44826959058627, Bias 12.221593328309297\n",
      "Iteration 63: Loss 1418.7314623473826, Weight 12.44826959185647, Bias 12.221593329198756\n",
      "Iteration 64: Loss 1418.7314623473828, Weight 12.448269592750318, Bias 12.22159332982106\n",
      "Iteration 65: Loss 1418.7314623473826, Weight 12.448269593379326, Bias 12.221593330256448\n",
      "Iteration 66: Loss 1418.7314623473828, Weight 12.448269593821964, Bias 12.221593330561063\n",
      "Iteration 67: Loss 1418.7314623473828, Weight 12.448269594133453, Bias 12.221593330774182\n",
      "Iteration 68: Loss 1418.7314623473828, Weight 12.448269594352652, Bias 12.221593330923287\n",
      "Iteration 69: Loss 1418.7314623473826, Weight 12.448269594506904, Bias 12.221593331027606\n",
      "Iteration 70: Loss 1418.7314623473824, Weight 12.448269594615454, Bias 12.221593331100589\n",
      "Iteration 71: Loss 1418.7314623473828, Weight 12.448269594691842, Bias 12.22159333115165\n",
      "Iteration 72: Loss 1418.7314623473826, Weight 12.448269594745597, Bias 12.221593331187375\n",
      "Iteration 73: Loss 1418.7314623473824, Weight 12.448269594783426, Bias 12.221593331212368\n",
      "Iteration 74: Loss 1418.7314623473828, Weight 12.448269594810046, Bias 12.221593331229855\n",
      "Iteration 75: Loss 1418.7314623473826, Weight 12.44826959482878, Bias 12.221593331242088\n",
      "Iteration 76: Loss 1418.7314623473826, Weight 12.448269594841964, Bias 12.221593331250647\n",
      "Iteration 77: Loss 1418.7314623473826, Weight 12.448269594851242, Bias 12.221593331256635\n",
      "Iteration 78: Loss 1418.7314623473826, Weight 12.44826959485777, Bias 12.221593331260824\n",
      "Iteration 79: Loss 1418.7314623473826, Weight 12.448269594862364, Bias 12.221593331263755\n",
      "Iteration 80: Loss 1418.7314623473826, Weight 12.448269594865597, Bias 12.221593331265804\n",
      "Iteration 81: Loss 1418.7314623473826, Weight 12.448269594867872, Bias 12.22159333126724\n",
      "Iteration 82: Loss 1418.7314623473826, Weight 12.448269594869474, Bias 12.221593331268243\n",
      "Iteration 83: Loss 1418.7314623473826, Weight 12.4482695948706, Bias 12.221593331268945\n",
      "Iteration 84: Loss 1418.7314623473828, Weight 12.448269594871393, Bias 12.221593331269435\n",
      "Iteration 85: Loss 1418.7314623473828, Weight 12.44826959487195, Bias 12.22159333126978\n",
      "Iteration 86: Loss 1418.7314623473826, Weight 12.448269594872343, Bias 12.22159333127002\n",
      "Iteration 87: Loss 1418.7314623473826, Weight 12.44826959487262, Bias 12.221593331270189\n",
      "Iteration 88: Loss 1418.7314623473826, Weight 12.448269594872814, Bias 12.221593331270306\n",
      "Iteration 89: Loss 1418.7314623473826, Weight 12.44826959487295, Bias 12.221593331270387\n",
      "Iteration 90: Loss 1418.7314623473826, Weight 12.448269594873047, Bias 12.221593331270446\n",
      "Iteration 91: Loss 1418.7314623473826, Weight 12.448269594873116, Bias 12.221593331270487\n",
      "Iteration 92: Loss 1418.7314623473826, Weight 12.448269594873164, Bias 12.221593331270515\n",
      "Iteration 93: Loss 1418.7314623473826, Weight 12.448269594873198, Bias 12.221593331270535\n",
      "Iteration 94: Loss 1418.7314623473828, Weight 12.44826959487322, Bias 12.22159333127055\n",
      "Iteration 95: Loss 1418.7314623473828, Weight 12.448269594873237, Bias 12.221593331270558\n",
      "Iteration 96: Loss 1418.7314623473826, Weight 12.448269594873247, Bias 12.221593331270565\n",
      "Iteration 97: Loss 1418.7314623473826, Weight 12.448269594873256, Bias 12.221593331270569\n",
      "Iteration 98: Loss 1418.7314623473826, Weight 12.448269594873262, Bias 12.221593331270572\n",
      "Iteration 99: Loss 1418.7314623473826, Weight 12.448269594873267, Bias 12.221593331270574\n",
      "Iteration 100: Loss 1418.7314623473826, Weight 12.44826959487327, Bias 12.221593331270576\n",
      "Iteration 101: Loss 1418.7314623473826, Weight 12.448269594873272, Bias 12.221593331270578\n",
      "Iteration 102: Loss 1418.7314623473826, Weight 12.448269594873274, Bias 12.221593331270578\n",
      "Iteration 103: Loss 1418.7314623473828, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 104: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 105: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 106: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 107: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 108: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 109: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 110: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 111: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 112: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 113: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 114: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 115: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 116: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 117: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 118: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 119: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 120: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 121: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 122: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 123: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 124: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 125: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 126: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 127: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 128: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 129: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 130: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 131: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 132: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 133: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 134: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 135: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 136: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 137: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 138: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 139: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 140: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 141: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 142: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 143: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 144: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 145: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 146: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 147: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 148: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 149: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 150: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 151: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 152: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 153: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 154: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 155: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 156: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 157: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 158: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 159: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 160: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 161: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 162: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 163: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 164: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 165: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 166: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 167: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 168: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 169: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 170: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 171: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 172: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 173: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 174: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 175: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 176: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 177: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 178: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 179: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 180: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 181: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 182: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 183: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 184: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 185: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 186: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 187: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 188: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 189: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 190: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 191: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 192: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 193: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 194: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 195: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 196: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 197: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 198: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 199: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 200: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 201: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 202: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 203: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 204: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 205: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 206: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 207: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 208: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 209: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 210: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 211: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 212: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 213: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 214: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 215: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 216: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 217: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 218: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 219: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 220: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 221: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 222: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 223: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 224: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 225: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 226: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 227: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 228: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 229: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 230: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 231: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 232: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 233: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 234: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 235: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 236: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 237: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 238: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 239: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 240: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 241: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 242: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 243: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 244: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 245: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 246: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 247: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 248: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 249: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 250: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 251: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 252: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 253: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 254: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 255: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 256: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 257: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 258: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 259: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 260: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 261: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 262: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 263: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 264: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 265: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 266: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 267: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 268: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 269: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 270: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 271: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 272: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 273: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 274: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 275: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 276: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 277: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 278: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 279: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 280: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 281: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 282: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 283: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 284: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 285: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 286: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 287: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 288: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 289: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 290: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 291: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 292: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 293: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 294: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 295: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 296: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 297: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 298: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 299: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 300: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 301: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 302: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 303: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 304: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 305: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 306: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 307: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 308: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 309: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 310: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 311: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 312: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 313: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 314: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 315: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 316: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 317: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 318: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 319: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 320: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 321: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 322: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 323: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 324: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 325: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 326: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 327: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 328: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 329: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 330: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 331: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 332: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 333: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 334: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 335: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 336: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 337: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 338: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 339: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 340: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 341: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 342: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 343: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 344: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 345: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 346: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 347: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 348: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 349: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 350: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 351: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 352: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 353: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 354: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 355: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 356: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 357: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 358: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 359: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 360: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 361: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 362: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 363: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 364: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 365: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 366: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 367: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 368: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 369: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 370: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 371: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 372: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 373: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 374: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 375: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 376: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 377: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 378: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 379: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 380: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 381: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 382: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 383: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 384: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 385: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 386: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 387: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 388: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 389: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 390: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 391: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 392: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 393: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 394: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 395: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 396: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 397: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 398: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 399: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 400: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 401: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 402: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 403: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 404: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 405: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 406: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 407: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 408: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 409: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 410: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 411: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 412: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 413: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 414: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 415: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 416: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 417: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 418: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 419: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 420: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 421: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 422: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 423: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 424: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 425: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 426: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 427: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 428: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 429: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 430: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 431: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 432: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 433: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 434: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 435: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 436: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 437: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 438: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 439: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 440: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 441: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 442: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 443: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 444: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 445: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 446: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 447: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 448: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 449: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 450: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 451: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 452: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 453: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 454: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 455: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 456: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 457: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 458: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 459: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 460: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 461: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 462: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 463: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 464: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 465: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 466: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 467: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 468: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 469: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 470: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 471: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 472: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 473: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 474: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 475: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 476: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 477: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 478: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 479: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 480: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 481: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 482: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 483: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 484: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 485: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 486: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 487: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 488: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 489: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 490: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 491: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 492: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 493: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 494: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 495: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 496: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 497: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 498: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 499: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 500: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 501: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 502: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 503: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 504: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 505: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 506: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 507: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 508: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 509: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 510: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 511: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 512: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 513: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 514: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 515: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 516: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 517: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 518: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 519: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 520: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 521: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 522: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 523: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 524: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 525: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 526: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 527: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 528: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 529: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 530: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 531: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 532: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 533: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 534: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 535: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 536: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 537: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 538: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 539: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 540: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 541: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 542: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 543: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 544: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 545: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 546: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 547: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 548: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 549: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 550: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 551: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 552: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 553: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 554: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 555: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 556: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 557: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 558: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 559: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 560: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 561: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 562: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 563: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 564: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 565: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 566: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 567: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 568: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 569: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 570: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 571: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 572: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 573: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 574: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 575: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 576: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 577: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 578: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 579: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 580: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 581: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 582: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 583: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 584: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 585: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 586: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 587: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 588: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 589: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 590: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 591: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 592: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 593: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 594: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 595: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 596: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 597: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 598: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 599: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 600: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 601: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 602: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 603: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 604: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 605: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 606: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 607: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 608: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 609: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 610: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 611: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 612: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 613: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 614: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 615: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 616: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 617: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 618: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 619: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 620: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 621: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 622: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 623: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 624: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 625: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 626: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 627: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 628: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 629: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 630: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 631: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 632: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 633: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 634: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 635: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 636: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 637: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 638: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 639: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 640: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 641: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 642: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 643: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 644: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 645: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 646: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 647: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 648: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 649: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 650: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 651: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 652: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 653: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 654: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 655: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 656: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 657: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 658: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 659: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 660: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 661: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 662: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 663: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 664: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 665: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 666: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 667: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 668: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 669: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 670: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 671: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 672: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 673: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 674: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 675: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 676: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 677: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 678: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 679: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 680: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 681: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 682: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 683: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 684: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 685: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 686: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 687: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 688: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 689: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 690: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 691: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 692: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 693: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 694: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 695: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 696: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 697: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 698: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 699: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 700: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 701: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 702: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 703: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 704: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 705: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 706: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 707: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 708: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 709: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 710: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 711: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 712: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 713: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 714: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 715: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 716: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 717: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 718: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 719: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 720: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 721: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 722: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 723: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 724: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 725: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 726: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 727: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 728: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 729: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 730: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 731: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 732: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 733: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 734: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 735: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 736: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 737: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 738: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 739: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 740: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 741: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 742: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 743: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 744: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 745: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 746: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 747: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 748: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 749: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 750: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 751: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 752: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 753: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 754: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 755: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 756: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 757: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 758: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 759: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 760: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 761: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 762: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 763: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 764: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 765: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 766: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 767: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 768: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 769: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 770: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 771: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 772: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 773: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 774: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 775: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 776: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 777: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 778: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 779: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 780: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 781: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 782: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 783: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 784: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 785: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 786: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 787: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 788: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 789: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 790: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 791: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 792: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 793: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 794: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 795: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 796: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 797: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 798: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 799: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 800: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 801: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 802: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 803: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 804: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 805: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 806: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 807: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 808: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 809: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 810: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 811: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 812: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 813: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 814: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 815: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 816: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 817: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 818: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 819: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 820: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 821: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 822: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 823: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 824: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 825: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 826: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 827: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 828: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 829: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 830: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 831: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 832: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 833: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 834: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 835: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 836: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 837: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 838: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 839: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 840: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 841: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 842: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 843: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 844: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 845: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 846: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 847: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 848: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 849: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 850: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 851: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 852: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 853: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 854: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 855: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 856: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 857: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 858: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 859: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 860: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 861: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 862: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 863: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 864: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 865: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 866: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 867: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 868: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 869: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 870: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 871: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 872: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 873: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 874: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 875: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 876: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 877: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 878: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 879: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 880: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 881: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 882: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 883: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 884: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 885: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 886: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 887: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 888: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 889: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 890: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 891: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 892: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 893: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 894: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 895: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 896: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 897: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 898: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 899: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 900: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 901: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 902: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 903: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 904: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 905: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 906: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 907: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 908: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 909: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 910: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 911: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 912: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 913: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 914: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 915: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 916: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 917: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 918: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 919: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 920: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 921: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 922: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 923: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 924: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 925: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 926: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 927: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 928: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 929: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 930: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 931: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 932: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 933: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 934: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 935: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 936: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 937: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 938: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 939: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 940: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 941: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 942: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 943: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 944: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 945: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 946: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 947: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 948: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 949: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 950: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 951: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 952: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 953: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 954: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 955: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 956: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 957: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 958: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 959: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 960: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 961: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 962: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 963: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 964: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 965: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 966: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 967: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 968: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 969: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 970: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 971: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 972: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 973: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 974: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 975: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 976: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 977: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 978: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 979: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 980: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 981: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 982: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 983: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 984: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 985: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 986: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 987: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 988: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 989: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 990: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 991: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 992: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 993: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 994: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 995: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 996: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 997: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 998: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 999: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1000: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1001: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1002: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1003: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1004: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1005: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1006: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1007: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1008: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1009: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1010: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1011: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1012: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1013: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1014: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1015: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1016: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1017: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1018: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1019: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1020: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1021: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1022: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1023: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1024: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1025: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1026: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1027: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1028: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1029: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1030: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1031: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1032: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1033: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1034: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1035: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1036: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1037: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1038: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1039: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1040: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1041: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1042: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1043: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1044: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1045: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1046: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1047: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1048: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1049: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1050: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1051: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1052: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1053: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1054: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1055: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1056: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1057: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1058: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1059: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1060: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1061: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1062: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1063: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1064: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1065: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1066: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1067: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1068: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1069: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1070: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1071: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1072: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1073: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1074: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1075: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1076: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1077: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1078: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1079: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1080: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1081: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1082: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1083: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1084: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1085: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1086: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1087: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1088: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1089: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1090: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1091: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1092: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1093: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1094: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1095: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1096: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1097: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1098: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1099: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1100: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1101: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1102: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1103: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1104: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1105: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1106: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1107: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1108: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1109: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1110: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1111: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1112: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1113: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1114: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1115: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1116: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1117: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1118: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1119: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1120: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1121: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1122: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1123: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1124: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1125: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1126: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1127: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1128: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1129: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1130: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1131: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1132: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1133: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1134: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1135: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1136: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1137: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1138: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1139: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1140: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1141: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1142: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1143: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1144: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1145: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1146: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1147: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1148: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1149: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1150: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1151: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1152: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1153: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1154: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1155: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1156: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1157: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1158: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1159: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1160: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1161: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1162: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1163: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1164: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1165: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1166: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1167: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1168: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1169: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1170: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1171: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1172: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1173: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1174: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1175: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1176: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1177: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1178: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1179: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1180: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1181: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1182: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1183: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1184: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1185: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1186: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1187: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1188: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1189: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1190: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1191: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1192: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1193: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1194: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1195: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1196: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1197: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1198: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1199: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1200: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1201: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1202: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1203: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1204: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1205: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1206: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1207: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1208: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1209: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1210: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1211: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1212: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1213: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1214: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1215: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1216: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1217: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1218: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1219: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1220: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1221: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1222: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1223: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1224: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1225: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1226: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1227: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1228: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1229: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1230: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1231: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1232: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1233: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1234: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1235: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1236: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1237: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1238: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1239: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1240: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1241: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1242: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1243: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1244: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1245: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1246: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1247: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1248: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1249: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1250: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1251: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1252: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1253: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1254: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1255: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1256: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1257: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1258: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1259: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1260: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1261: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1262: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1263: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1264: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1265: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1266: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1267: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1268: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1269: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1270: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1271: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1272: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1273: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1274: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1275: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1276: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1277: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1278: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1279: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1280: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1281: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1282: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1283: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1284: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1285: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1286: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1287: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1288: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1289: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1290: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1291: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1292: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1293: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1294: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1295: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1296: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1297: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1298: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1299: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1300: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1301: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1302: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1303: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1304: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1305: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1306: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1307: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1308: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1309: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1310: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1311: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1312: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1313: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1314: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1315: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1316: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1317: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1318: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1319: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1320: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1321: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1322: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1323: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1324: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1325: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1326: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1327: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1328: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1329: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1330: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1331: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1332: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1333: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1334: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1335: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1336: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1337: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1338: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1339: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1340: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1341: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1342: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1343: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1344: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1345: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1346: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1347: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1348: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1349: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1350: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1351: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1352: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1353: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1354: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1355: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1356: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1357: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1358: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1359: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1360: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1361: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1362: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1363: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1364: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1365: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1366: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1367: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1368: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1369: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1370: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1371: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1372: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1373: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1374: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1375: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1376: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1377: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1378: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1379: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1380: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1381: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1382: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1383: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1384: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1385: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1386: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1387: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1388: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1389: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1390: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1391: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1392: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1393: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1394: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1395: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1396: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1397: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1398: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1399: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1400: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1401: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1402: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1403: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1404: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1405: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1406: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1407: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1408: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1409: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1410: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1411: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1412: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1413: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1414: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1415: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1416: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1417: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1418: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1419: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1420: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1421: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1422: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1423: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1424: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1425: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1426: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1427: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1428: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1429: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1430: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1431: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1432: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1433: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1434: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1435: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1436: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1437: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1438: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1439: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1440: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1441: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1442: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1443: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1444: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1445: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1446: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1447: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1448: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1449: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1450: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1451: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1452: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1453: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1454: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1455: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1456: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1457: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1458: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1459: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1460: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1461: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1462: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1463: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1464: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1465: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1466: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1467: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1468: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1469: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1470: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1471: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1472: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1473: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1474: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1475: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1476: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1477: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1478: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1479: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1480: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1481: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1482: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1483: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1484: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1485: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1486: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1487: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1488: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1489: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1490: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1491: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1492: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1493: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1494: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1495: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1496: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1497: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1498: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1499: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1500: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1501: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1502: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1503: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1504: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1505: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1506: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1507: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1508: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1509: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1510: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1511: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1512: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1513: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1514: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1515: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1516: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1517: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1518: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1519: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1520: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1521: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1522: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1523: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1524: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1525: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1526: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1527: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1528: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1529: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1530: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1531: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1532: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1533: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1534: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1535: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1536: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1537: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1538: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1539: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1540: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1541: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1542: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1543: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1544: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1545: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1546: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1547: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1548: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1549: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1550: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1551: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1552: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1553: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1554: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1555: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1556: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1557: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1558: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1559: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1560: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1561: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1562: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1563: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1564: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1565: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1566: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1567: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1568: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1569: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1570: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1571: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1572: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1573: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1574: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1575: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1576: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1577: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1578: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1579: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1580: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1581: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1582: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1583: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1584: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1585: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1586: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1587: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1588: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1589: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1590: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1591: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1592: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1593: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1594: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1595: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1596: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1597: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1598: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1599: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1600: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1601: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1602: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1603: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1604: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1605: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1606: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1607: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1608: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1609: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1610: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1611: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1612: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1613: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1614: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1615: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1616: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1617: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1618: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1619: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1620: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1621: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1622: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1623: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1624: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1625: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1626: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1627: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1628: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1629: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1630: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1631: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1632: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1633: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1634: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1635: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1636: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1637: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1638: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1639: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1640: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1641: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1642: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1643: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1644: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1645: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1646: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1647: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1648: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1649: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1650: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1651: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1652: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1653: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1654: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1655: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1656: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1657: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1658: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1659: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1660: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1661: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1662: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1663: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1664: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1665: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1666: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1667: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1668: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1669: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1670: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1671: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1672: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1673: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1674: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1675: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1676: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1677: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1678: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1679: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1680: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1681: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1682: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1683: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1684: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1685: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1686: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1687: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1688: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1689: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1690: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1691: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1692: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1693: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1694: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1695: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1696: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1697: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1698: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1699: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1700: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1701: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1702: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1703: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1704: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1705: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1706: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1707: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1708: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1709: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1710: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1711: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1712: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1713: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1714: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1715: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1716: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1717: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1718: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1719: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1720: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1721: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1722: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1723: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1724: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1725: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1726: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1727: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1728: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1729: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1730: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1731: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1732: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1733: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1734: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1735: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1736: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1737: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1738: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1739: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1740: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1741: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1742: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1743: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1744: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1745: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1746: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1747: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1748: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1749: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1750: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1751: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1752: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1753: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1754: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1755: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1756: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1757: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1758: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1759: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1760: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1761: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1762: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1763: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1764: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1765: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1766: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1767: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1768: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1769: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1770: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1771: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1772: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1773: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1774: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1775: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1776: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1777: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1778: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1779: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1780: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1781: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1782: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1783: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1784: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1785: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1786: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1787: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1788: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1789: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1790: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1791: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1792: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1793: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1794: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1795: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1796: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1797: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1798: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1799: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1800: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1801: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1802: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1803: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1804: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1805: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1806: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1807: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1808: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1809: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1810: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1811: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1812: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1813: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1814: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1815: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1816: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1817: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1818: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1819: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1820: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1821: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1822: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1823: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1824: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1825: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1826: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1827: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1828: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1829: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1830: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1831: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1832: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1833: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1834: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1835: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1836: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1837: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1838: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1839: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1840: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1841: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1842: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1843: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1844: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1845: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1846: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1847: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1848: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1849: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1850: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1851: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1852: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1853: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1854: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1855: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1856: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1857: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1858: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1859: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1860: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1861: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1862: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1863: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1864: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1865: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1866: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1867: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1868: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1869: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1870: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1871: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1872: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1873: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1874: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1875: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1876: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1877: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1878: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1879: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1880: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1881: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1882: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1883: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1884: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1885: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1886: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1887: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1888: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1889: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1890: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1891: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1892: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1893: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1894: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1895: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1896: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1897: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1898: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1899: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1900: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1901: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1902: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1903: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1904: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1905: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1906: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1907: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1908: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1909: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1910: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1911: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1912: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1913: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1914: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1915: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1916: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1917: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1918: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1919: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1920: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1921: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1922: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1923: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1924: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1925: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1926: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1927: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1928: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1929: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1930: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1931: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1932: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1933: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1934: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1935: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1936: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1937: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1938: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1939: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1940: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1941: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1942: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1943: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1944: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1945: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1946: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1947: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1948: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1949: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1950: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1951: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1952: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1953: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1954: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1955: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1956: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1957: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1958: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1959: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1960: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1961: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1962: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1963: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1964: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1965: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1966: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1967: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1968: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1969: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1970: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1971: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1972: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1973: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1974: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1975: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1976: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1977: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1978: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1979: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1980: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1981: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1982: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1983: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1984: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1985: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1986: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1987: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1988: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1989: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1990: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1991: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1992: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1993: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1994: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1995: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1996: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1997: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1998: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 1999: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n",
      "Iteration 2000: Loss 1418.7314623473826, Weight 12.448269594873276, Bias 12.221593331270578\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Weight: 12.448269594873276\n",
      "Estimated Bias: 12.221593331270578\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we'll input the training dataset from f1 to train_model\n",
    "# and have est_weight & est_bias as the final estimated value\n",
    "est_weight1, est_bias1 = train_model(X_train1, y_train1, alpha, max_epoch)\n",
    "print(f\"Estimated Weight: {est_weight1}\\nEstimated Bias: {est_bias1}\")\n",
    "\n",
    "# y_pred1 is the predicted y value using both estimated weight & bias and the X test dataset\n",
    "y_pred1 = (est_weight1*X_test1) + est_bias1\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "# scatter the y test & y pred value and plot them\n",
    "plt.scatter(y_test1, y_pred1, marker='o', color='red')\n",
    "plt.plot([min(y_test1), max(y_test1)], [min(y_pred1), max(y_pred1)], color='blue', label=\"line1\")\n",
    "\n",
    "plt.title(\"LG: Gradient Descent f1\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.437113026637256\n",
      "1573.368635769852\n",
      "0.07253821819781281\n"
     ]
    }
   ],
   "source": [
    "# this is to measure the error using mean absolute error, mean squared error, and r2 score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "print(mean_absolute_error(y_test1, y_pred1))\n",
    "print(mean_squared_error(y_test1, y_pred1))\n",
    "print(r2_score(y_test1, y_pred1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}