{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         f1        f2        f3        f4        f5   response\n0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075\n5 -0.699867  0.019159  1.103377 -0.671614 -0.119063 -18.597563\n6 -1.028250  0.962967  0.471027 -1.941219 -0.465591 -73.174734\n7  0.337585  1.352948 -1.789795 -0.885796 -0.846150 -25.865464\n8  0.295433 -0.907789  0.275980 -0.675526 -0.942592  -9.001596\n9  0.442269 -0.704559 -1.127342  1.030206  0.800113  57.076963",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.764216</td>\n      <td>-1.016209</td>\n      <td>0.149410</td>\n      <td>-0.050119</td>\n      <td>-0.578127</td>\n      <td>6.242514</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.763880</td>\n      <td>-1.159509</td>\n      <td>-0.721492</td>\n      <td>-0.654067</td>\n      <td>-0.431670</td>\n      <td>-8.118241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.519329</td>\n      <td>-0.664621</td>\n      <td>-1.694904</td>\n      <td>1.339779</td>\n      <td>0.182764</td>\n      <td>66.722455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.177388</td>\n      <td>0.515623</td>\n      <td>0.135144</td>\n      <td>-0.647634</td>\n      <td>-0.405631</td>\n      <td>-27.716793</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.104022</td>\n      <td>0.749665</td>\n      <td>-0.939338</td>\n      <td>-0.090725</td>\n      <td>-0.639963</td>\n      <td>8.192075</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.699867</td>\n      <td>0.019159</td>\n      <td>1.103377</td>\n      <td>-0.671614</td>\n      <td>-0.119063</td>\n      <td>-18.597563</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-1.028250</td>\n      <td>0.962967</td>\n      <td>0.471027</td>\n      <td>-1.941219</td>\n      <td>-0.465591</td>\n      <td>-73.174734</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.337585</td>\n      <td>1.352948</td>\n      <td>-1.789795</td>\n      <td>-0.885796</td>\n      <td>-0.846150</td>\n      <td>-25.865464</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.295433</td>\n      <td>-0.907789</td>\n      <td>0.275980</td>\n      <td>-0.675526</td>\n      <td>-0.942592</td>\n      <td>-9.001596</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.442269</td>\n      <td>-0.704559</td>\n      <td>-1.127342</td>\n      <td>1.030206</td>\n      <td>0.800113</td>\n      <td>57.076963</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset to be processed with pandas & displaying the top 10 result\n",
    "dt = pd.read_csv('assignment1_dataset.csv', sep=',')\n",
    "dt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                f1           f2           f3           f4           f5  \\\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \nmean      0.012255    -0.043030    -0.065785     0.039616     0.008074   \nstd       0.998816     1.042413     0.982640     1.023960     1.006679   \nmin      -3.174809    -3.381691    -3.158010    -2.764936    -2.946633   \n25%      -0.655282    -0.759477    -0.734505    -0.660802    -0.685371   \n50%      -0.001177    -0.038444    -0.049838    -0.006831    -0.000368   \n75%       0.697331     0.696343     0.591642     0.737806     0.710398   \nmax       3.092866     3.534175     3.406115     3.145835     3.007734   \n\n          response  \ncount  1000.000000  \nmean     11.229435  \nstd      40.028188  \nmin    -103.044475  \n25%     -16.580272  \n50%      10.554227  \n75%      38.485118  \nmax     157.890314  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.012255</td>\n      <td>-0.043030</td>\n      <td>-0.065785</td>\n      <td>0.039616</td>\n      <td>0.008074</td>\n      <td>11.229435</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.998816</td>\n      <td>1.042413</td>\n      <td>0.982640</td>\n      <td>1.023960</td>\n      <td>1.006679</td>\n      <td>40.028188</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-3.174809</td>\n      <td>-3.381691</td>\n      <td>-3.158010</td>\n      <td>-2.764936</td>\n      <td>-2.946633</td>\n      <td>-103.044475</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.655282</td>\n      <td>-0.759477</td>\n      <td>-0.734505</td>\n      <td>-0.660802</td>\n      <td>-0.685371</td>\n      <td>-16.580272</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.001177</td>\n      <td>-0.038444</td>\n      <td>-0.049838</td>\n      <td>-0.006831</td>\n      <td>-0.000368</td>\n      <td>10.554227</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.697331</td>\n      <td>0.696343</td>\n      <td>0.591642</td>\n      <td>0.737806</td>\n      <td>0.710398</td>\n      <td>38.485118</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.092866</td>\n      <td>3.534175</td>\n      <td>3.406115</td>\n      <td>3.145835</td>\n      <td>3.007734</td>\n      <td>157.890314</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying additional description\n",
    "dt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "f2         -0.031751\nf5         -0.028999\nf3          0.015218\nf1          0.308474\nf4          0.947255\nresponse    1.000000\nName: response, dtype: float64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a correlation matrix between the columns/features and target in ascending order\n",
    "corr_matrix = dt.corr()\n",
    "corr_matrix['response'].sort_values(ascending=True)\n",
    "# Correlation between f4 and response are the closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'relationship between f4 & response')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's plot f4 & response, cuz f4 corr value is close to 1\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(dt.f4, dt.response)\n",
    "plt.xlabel('f4')\n",
    "plt.ylabel('response')\n",
    "plt.title('relationship between f4 & response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         f1        f2        f3        f4        f5   response\n0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.764216</td>\n      <td>-1.016209</td>\n      <td>0.149410</td>\n      <td>-0.050119</td>\n      <td>-0.578127</td>\n      <td>6.242514</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.763880</td>\n      <td>-1.159509</td>\n      <td>-0.721492</td>\n      <td>-0.654067</td>\n      <td>-0.431670</td>\n      <td>-8.118241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.519329</td>\n      <td>-0.664621</td>\n      <td>-1.694904</td>\n      <td>1.339779</td>\n      <td>0.182764</td>\n      <td>66.722455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.177388</td>\n      <td>0.515623</td>\n      <td>0.135144</td>\n      <td>-0.647634</td>\n      <td>-0.405631</td>\n      <td>-27.716793</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.104022</td>\n      <td>0.749665</td>\n      <td>-0.939338</td>\n      <td>-0.090725</td>\n      <td>-0.639963</td>\n      <td>8.192075</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine each column to be processed\n",
    "columns = ['f1','f2','f3','f4','f5','response']\n",
    "dt = dt.loc[:, columns]\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the training and test set with the ratio of 8:2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "features = ['f1','f2','f3','f4','f5'] # Data that we want to utilize as training & test\n",
    "#X = dt.loc[:, features] # X are the data we want to use from 'features' = independent variable\n",
    "#y = dt.loc[:, ['response']] # y is the data we want to use as target = dependent variable\n",
    "\n",
    "X_data = np.array(dt.iloc[:,4])\n",
    "y_data = np.array(dt.iloc[:,-1])\n",
    "\n",
    "#X = dt[['f1','f2','f3','f4','f5']]\n",
    "#y = dt['response']\n",
    "#y = np.array((y-y.mean())/y.std())\n",
    "#X = X.apply(lambda rec:(rec-rec.mean())/rec.std(),axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=42, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.43233578e-01  1.29708222e-01 -5.05474951e-01 -1.45673015e+00\n",
      " -2.87227313e-01  1.02388856e+00 -6.30259835e-01  6.47133487e-01\n",
      " -4.51004082e-01 -1.25182980e+00 -5.37871808e-01 -8.54856516e-01\n",
      "  9.53672927e-01  1.14084691e+00 -1.24760250e-01  8.44818951e-01\n",
      "  2.80146667e-01 -1.45314479e+00  1.59063828e-01  1.37371403e+00\n",
      " -2.21643200e-01 -1.12204740e+00  6.34058371e-01  2.10690554e+00\n",
      "  1.01310187e+00 -6.50705320e-01  5.65847620e-01  1.15138314e+00\n",
      " -1.33014896e+00  5.59302961e-01 -1.58885135e+00 -8.00119911e-01\n",
      "  5.19013886e-01  1.19656250e+00 -1.77125048e+00 -2.20357270e-02\n",
      " -7.91072058e-01  5.66733883e-01 -7.44129727e-01  7.47275491e-01\n",
      "  9.14018250e-01 -2.16927780e+00  1.20780102e+00 -8.26788157e-01\n",
      " -6.40783958e-01 -6.01851880e-02 -3.46261000e-04  7.99828126e-01\n",
      " -5.44331315e-01  6.58181930e-02  9.77098785e-01  1.16675177e+00\n",
      "  9.54000000e-05 -1.01367488e+00  6.21544516e-01 -8.46025188e-01\n",
      " -2.73838080e-02  8.55309243e-01  2.60936170e-01 -3.90571000e-04\n",
      "  1.50731816e+00  4.88832872e-01  1.22789944e+00  1.12329830e+00\n",
      " -1.76298486e+00 -2.17497534e+00  5.84699331e-01  1.16853441e+00\n",
      "  4.99692070e-02 -7.26402818e-01  2.51235055e-01  7.83101446e-01\n",
      " -5.55211603e-01  1.08106378e+00 -7.03911174e-01 -2.02602740e+00\n",
      " -7.69127200e-01 -2.41834030e-02  8.37936395e-01  1.59743491e+00\n",
      "  4.12881789e-01 -1.06440437e+00  2.63913589e-01 -8.02571617e-01\n",
      "  1.22629307e+00  4.08989790e-02 -2.00138650e-01  1.05053870e-02\n",
      " -1.09048142e+00 -6.43312938e-01  1.09057727e+00 -8.74027247e-01\n",
      "  6.33029099e-01 -1.46862980e-02 -1.02204765e+00  2.28478650e-02\n",
      " -2.60222900e-01 -1.33829154e-01  4.59418695e-01  1.24161849e+00\n",
      "  6.22094979e-01  1.53224836e+00  6.38300448e-01  7.46010449e-01\n",
      " -7.44180695e-01 -6.62743940e-02 -3.53848587e-01 -1.49828218e+00\n",
      " -3.33476890e-02  6.28614675e-01  3.35892736e-01  1.47194462e+00\n",
      "  1.49539087e+00 -9.45311963e-01  4.43781395e-01  1.27711982e+00\n",
      " -6.34115487e-01 -9.72576918e-01  6.78997801e-01 -1.83870326e-01\n",
      " -1.77417142e+00  4.98388862e-01  2.47776525e-01 -6.05436078e-01\n",
      "  2.13061780e+00 -2.42011300e-03 -9.12972546e-01 -7.85679636e-01\n",
      "  4.73409901e-01 -7.33525750e-01 -2.65990735e+00  1.86451516e+00\n",
      " -1.42291351e+00  4.30062768e-01 -1.71464901e+00  5.93636270e-02\n",
      " -5.37821548e-01 -1.73544688e+00  1.06748872e+00  1.38224967e+00\n",
      " -3.36426597e-01  1.44145057e-01  8.76084090e-02  9.91543455e-01\n",
      "  5.50265343e-01  7.82265427e-01  1.11287157e+00 -3.31299190e-01\n",
      "  2.18240407e-01 -1.81963196e-01  6.12313719e-01  1.03844092e-01\n",
      " -2.99125360e-01 -1.11050819e-01 -5.22266200e-02  8.03656170e-02\n",
      "  6.57882435e-01  2.45330541e-01  9.04772040e-01 -8.74078744e-01\n",
      "  6.49126590e-01 -6.82447804e-01 -1.33307323e+00  1.25671463e+00\n",
      "  1.15939679e+00 -1.67231506e+00 -1.59374355e+00  4.29808229e-01\n",
      "  7.29526753e-01  1.53974521e+00 -1.34436430e+00  1.56559415e-01\n",
      " -9.68412234e-01 -4.61751874e-01 -1.60124715e-01 -3.88180302e-01\n",
      "  5.79467802e-01  1.78878456e+00 -3.17013494e-01 -7.85438107e-01\n",
      "  3.00773425e+00  8.75802676e-01 -1.19182762e+00 -1.95613891e+00\n",
      " -1.10720048e+00  1.92165305e+00  3.12903550e-02 -1.76578663e+00\n",
      " -1.79890143e-01 -1.29499697e+00  7.78856321e-01  1.80907311e+00\n",
      "  2.07775026e+00 -4.58695614e-01 -7.39881474e-01  4.44583925e-01\n",
      "  1.08175252e+00 -1.61078333e+00 -5.24773405e-01  5.08284917e-01]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "#print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.1 # Set learning rate to 0.1\n",
    "max_epoch = 1500 # Set max iteration to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) (800,)\n",
      "(200,) (200,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def loss_fn(y, yhat):\n",
    "    loss = np.sum((y-yhat)**2)/len(y)\n",
    "    return loss\n",
    "    #loss = mean_squared_error(y, yhat)\n",
    "    #return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train_model(X, y, alpha, max_epoch):\n",
    "    w = b = 0\n",
    "    n = len(X)\n",
    "    losses = []\n",
    "    weights = []\n",
    "\n",
    "    def prediction(w, X):\n",
    "            yhat = (w * X) + b\n",
    "            return yhat;\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        y_predict = prediction(w, X)\n",
    "        loss = loss_fn(y, y_predict)\n",
    "\n",
    "        losses.append(loss)\n",
    "        weights.append(w)\n",
    "\n",
    "        loss_fn(y, y_predict)\n",
    "\n",
    "        wd = -(2/n)*sum(X*(y-y_predict))\n",
    "        bd = -(2/n)*sum(y-y_predict)\n",
    "\n",
    "        w = w - alpha * wd\n",
    "        b = b - alpha * bd\n",
    "\n",
    "        print(f\"Iteration {i+1}: Loss {loss}, Weight {w}, Bias {b}\");\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(weights, losses)\n",
    "    plt.scatter(weights, losses, marker='o', color='red')\n",
    "    plt.title(\"Loss vs Weights\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Weight\")\n",
    "    plt.show()\n",
    "\n",
    "    return w, b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss 1721.3069867699319, Weight -0.20427783377291586, Bias 2.4463995536742518\n",
      "Iteration 2: Loss 1667.0651748347786, Weight -0.368581914746362, Bias 4.403642592126182\n",
      "Iteration 3: Loss 1632.3434607882796, Weight -0.5007281326604676, Bias 5.969536271963891\n",
      "Iteration 4: Loss 1610.1170947995126, Weight -0.6070054786864416, Bias 7.222331039718946\n",
      "Iteration 5: Loss 1595.8893488516874, Weight -0.6924740662765135, Bias 8.224631051527505\n",
      "Iteration 6: Loss 1586.7817462015198, Weight -0.7612050405279487, Bias 9.026522688896852\n",
      "Iteration 7: Loss 1580.9516948796704, Weight -0.8164736955755654, Bias 9.668077516237656\n",
      "Iteration 8: Loss 1577.219700264532, Weight -0.8609149170429552, Bias 10.181354763543707\n",
      "Iteration 9: Loss 1574.8307342913547, Weight -0.8966482921364797, Bias 10.592003406431886\n",
      "Iteration 10: Loss 1573.301481640302, Weight -0.9253788004776791, Bias 10.920543905747774\n",
      "Iteration 11: Loss 1572.3225579468633, Weight -0.9484778472621747, Bias 11.183393660073191\n",
      "Iteration 12: Loss 1571.695917020262, Weight -0.9670484728161288, Bias 11.393687416680887\n",
      "Iteration 13: Loss 1571.2947834707975, Weight -0.9819778255770275, Bias 11.561933639688593\n",
      "Iteration 14: Loss 1571.038004455056, Weight -0.9939793838798138, Bias 11.696539636278972\n",
      "Iteration 15: Loss 1570.8736314921862, Weight -1.003626927416059, Bias 11.804231683179934\n",
      "Iteration 16: Loss 1570.7684107053587, Weight -1.0113818690748597, Bias 11.890391148369604\n",
      "Iteration 17: Loss 1570.701055213459, Weight -1.0176152437155677, Bias 11.959323404950302\n",
      "Iteration 18: Loss 1570.657938586101, Weight -1.022625397476236, Bias 12.014472975530188\n",
      "Iteration 19: Loss 1570.6303380922132, Weight -1.026652217573872, Bias 12.058595658413948\n",
      "Iteration 20: Loss 1570.6126700176642, Weight -1.0298885786052958, Bias 12.093896237150949\n",
      "Iteration 21: Loss 1570.6013600363117, Weight -1.0324895493793347, Bias 12.122138655087989\n",
      "Iteration 22: Loss 1570.5941200982863, Weight -1.0345797980746914, Bias 12.14473416057295\n",
      "Iteration 23: Loss 1570.5894855426511, Weight -1.0362595480074983, Bias 12.162811827590861\n",
      "Iteration 24: Loss 1570.586518787939, Weight -1.037609367468763, Bias 12.177274975870365\n",
      "Iteration 25: Loss 1570.5846196545515, Weight -1.0386940217016278, Bias 12.188846309862239\n",
      "Iteration 26: Loss 1570.5834039456915, Weight -1.0395655705121123, Bias 12.198104032249024\n",
      "Iteration 27: Loss 1570.5826257228427, Weight -1.040265859135498, Bias 12.205510736623854\n",
      "Iteration 28: Loss 1570.5821275516, Weight -1.0408285211155195, Bias 12.21143652313816\n",
      "Iteration 29: Loss 1570.5818086522686, Weight -1.0412805887277774, Bias 12.216177492229669\n",
      "Iteration 30: Loss 1570.581604511926, Weight -1.0416437877915625, Bias 12.219970540577608\n",
      "Iteration 31: Loss 1570.5814738333645, Weight -1.0419355786794846, Bias 12.223005198648996\n",
      "Iteration 32: Loss 1570.5813901806337, Weight -1.042169993238738, Bias 12.22543310136452\n",
      "Iteration 33: Loss 1570.5813366310415, Weight -1.042358307607339, Bias 12.227375565136757\n",
      "Iteration 34: Loss 1570.581302351699, Weight -1.0425095830812359, Bias 12.228929649907213\n",
      "Iteration 35: Loss 1570.5812804080388, Weight -1.0426311008919082, Bias 12.230173009102629\n",
      "Iteration 36: Loss 1570.5812663609636, Weight -1.042728711689703, Bias 12.23116776986268\n",
      "Iteration 37: Loss 1570.5812573688245, Weight -1.0428071164548078, Bias 12.231963637433234\n",
      "Iteration 38: Loss 1570.5812516125648, Weight -1.0428700922817493, Bias 12.232600378850647\n",
      "Iteration 39: Loss 1570.5812479277301, Weight -1.0429206738485794, Bias 12.233109810025583\n",
      "Iteration 40: Loss 1570.581245568904, Weight -1.0429612992631316, Bias 12.233517385519695\n",
      "Iteration 41: Loss 1570.5812440589145, Weight -1.0429939272748534, Bias 12.233843470455064\n",
      "Iteration 42: Loss 1570.581243092302, Weight -1.043020131470629, Bias 12.234104358112546\n",
      "Iteration 43: Loss 1570.5812424735295, Weight -1.0430411759713276, Bias 12.234313084067367\n",
      "Iteration 44: Loss 1570.5812420774246, Weight -1.043058076259983, Bias 12.234480077543306\n",
      "Iteration 45: Loss 1570.5812418238597, Weight -1.0430716480603455, Bias 12.234613682532801\n",
      "Iteration 46: Loss 1570.5812416615413, Weight -1.0430825466119062, Bias 12.234720574722543\n",
      "Iteration 47: Loss 1570.5812415576336, Weight -1.0430912982271872, Bias 12.234806095057685\n",
      "Iteration 48: Loss 1570.581241491117, Weight -1.0430983256470008, Bias 12.234874516612276\n",
      "Iteration 49: Loss 1570.5812414485367, Weight -1.0431039684118928, Bias 12.234929258100912\n",
      "Iteration 50: Loss 1570.5812414212792, Weight -1.043108499228838, Bias 12.234973054700374\n",
      "Iteration 51: Loss 1570.5812414038303, Weight -1.0431121371200354, Bias 12.235008094716816\n",
      "Iteration 52: Loss 1570.5812413926603, Weight -1.0431150579861403, Bias 12.235036128927465\n",
      "Iteration 53: Loss 1570.58124138551, Weight -1.0431174030920827, Bias 12.235058558060354\n",
      "Iteration 54: Loss 1570.5812413809326, Weight -1.0431192858838043, Bias 12.235076502783244\n",
      "Iteration 55: Loss 1570.5812413780025, Weight -1.0431207974640453, Bias 12.23509085969887\n",
      "Iteration 56: Loss 1570.581241376127, Weight -1.0431220109908172, Bias 12.235102346144451\n",
      "Iteration 57: Loss 1570.5812413749259, Weight -1.0431229852104147, Bias 12.235111536033957\n",
      "Iteration 58: Loss 1570.5812413741573, Weight -1.043123767295165, Bias 12.235118888534046\n",
      "Iteration 59: Loss 1570.5812413736653, Weight -1.0431243951226752, Bias 12.235124771006541\n",
      "Iteration 60: Loss 1570.5812413733502, Weight -1.0431248991064364, Bias 12.235129477363781\n",
      "Iteration 61: Loss 1570.5812413731485, Weight -1.0431253036660535, Bias 12.235133242754008\n",
      "Iteration 62: Loss 1570.5812413730196, Weight -1.0431256284080141, Bias 12.235136255310566\n",
      "Iteration 63: Loss 1570.5812413729366, Weight -1.0431258890739503, Bias 12.235138665551975\n",
      "Iteration 64: Loss 1570.5812413728836, Weight -1.0431260983021677, Bias 12.23514059390256\n",
      "Iteration 65: Loss 1570.5812413728502, Weight -1.0431262662391814, Bias 12.235142136709415\n",
      "Iteration 66: Loss 1570.5812413728283, Weight -1.0431264010308043, Bias 12.235143371056342\n",
      "Iteration 67: Loss 1570.5812413728145, Weight -1.0431265092164843, Bias 12.235144358615305\n",
      "Iteration 68: Loss 1570.5812413728056, Weight -1.043126596045954, Bias 12.235145148727826\n",
      "Iteration 69: Loss 1570.5812413728, Weight -1.0431266657334899, Bias 12.235145780870292\n",
      "Iteration 70: Loss 1570.5812413727963, Weight -1.0431267216620645, Bias 12.23514628662636\n",
      "Iteration 71: Loss 1570.581241372794, Weight -1.0431267665472672, Bias 12.235146691265\n",
      "Iteration 72: Loss 1570.5812413727929, Weight -1.0431268025689076, Bias 12.235147015003024\n",
      "Iteration 73: Loss 1570.5812413727917, Weight -1.0431268314766884, Bias 12.235147274015203\n",
      "Iteration 74: Loss 1570.581241372791, Weight -1.0431268546750374, Bias 12.235147481242407\n",
      "Iteration 75: Loss 1570.5812413727906, Weight -1.0431268732912153, Bias 12.235147647038184\n",
      "Iteration 76: Loss 1570.5812413727906, Weight -1.0431268882299976, Bias 12.23514777968605\n",
      "Iteration 77: Loss 1570.5812413727901, Weight -1.0431269002175692, Bias 12.235147885813367\n",
      "Iteration 78: Loss 1570.5812413727901, Weight -1.0431269098367608, Bias 12.235147970722462\n",
      "Iteration 79: Loss 1570.58124137279, Weight -1.043126917555342, Bias 12.235148038655549\n",
      "Iteration 80: Loss 1570.58124137279, Weight -1.0431269237487237, Bias 12.23514809300668\n",
      "Iteration 81: Loss 1570.58124137279, Weight -1.0431269287181926, Bias 12.235148136491327\n",
      "Iteration 82: Loss 1570.58124137279, Weight -1.0431269327055375, Bias 12.235148171282047\n",
      "Iteration 83: Loss 1570.58124137279, Weight -1.0431269359047968, Bias 12.235148199117031\n",
      "Iteration 84: Loss 1570.58124137279, Weight -1.0431269384716846, Bias 12.23514822138695\n",
      "Iteration 85: Loss 1570.58124137279, Weight -1.0431269405311587, Bias 12.235148239204436\n",
      "Iteration 86: Loss 1570.58124137279, Weight -1.0431269421834921, Bias 12.23514825345967\n",
      "Iteration 87: Loss 1570.58124137279, Weight -1.043126943509149, Bias 12.235148264864854\n",
      "Iteration 88: Loss 1570.58124137279, Weight -1.0431269445726972, Bias 12.235148273989802\n",
      "Iteration 89: Loss 1570.58124137279, Weight -1.0431269454259449, Bias 12.235148281290403\n",
      "Iteration 90: Loss 1570.58124137279, Weight -1.0431269461104637, Bias 12.2351482871314\n",
      "Iteration 91: Loss 1570.58124137279, Weight -1.04312694665961, Bias 12.235148291804611\n",
      "Iteration 92: Loss 1570.58124137279, Weight -1.0431269471001479, Bias 12.235148295543512\n",
      "Iteration 93: Loss 1570.58124137279, Weight -1.0431269474535514, Bias 12.235148298534899\n",
      "Iteration 94: Loss 1570.58124137279, Weight -1.0431269477370502, Bias 12.23514830092822\n",
      "Iteration 95: Loss 1570.58124137279, Weight -1.0431269479644671, Bias 12.235148302843049\n",
      "Iteration 96: Loss 1570.58124137279, Weight -1.0431269481468943, Bias 12.23514830437505\n",
      "Iteration 97: Loss 1570.58124137279, Weight -1.0431269482932293, Bias 12.235148305600761\n",
      "Iteration 98: Loss 1570.58124137279, Weight -1.0431269484106114, Bias 12.235148306581419\n",
      "Iteration 99: Loss 1570.58124137279, Weight -1.0431269485047665, Bias 12.235148307366016\n",
      "Iteration 100: Loss 1570.58124137279, Weight -1.0431269485802903, Bias 12.23514830799375\n",
      "Iteration 101: Loss 1570.58124137279, Weight -1.0431269486408679, Bias 12.235148308495983\n",
      "Iteration 102: Loss 1570.58124137279, Weight -1.0431269486894568, Bias 12.235148308897806\n",
      "Iteration 103: Loss 1570.58124137279, Weight -1.0431269487284285, Bias 12.235148309219293\n",
      "Iteration 104: Loss 1570.58124137279, Weight -1.0431269487596866, Bias 12.235148309476505\n",
      "Iteration 105: Loss 1570.58124137279, Weight -1.043126948784757, Bias 12.235148309682295\n",
      "Iteration 106: Loss 1570.58124137279, Weight -1.0431269488048645, Bias 12.235148309846942\n",
      "Iteration 107: Loss 1570.58124137279, Weight -1.0431269488209913, Bias 12.23514830997867\n",
      "Iteration 108: Loss 1570.58124137279, Weight -1.0431269488339252, Bias 12.235148310084066\n",
      "Iteration 109: Loss 1570.58124137279, Weight -1.043126948844298, Bias 12.235148310168388\n",
      "Iteration 110: Loss 1570.58124137279, Weight -1.0431269488526176, Bias 12.235148310235852\n",
      "Iteration 111: Loss 1570.58124137279, Weight -1.0431269488592896, Bias 12.235148310289828\n",
      "Iteration 112: Loss 1570.58124137279, Weight -1.0431269488646404, Bias 12.235148310333013\n",
      "Iteration 113: Loss 1570.58124137279, Weight -1.0431269488689314, Bias 12.235148310367565\n",
      "Iteration 114: Loss 1570.58124137279, Weight -1.0431269488723722, Bias 12.235148310395209\n",
      "Iteration 115: Loss 1570.58124137279, Weight -1.0431269488751311, Bias 12.235148310417326\n",
      "Iteration 116: Loss 1570.58124137279, Weight -1.0431269488773447, Bias 12.235148310435022\n",
      "Iteration 117: Loss 1570.58124137279, Weight -1.0431269488791193, Bias 12.23514831044918\n",
      "Iteration 118: Loss 1570.58124137279, Weight -1.0431269488805426, Bias 12.235148310460508\n",
      "Iteration 119: Loss 1570.58124137279, Weight -1.0431269488816841, Bias 12.23514831046957\n",
      "Iteration 120: Loss 1570.58124137279, Weight -1.043126948882599, Bias 12.235148310476822\n",
      "Iteration 121: Loss 1570.58124137279, Weight -1.0431269488833328, Bias 12.235148310482623\n",
      "Iteration 122: Loss 1570.58124137279, Weight -1.0431269488839208, Bias 12.235148310487265\n",
      "Iteration 123: Loss 1570.58124137279, Weight -1.0431269488843924, Bias 12.235148310490978\n",
      "Iteration 124: Loss 1570.58124137279, Weight -1.0431269488847705, Bias 12.235148310493948\n",
      "Iteration 125: Loss 1570.58124137279, Weight -1.043126948885074, Bias 12.235148310496324\n",
      "Iteration 126: Loss 1570.58124137279, Weight -1.0431269488853174, Bias 12.235148310498227\n",
      "Iteration 127: Loss 1570.58124137279, Weight -1.0431269488855126, Bias 12.235148310499747\n",
      "Iteration 128: Loss 1570.58124137279, Weight -1.0431269488856691, Bias 12.235148310500964\n",
      "Iteration 129: Loss 1570.58124137279, Weight -1.0431269488857946, Bias 12.235148310501938\n",
      "Iteration 130: Loss 1570.58124137279, Weight -1.0431269488858954, Bias 12.235148310502717\n",
      "Iteration 131: Loss 1570.58124137279, Weight -1.0431269488859765, Bias 12.235148310503341\n",
      "Iteration 132: Loss 1570.58124137279, Weight -1.043126948886041, Bias 12.23514831050384\n",
      "Iteration 133: Loss 1570.58124137279, Weight -1.0431269488860924, Bias 12.23514831050424\n",
      "Iteration 134: Loss 1570.58124137279, Weight -1.043126948886134, Bias 12.23514831050456\n",
      "Iteration 135: Loss 1570.58124137279, Weight -1.0431269488861679, Bias 12.235148310504815\n",
      "Iteration 136: Loss 1570.58124137279, Weight -1.0431269488861952, Bias 12.23514831050502\n",
      "Iteration 137: Loss 1570.58124137279, Weight -1.043126948886216, Bias 12.235148310505183\n",
      "Iteration 138: Loss 1570.5812413727897, Weight -1.0431269488862334, Bias 12.235148310505314\n",
      "Iteration 139: Loss 1570.58124137279, Weight -1.0431269488862465, Bias 12.23514831050542\n",
      "Iteration 140: Loss 1570.58124137279, Weight -1.0431269488862578, Bias 12.235148310505503\n",
      "Iteration 141: Loss 1570.58124137279, Weight -1.0431269488862662, Bias 12.23514831050557\n",
      "Iteration 142: Loss 1570.58124137279, Weight -1.043126948886273, Bias 12.235148310505624\n",
      "Iteration 143: Loss 1570.58124137279, Weight -1.0431269488862787, Bias 12.235148310505666\n",
      "Iteration 144: Loss 1570.58124137279, Weight -1.043126948886283, Bias 12.2351483105057\n",
      "Iteration 145: Loss 1570.58124137279, Weight -1.0431269488862869, Bias 12.235148310505727\n",
      "Iteration 146: Loss 1570.58124137279, Weight -1.0431269488862898, Bias 12.235148310505748\n",
      "Iteration 147: Loss 1570.58124137279, Weight -1.0431269488862922, Bias 12.235148310505766\n",
      "Iteration 148: Loss 1570.58124137279, Weight -1.0431269488862944, Bias 12.23514831050578\n",
      "Iteration 149: Loss 1570.58124137279, Weight -1.0431269488862964, Bias 12.23514831050579\n",
      "Iteration 150: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.2351483105058\n",
      "Iteration 151: Loss 1570.58124137279, Weight -1.0431269488862989, Bias 12.235148310505807\n",
      "Iteration 152: Loss 1570.58124137279, Weight -1.0431269488862995, Bias 12.235148310505812\n",
      "Iteration 153: Loss 1570.58124137279, Weight -1.0431269488863, Bias 12.235148310505817\n",
      "Iteration 154: Loss 1570.58124137279, Weight -1.0431269488863, Bias 12.23514831050582\n",
      "Iteration 155: Loss 1570.5812413727901, Weight -1.0431269488862998, Bias 12.235148310505824\n",
      "Iteration 156: Loss 1570.58124137279, Weight -1.0431269488862995, Bias 12.235148310505828\n",
      "Iteration 157: Loss 1570.58124137279, Weight -1.043126948886299, Bias 12.23514831050583\n",
      "Iteration 158: Loss 1570.58124137279, Weight -1.0431269488862989, Bias 12.235148310505831\n",
      "Iteration 159: Loss 1570.58124137279, Weight -1.0431269488862984, Bias 12.235148310505833\n",
      "Iteration 160: Loss 1570.58124137279, Weight -1.0431269488862982, Bias 12.235148310505833\n",
      "Iteration 161: Loss 1570.58124137279, Weight -1.043126948886298, Bias 12.235148310505833\n",
      "Iteration 162: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 163: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 164: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 165: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 166: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 167: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 168: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 169: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 170: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 171: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 172: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 173: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 174: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 175: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 176: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 177: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 178: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 179: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 180: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 181: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 182: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 183: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 184: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 185: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 186: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 187: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 188: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 189: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 190: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 191: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 192: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 193: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 194: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 195: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 196: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 197: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 198: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 199: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 200: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 201: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 202: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 203: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 204: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 205: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 206: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 207: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 208: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 209: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 210: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 211: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 212: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 213: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 214: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 215: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 216: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 217: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 218: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 219: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 220: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 221: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 222: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 223: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 224: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 225: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 226: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 227: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 228: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 229: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 230: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 231: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 232: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 233: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 234: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 235: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 236: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 237: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 238: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 239: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 240: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 241: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 242: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 243: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 244: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 245: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 246: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 247: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 248: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 249: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 250: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 251: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 252: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 253: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 254: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 255: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 256: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 257: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 258: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 259: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 260: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 261: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 262: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 263: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 264: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 265: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 266: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 267: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 268: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 269: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 270: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 271: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 272: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 273: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 274: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 275: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 276: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 277: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 278: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 279: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 280: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 281: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 282: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 283: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 284: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 285: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 286: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 287: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 288: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 289: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 290: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 291: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 292: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 293: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 294: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 295: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 296: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 297: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 298: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 299: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 300: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 301: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 302: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 303: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 304: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 305: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 306: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 307: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 308: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 309: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 310: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 311: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 312: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 313: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 314: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 315: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 316: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 317: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 318: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 319: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 320: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 321: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 322: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 323: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 324: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 325: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 326: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 327: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 328: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 329: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 330: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 331: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 332: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 333: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 334: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 335: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 336: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 337: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 338: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 339: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 340: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 341: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 342: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 343: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 344: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 345: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 346: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 347: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 348: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 349: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 350: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 351: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 352: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 353: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 354: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 355: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 356: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 357: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 358: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 359: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 360: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 361: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 362: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 363: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 364: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 365: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 366: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 367: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 368: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 369: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 370: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 371: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 372: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 373: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 374: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 375: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 376: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 377: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 378: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 379: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 380: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 381: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 382: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 383: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 384: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 385: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 386: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 387: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 388: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 389: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 390: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 391: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 392: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 393: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 394: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 395: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 396: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 397: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 398: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 399: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 400: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 401: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 402: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 403: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 404: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 405: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 406: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 407: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 408: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 409: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 410: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 411: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 412: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 413: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 414: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 415: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 416: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 417: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 418: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 419: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 420: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 421: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 422: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 423: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 424: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 425: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 426: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 427: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 428: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 429: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 430: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 431: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 432: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 433: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 434: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 435: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 436: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 437: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 438: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 439: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 440: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 441: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 442: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 443: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 444: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 445: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 446: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 447: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 448: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 449: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 450: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 451: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 452: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 453: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 454: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 455: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 456: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 457: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 458: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 459: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 460: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 461: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 462: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 463: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 464: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 465: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 466: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 467: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 468: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 469: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 470: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 471: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 472: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 473: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 474: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 475: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 476: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 477: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 478: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 479: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 480: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 481: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 482: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 483: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 484: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 485: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 486: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 487: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 488: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 489: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 490: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 491: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 492: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 493: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 494: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 495: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 496: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 497: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 498: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 499: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 500: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 501: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 502: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 503: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 504: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 505: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 506: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 507: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 508: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 509: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 510: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 511: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 512: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 513: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 514: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 515: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 516: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 517: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 518: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 519: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 520: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 521: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 522: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 523: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 524: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 525: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 526: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 527: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 528: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 529: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 530: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 531: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 532: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 533: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 534: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 535: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 536: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 537: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 538: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 539: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 540: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 541: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 542: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 543: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 544: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 545: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 546: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 547: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 548: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 549: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 550: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 551: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 552: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 553: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 554: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 555: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 556: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 557: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 558: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 559: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 560: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 561: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 562: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 563: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 564: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 565: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 566: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 567: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 568: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 569: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 570: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 571: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 572: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 573: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 574: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 575: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 576: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 577: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 578: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 579: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 580: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 581: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 582: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 583: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 584: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 585: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 586: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 587: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 588: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 589: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 590: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 591: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 592: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 593: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 594: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 595: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 596: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 597: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 598: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 599: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 600: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 601: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 602: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 603: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 604: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 605: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 606: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 607: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 608: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 609: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 610: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 611: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 612: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 613: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 614: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 615: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 616: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 617: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 618: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 619: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 620: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 621: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 622: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 623: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 624: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 625: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 626: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 627: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 628: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 629: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 630: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 631: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 632: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 633: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 634: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 635: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 636: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 637: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 638: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 639: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 640: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 641: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 642: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 643: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 644: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 645: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 646: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 647: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 648: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 649: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 650: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 651: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 652: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 653: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 654: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 655: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 656: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 657: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 658: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 659: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 660: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 661: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 662: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 663: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 664: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 665: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 666: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 667: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 668: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 669: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 670: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 671: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 672: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 673: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 674: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 675: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 676: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 677: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 678: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 679: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 680: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 681: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 682: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 683: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 684: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 685: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 686: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 687: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 688: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 689: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 690: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 691: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 692: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 693: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 694: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 695: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 696: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 697: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 698: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 699: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 700: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 701: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 702: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 703: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 704: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 705: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 706: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 707: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 708: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 709: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 710: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 711: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 712: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 713: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 714: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 715: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 716: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 717: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 718: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 719: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 720: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 721: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 722: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 723: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 724: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 725: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 726: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 727: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 728: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 729: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 730: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 731: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 732: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 733: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 734: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 735: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 736: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 737: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 738: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 739: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 740: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 741: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 742: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 743: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 744: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 745: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 746: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 747: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 748: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 749: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 750: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 751: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 752: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 753: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 754: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 755: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 756: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 757: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 758: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 759: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 760: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 761: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 762: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 763: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 764: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 765: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 766: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 767: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 768: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 769: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 770: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 771: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 772: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 773: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 774: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 775: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 776: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 777: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 778: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 779: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 780: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 781: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 782: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 783: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 784: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 785: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 786: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 787: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 788: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 789: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 790: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 791: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 792: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 793: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 794: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 795: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 796: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 797: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 798: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 799: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 800: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 801: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 802: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 803: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 804: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 805: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 806: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 807: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 808: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 809: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 810: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 811: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 812: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 813: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 814: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 815: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 816: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 817: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 818: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 819: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 820: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 821: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 822: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 823: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 824: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 825: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 826: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 827: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 828: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 829: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 830: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 831: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 832: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 833: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 834: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 835: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 836: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 837: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 838: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 839: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 840: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 841: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 842: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 843: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 844: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 845: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 846: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 847: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 848: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 849: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 850: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 851: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 852: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 853: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 854: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 855: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 856: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 857: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 858: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 859: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 860: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 861: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 862: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 863: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 864: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 865: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 866: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 867: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 868: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 869: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 870: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 871: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 872: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 873: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 874: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 875: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 876: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 877: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 878: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 879: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 880: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 881: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 882: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 883: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 884: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 885: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 886: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 887: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 888: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 889: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 890: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 891: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 892: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 893: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 894: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 895: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 896: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 897: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 898: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 899: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 900: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 901: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 902: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 903: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 904: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 905: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 906: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 907: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 908: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 909: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 910: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 911: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 912: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 913: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 914: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 915: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 916: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 917: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 918: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 919: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 920: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 921: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 922: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 923: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 924: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 925: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 926: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 927: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 928: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 929: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 930: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 931: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 932: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 933: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 934: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 935: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 936: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 937: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 938: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 939: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 940: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 941: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 942: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 943: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 944: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 945: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 946: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 947: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 948: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 949: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 950: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 951: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 952: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 953: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 954: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 955: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 956: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 957: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 958: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 959: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 960: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 961: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 962: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 963: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 964: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 965: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 966: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 967: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 968: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 969: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 970: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 971: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 972: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 973: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 974: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 975: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 976: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 977: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 978: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 979: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 980: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 981: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 982: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 983: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 984: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 985: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 986: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 987: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 988: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 989: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 990: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 991: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 992: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 993: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 994: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 995: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 996: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 997: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 998: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 999: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1000: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1001: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1002: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1003: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1004: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1005: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1006: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1007: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1008: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1009: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1010: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1011: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1012: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1013: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1014: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1015: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1016: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1017: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1018: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1019: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1020: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1021: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1022: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1023: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1024: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1025: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1026: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1027: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1028: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1029: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1030: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1031: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1032: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1033: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1034: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1035: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1036: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1037: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1038: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1039: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1040: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1041: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1042: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1043: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1044: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1045: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1046: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1047: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1048: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1049: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1050: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1051: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1052: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1053: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1054: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1055: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1056: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1057: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1058: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1059: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1060: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1061: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1062: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1063: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1064: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1065: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1066: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1067: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1068: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1069: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1070: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1071: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1072: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1073: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1074: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1075: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1076: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1077: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1078: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1079: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1080: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1081: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1082: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1083: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1084: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1085: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1086: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1087: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1088: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1089: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1090: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1091: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1092: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1093: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1094: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1095: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1096: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1097: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1098: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1099: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1100: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1101: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1102: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1103: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1104: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1105: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1106: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1107: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1108: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1109: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1110: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1111: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1112: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1113: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1114: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1115: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1116: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1117: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1118: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1119: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1120: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1121: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1122: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1123: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1124: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1125: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1126: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1127: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1128: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1129: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1130: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1131: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1132: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1133: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1134: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1135: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1136: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1137: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1138: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1139: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1140: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1141: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1142: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1143: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1144: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1145: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1146: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1147: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1148: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1149: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1150: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1151: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1152: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1153: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1154: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1155: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1156: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1157: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1158: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1159: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1160: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1161: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1162: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1163: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1164: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1165: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1166: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1167: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1168: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1169: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1170: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1171: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1172: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1173: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1174: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1175: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1176: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1177: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1178: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1179: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1180: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1181: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1182: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1183: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1184: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1185: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1186: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1187: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1188: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1189: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1190: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1191: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1192: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1193: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1194: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1195: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1196: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1197: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1198: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1199: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1200: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1201: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1202: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1203: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1204: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1205: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1206: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1207: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1208: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1209: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1210: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1211: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1212: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1213: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1214: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1215: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1216: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1217: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1218: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1219: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1220: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1221: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1222: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1223: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1224: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1225: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1226: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1227: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1228: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1229: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1230: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1231: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1232: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1233: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1234: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1235: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1236: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1237: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1238: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1239: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1240: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1241: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1242: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1243: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1244: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1245: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1246: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1247: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1248: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1249: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1250: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1251: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1252: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1253: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1254: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1255: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1256: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1257: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1258: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1259: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1260: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1261: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1262: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1263: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1264: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1265: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1266: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1267: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1268: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1269: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1270: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1271: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1272: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1273: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1274: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1275: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1276: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1277: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1278: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1279: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1280: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1281: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1282: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1283: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1284: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1285: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1286: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1287: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1288: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1289: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1290: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1291: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1292: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1293: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1294: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1295: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1296: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1297: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1298: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1299: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1300: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1301: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1302: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1303: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1304: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1305: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1306: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1307: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1308: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1309: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1310: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1311: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1312: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1313: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1314: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1315: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1316: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1317: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1318: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1319: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1320: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1321: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1322: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1323: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1324: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1325: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1326: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1327: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1328: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1329: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1330: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1331: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1332: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1333: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1334: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1335: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1336: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1337: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1338: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1339: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1340: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1341: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1342: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1343: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1344: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1345: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1346: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1347: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1348: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1349: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1350: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1351: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1352: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1353: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1354: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1355: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1356: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1357: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1358: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1359: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1360: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1361: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1362: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1363: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1364: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1365: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1366: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1367: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1368: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1369: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1370: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1371: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1372: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1373: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1374: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1375: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1376: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1377: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1378: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1379: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1380: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1381: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1382: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1383: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1384: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1385: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1386: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1387: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1388: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1389: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1390: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1391: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1392: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1393: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1394: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1395: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1396: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1397: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1398: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1399: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1400: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1401: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1402: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1403: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1404: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1405: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1406: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1407: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1408: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1409: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1410: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1411: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1412: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1413: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1414: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1415: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1416: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1417: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1418: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1419: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1420: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1421: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1422: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1423: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1424: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1425: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1426: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1427: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1428: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1429: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1430: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1431: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1432: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1433: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1434: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1435: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1436: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1437: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1438: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1439: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1440: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1441: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1442: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1443: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1444: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1445: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1446: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1447: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1448: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1449: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1450: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1451: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1452: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1453: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1454: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1455: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1456: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1457: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1458: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1459: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1460: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1461: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1462: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1463: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1464: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1465: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1466: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1467: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1468: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1469: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1470: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1471: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1472: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1473: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1474: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1475: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1476: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1477: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1478: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1479: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1480: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1481: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1482: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1483: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1484: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1485: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1486: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1487: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1488: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1489: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1490: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1491: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1492: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1493: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1494: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1495: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1496: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1497: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1498: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1499: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1500: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Weight: -1.0431269488862978\n",
      "Estimated Bias: 12.235148310505833\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "est_weight, est_bias = train_model(X_train, y_train, alpha, max_epoch)\n",
    "print(f\"Estimated Weight: {est_weight}\\nEstimated Bias: {est_bias}\")\n",
    "y_pred = (est_weight*X_test) + est_bias\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(y_test, y_pred, marker='o', color='red')\n",
    "\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_pred), max(y_pred)], color='blue', label=\"line1\")\n",
    "#plt.plot(y_test, y_test, color='orange', label=\"line2\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.841913305022075\n",
      "1719.2467180299154\n",
      "-0.013453292642622738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}