{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         f1        f2        f3        f4        f5   response\n0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075\n5 -0.699867  0.019159  1.103377 -0.671614 -0.119063 -18.597563\n6 -1.028250  0.962967  0.471027 -1.941219 -0.465591 -73.174734\n7  0.337585  1.352948 -1.789795 -0.885796 -0.846150 -25.865464\n8  0.295433 -0.907789  0.275980 -0.675526 -0.942592  -9.001596\n9  0.442269 -0.704559 -1.127342  1.030206  0.800113  57.076963",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.764216</td>\n      <td>-1.016209</td>\n      <td>0.149410</td>\n      <td>-0.050119</td>\n      <td>-0.578127</td>\n      <td>6.242514</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.763880</td>\n      <td>-1.159509</td>\n      <td>-0.721492</td>\n      <td>-0.654067</td>\n      <td>-0.431670</td>\n      <td>-8.118241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.519329</td>\n      <td>-0.664621</td>\n      <td>-1.694904</td>\n      <td>1.339779</td>\n      <td>0.182764</td>\n      <td>66.722455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.177388</td>\n      <td>0.515623</td>\n      <td>0.135144</td>\n      <td>-0.647634</td>\n      <td>-0.405631</td>\n      <td>-27.716793</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.104022</td>\n      <td>0.749665</td>\n      <td>-0.939338</td>\n      <td>-0.090725</td>\n      <td>-0.639963</td>\n      <td>8.192075</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.699867</td>\n      <td>0.019159</td>\n      <td>1.103377</td>\n      <td>-0.671614</td>\n      <td>-0.119063</td>\n      <td>-18.597563</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-1.028250</td>\n      <td>0.962967</td>\n      <td>0.471027</td>\n      <td>-1.941219</td>\n      <td>-0.465591</td>\n      <td>-73.174734</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.337585</td>\n      <td>1.352948</td>\n      <td>-1.789795</td>\n      <td>-0.885796</td>\n      <td>-0.846150</td>\n      <td>-25.865464</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.295433</td>\n      <td>-0.907789</td>\n      <td>0.275980</td>\n      <td>-0.675526</td>\n      <td>-0.942592</td>\n      <td>-9.001596</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.442269</td>\n      <td>-0.704559</td>\n      <td>-1.127342</td>\n      <td>1.030206</td>\n      <td>0.800113</td>\n      <td>57.076963</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset to be processed with pandas & displaying the top 10 result\n",
    "dt = pd.read_csv('assignment1_dataset.csv', sep=',')\n",
    "dt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                f1           f2           f3           f4           f5  \\\ncount  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \nmean      0.012255    -0.043030    -0.065785     0.039616     0.008074   \nstd       0.998816     1.042413     0.982640     1.023960     1.006679   \nmin      -3.174809    -3.381691    -3.158010    -2.764936    -2.946633   \n25%      -0.655282    -0.759477    -0.734505    -0.660802    -0.685371   \n50%      -0.001177    -0.038444    -0.049838    -0.006831    -0.000368   \n75%       0.697331     0.696343     0.591642     0.737806     0.710398   \nmax       3.092866     3.534175     3.406115     3.145835     3.007734   \n\n          response  \ncount  1000.000000  \nmean     11.229435  \nstd      40.028188  \nmin    -103.044475  \n25%     -16.580272  \n50%      10.554227  \n75%      38.485118  \nmax     157.890314  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.012255</td>\n      <td>-0.043030</td>\n      <td>-0.065785</td>\n      <td>0.039616</td>\n      <td>0.008074</td>\n      <td>11.229435</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.998816</td>\n      <td>1.042413</td>\n      <td>0.982640</td>\n      <td>1.023960</td>\n      <td>1.006679</td>\n      <td>40.028188</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-3.174809</td>\n      <td>-3.381691</td>\n      <td>-3.158010</td>\n      <td>-2.764936</td>\n      <td>-2.946633</td>\n      <td>-103.044475</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.655282</td>\n      <td>-0.759477</td>\n      <td>-0.734505</td>\n      <td>-0.660802</td>\n      <td>-0.685371</td>\n      <td>-16.580272</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.001177</td>\n      <td>-0.038444</td>\n      <td>-0.049838</td>\n      <td>-0.006831</td>\n      <td>-0.000368</td>\n      <td>10.554227</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.697331</td>\n      <td>0.696343</td>\n      <td>0.591642</td>\n      <td>0.737806</td>\n      <td>0.710398</td>\n      <td>38.485118</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.092866</td>\n      <td>3.534175</td>\n      <td>3.406115</td>\n      <td>3.145835</td>\n      <td>3.007734</td>\n      <td>157.890314</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying additional description\n",
    "dt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "f2         -0.031751\nf5         -0.028999\nf3          0.015218\nf1          0.308474\nf4          0.947255\nresponse    1.000000\nName: response, dtype: float64"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a correlation matrix between the columns/features and target in ascending order\n",
    "corr_matrix = dt.corr()\n",
    "corr_matrix['response'].sort_values(ascending=True)\n",
    "# Correlation between f4 and response are the closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'relationship between f4 & response')"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAwzUlEQVR4nO3de5xddX3v/9d7Jjuwg8KEH9GSAQQphkqRpKTUU/xZwUssXoggokdbz9Ee7Tn11+KvzTFYfyW0ehKbetT29LRia72AEBSMKNaoBfQcFCUxQUDJKZRLMgGJwiCQASaTz++PtfZkz5619l5z2ff38/GYR2avy97ftWdnffb39vkqIjAzMytioN0FMDOz7uGgYWZmhTlomJlZYQ4aZmZWmIOGmZkV5qBhZmaFOWgYkm6S9HuzPPc4SU9IGpzvclW9xjpJl9fZf6ekl83yuUPSL8+2bJ1O0hsk7Ur/RivaXR7rfg4aNiOS7pP0isrjiHggIp4VERPtKlNEnBIRN7X6dbsk4PwV8J70b7S9slHSSZKeqheM0+NOkPRtSY+nf/vfbXqJraM5aPQ4SQvaXQZrq+cBd2Zs/1vg1gLn/zfgPuBI4MXAj2fy4v789R4HjR6UfiN8n6QfAU9KWiDpxZK+K2lU0m15zTmSTpR0g6SfS/qZpCskDaX7PgccB3wlbe74r5KOT79xL0iPWSrpOkmPSLpb0n+qeu51kq6W9Nn0m+udklZW7X+fpJF0305JL68q2sI6503WftLX+KKkTemxP5R0WoO37BxJ/5Ze70ZJk/8vJL1D0k8kPSppi6Tnpdu/kx5yW/peXJh+Iz8/3f+S9H05J338Ckk7Gj1vuu9kSd9M38Odkt5Ute/Tkv5W0vXp9X1f0okZf8dDJD0BDKZlvKdq35uBUeBfGrwvAPuB3RExHhEPRcTWegdXfR7eKekB4IYG76MkfVTSw5Iek/QjSb9ada1/n74Xj6fvb/X79JuSbk3Pu1XSb1btu0nSX0i6OT33G5KOSvcdKuny9DM+mp773HTfEZL+UdKD6Wfxg2pi02tXigj/9NgPyTfDHcCxQBkYBn4OnEPyReGV6eMl6fE3Ab+X/v7L6f5DgCXAd4CP1Tz3K6oeHw8EsCB9/G3gfwKHAsuBvcDL033rgKfScgwC64Fb0n3LgF3A0qrnPbHRebVlSo8dB94IlIA/Ae4FSjnvVQA3knyTPg74P1XvxWrgbuBXgAXAB4Dv1pz7y1WP/xz4m/T39wP3AB+u2vfxRs8LHJa+D/8x3fdrwM+AU9L9nwYeAc5I918BXFXns1BbxsPTazw2fa8ub/BZ+n+Ap4FXF/zsVT4Pn02vpdzgelcB24AhQOkxR1dd6+PAS0k+jx8H/ne670jgUeB30ud8S/r4/6r6TN8DvCAtw03AhnTfu4GvAItIPk+nA4en+zYDn0jL/hzgB8C72/1/upN+2l4A/zThj5rcRN9R9fh9wOdqjtkCvD39/SbSG2XGc60Gttc8d2bQSG9EE8Czq/avBz6d/r4O+FbVvhcCY+nvvww8DLyCmht8vfNqy5QeWx1QBoAHgf875/qCqhsi8F+Af0l//2fgnTXPtQ94XtW51TfklwM/Sn//OvB7HAyK3wbOa/S8wIXA/6op4yeAS9LfPw38Q9W+c4C76nwWasv4ceB9Ve9VbtAAziQJuL8F7AZWpdtPIglkyjin8nl4ftW2etd7NkkQezEwUPNcn6YqIALPSj9fx5IEix/UHP894D9UfaY/UPN3/Xr6+zuA7wIvqjn/uSQBsly17S3Aja38/9vpP26e6l27qn5/HnBBWhUflTQKvAQ4uvYkSc+RdFVaNf8FcDlwVMHXXAo8EhGPV227n6SmU/FQ1e/7gEMlLYiIu4GLSG5kD6dlWNrovJxyTF57RBwgueEtzTl2yvFpeSvHPg/4eNV79gjJt+Fhsn0PeEHa1LGc5Nv2sWmzyBkktbZGz/s84Ddq/lZvBX6p6nVq34tn1bm2SZKWkwTljxY5HngPyZeNbwNvAD4naRXwmySBtV6209rPX+b1RsQNwP8g6WP5qaTLJB2e9TwR8UR67tL05/6a12z0Wau8T58j+dJ0laQ9kv5SUiktZwl4sKqsnyCpcVjKQaN3Vf+H3kXyn3+o6uewiNiQcd769NwXRcThwNtI/oNnPW+tPcCRkp5dte04YKRQgSM+HxEvIfnPG8CHi5yX4djKL2n/xDFp2RoeT1LeyrG7SJomqt+3ckR8N6f8+0iaWv4IuCMiniH5Rvv/AvdExM8KPO8u4Ns1+54VEf95xu/CdC8jqQk8IOkhkqa78yX9MOf4BSR9GkTErcCbgU0kgf2DDV6r9vOX+z5GxF9HxOnAKSTNSWuqzq3+Wz6LpFlqT/rzPKYq9FmLpH/m0oh4IUkAfC3wu2k5nwaOqirn4RFxSqPn7CcOGv3hcuB1klZJGkw7Al8m6ZiMY58NPAGMShpm6n9ggJ8Cz896kYjYRXKTXJ++xouAd5K0u9claZmksyUdQtJ/MUbSFDEbp0s6L62JXERyI7ilzvFrJC2WdCzJDX9Tuv3vgYslnZKW8QhJF1Sdl/VefJvkG/q308c31Txu9LxfJamt/I6kUvrz65J+pejF13EZcCJJLWh5Wo7rSfoVsnwB+ENJL02D74MkTYHPJflGXlTu9abX9hvpN/0nSf721X/3c5QMKlgI/AXw/fRz9jWS9+nfKxnocSFJs+VXGxVG0lmSTk07uH9B0gc2EREPAt8APiLpcEkDSgaG/NYMrrXnOWj0gfQ/2bkknbN7Sb5RrSH7738pSefrYyQ3lGtr9q8HPpBW3/8k4/y3kHyb3QN8iaQt/psFinkIsIGkrfwhkiaB9xc4L8uXSfoGKh2l50XEeIPjt5EMHrge+EeAiPgSSW3nqrSp7g7gt6vOWwd8Jn0vKiOcvk0SeL+T87ju86ZNe68i+Va/h+S9+DDJ+zMnEbEvkhFQD0XEQyRfDp6KiL05x18NrCUJNqPAlSRNW2uAr0o6ruDr1nsfDwc+SfK3up9kgMZfVZ3+eeASkmap00ma6oiIn5PUEP44Pee/Aq+tqs3V80vAF0kCxk9I/kaV+Sq/CywkGVr8aHrctGbcfqb6zZJm3UXSOpKO37e1uyw2N5I+TTLc9wPtLosd5JqGmZkV5qBhZmaFuXnKzMwKc03DzMwK6/lkYkcddVQcf/zx7S6GmVlX2bZt288iYknt9p4PGscffzxbt9bNsWZmZjUk1c64B9w8ZWZmM+CgYWZmhTlomJlZYQ4aZmZWmIOGmZkV1vOjp8zM+snm7SNs3LKTPaNjLB0qs2bVMlavyFsCZuYcNMzMesTm7SNcfO3tjI0n2eVHRse4+NrbAeYtcLh5ysysR2zcsnMyYFSMjU+wccvOeXsNBw0zsx6xZ3RsRttnw0HDzKxHLB0qz2j7bDhomJn1iDWrllEuDU7ZVi4NsmbVsnl7jbYGDUmfkvSwpDuqtq2TNCJpR/pzTtW+iyXdLWmnpLx1jc3M+tLqFcOsP+9UhofKCBgeKrP+vFN7avTUp4H/AXy2ZvtHI6J6nWAkvZBk3eRTgKXAtyS9ICImMDMzIAkc8xkkarW1phER3yFZML6Ic4GrIuLpiLgXuBs4o2mFMzOzaTq1T+M9kn6UNl8tTrcNA7uqjtmdbptG0rskbZW0de/evc0uq5lZ3+jEoPF3wInAcuBB4CPpdmUcm7lWbURcFhErI2LlkiXT1hAxM7NZ6rigERE/jYiJiDgAfJKDTVC7gWOrDj0G2NPq8pmZ9bOOCxqSjq56+AagMrLqOuDNkg6RdAJwEvCDVpfPzKyftXX0lKQrgZcBR0naDVwCvEzScpKmp/uAdwNExJ2SrgZ+DOwH/sAjp8zMWksRmd0CPWPlypXhNcLNzGZG0raIWFm7veOap8zMrHM5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYQ4aZmZWmIOGmZkV5qBhZmaFOWiYmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYW1dI9zMrBds3j7Cxi072TM6xtKhMmtWLWP1iuF2F6spHDTMzOZg8/YRLr72dsbGJwAYGR3j4mtvB+jJwOGgYWY2Bxu37JwMGBVj4xNs3LJzMmj0Uk3EQcPMek4rb9J7Rsfqbu+1mog7ws2sp1Ru0iOjYwQHb9Kbt4805fWWDpXrbq9XE+lGDhpm1lNadZPevH2EMzfcwMjoGKrZVy4NctbJSyb3Z8mroXQ6N0+ZWU9p1FxUbbbNWLVNTlG1b3iozFknL+GabSPTgle1vBpKp3NNw8x6SqPmooq5NGNl1WYABKxZtYwb79pbN2CUS4OsWbWs4et0IgcNM+spa1Yto1wanLIt6yY9l2asvNpMpM9br+lpeKjM+vNO7cpOcHDzlJn1mMrNuFGz00yasWodUS4xOjaee/7SoXJmX8bwUJmb157d8Pk7mWsaZtZzVq8Y5ua1Z/PRC5cD8N5NOzhzww1Tmp6KNmPV2rx9hCef2Z+7vxKkitR2ulFbg4akT0l6WNIdVduOlPRNSf+a/ru4at/Fku6WtFPSqvaU2sy6QaM+i9ne2Ddu2cn4RGTuq5y/esUw6887leGhMqL7m6SqKSL74lvy4tJLgSeAz0bEr6bb/hJ4JCI2SFoLLI6I90l6IXAlcAawFPgW8IKIyO9tAlauXBlbt25t6nWYWWfZvH2EP776NiYy7m/VTUSzGT11wtrrybtrfuzC5T0RGAAkbYuIlbXb29qnERHfkXR8zeZzgZelv38GuAl4X7r9qoh4GrhX0t0kAeR7LSmsmXWFSg0jK2DA1D6L1SuGZ3yTz+uvWLyo1JNpQ2p1Yp/GcyPiQYD03+ek24eBXVXH7U63TSPpXZK2Stq6d+/ephbWzDpL3nDYirnOj1izahmlwdrpfPDEU/vZvH2k5TPSW60Tg0ae6X8lsmuJEXFZRKyMiJVLlixpcrHMrJPUG/0kkpt4bad4PZWZ3yesvZ4zN9wAwGELpzfSjB8INm7Z2XNpQ2p14pDbn0o6OiIelHQ08HC6fTdwbNVxxwB7Wl46M+toec1HcPBbZtGkgVnJBt+7aUdun0a9gFW9r5ubrzqxpnEd8Pb097cDX67a/mZJh0g6ATgJ+EEbymdmHSxrVFRWM0WRb/9ZtYZ6Q4eWDpU5olzK3QetT6g439o95PZKko7sZZJ2S3onsAF4paR/BV6ZPiYi7gSuBn4MfB34g0Yjp8ysd9U2G1VuulnDXfNu9COjY3Vv1jNJKlhJUpg1h6M0oMmhvN3efNXu0VNvydn18pzjPwR8qHklMrNOk9WUAxRaoyKAhx57qu7z12umqtfUVW04LVfeHI5nHbpg8vnnMhO9E3Rin4aZGZC/gNEhCwYyv63/8dW3cdGmHYiDzUh5Q2+rz6teZa/amlXLprx+lup5Hxdt2pF5zOi+gylH8gJRt2S9ddAws45RW6vY98z+zOCQdxOvBIiZTlmujKiq7ZCu/J4XDIDJms/m7SNTglW16oCQFYi6KcWIg4aZdYSsWkUrjYyOseaLt7Huujt5bGx8SlPYoJRZYxkql6YkSMwKGJV06RVFEyp2KgcNM5t3MxlSWjl2JkFi8aISTzy9PzcH1GyNT8Rk9tpKECGym7jKpUHWvf6Uycf10qXXXvtsZqJ3CgcNM5tXef0QMP3muXn7CGu+cBvjB2Z2839033jmMNr5lheUBqVpCQjrpUPvJZ04T8PMuthMhpSuu+7OGQeMivalWoUDEdMCYC+nQ6/mmoaZzVi95qe8ZppKZ3P1OXkLGc2Xcmmw4bKr9fbnyRrp1O19FUW1NTV6Kzg1utn8qm1+guTmW2muOXPDDZnNNLUji2Z7w56pvBFNQ+US615/yuRNfmhRiSee2j+l5lMaFARTtlWuFXo7QOSlRnfQMLMZyQsKkLTfn3XyEq7ZNjIlIOTduFulNKCpwWBAbLzgtMw+lku/ciePpvMqhsolXnva0dx41966kwthauDsBXlBw30aZjYj9WYuj4yOcc22Ec4/fbhQGo/5UnmdLMNDZTZecNqU8lx4xrFs3LJzWgoSgKfGD0z+Pjo2zjXbRlizahn3bngNN689m9Urhrs+FchcuE/DzGakUWqNsfEJrvz+Lg5ETH4zzxtSmzf/YaYGpMznLw1qstmoeoGkvNFdecHgok072Lhl5+RzdXsqkLlw85SZzUhWn0Y95dIg558+nNtk1Yqmq8WLSkTAY2PjDOQEquGhMnvSzLN5Kk1QeUGwOqVIt3PzlJnNi+osskVUah6VJiuYGiha8bX10X3jjI6NE+TnohoZHWNA9Wd/VJqg+mV4bRY3T5kZMPuFgYrUFCYi2PSDXZPLpHZq+0aRprI9o2N9M7w2i4OGmc14Fnf1sUWbmMYPxKwn8rVavb6WyhyNbk4FMhdunjLrM1mLF81kNFDeanZDOSvWdarBOk1RExF87MLlfdsEVY+DhlkfyVpq9KJNO3JHQ2WNBsobITQ6Ns5hCwcz982XRaWByaGz9W76jZRLg3zkTafl9stUnrl2BcBemocxW26eMusjWbWEegKmrTNxRLmUm/7jyWeaN8O7NCj+23kvYvWKYTZvH6m7xgXAgCCrNawyE7xyPe/dtGNa01qQvFeVeRl2kGsaZn1kNvMIKinCl1/6DU5Yez2/eKq5+aKyDA+V2fjG0yYDRqW/pd7xR+Q0lx12yMGlV1evGM7ti+mHORez4ZqGWQ+rHRE1tKg0mSJjJqrXmWjl1K6hcokdl7xq8jreu2lH7jwLODgn5Ma79uZeZ20wGO7y5VdbzTUNsx6V1X/xxFP7J4e9drrKIke111FvWOyvHXcE12wbqTtjvTYY9POci9lwTcOsR9TWKp58evr62uMHAgELB8Uz87zq3Xw7tJR8p51JP8x373mk4Yzu2mDQz3MuZsNpRMx6wExTe3SaobRzvZnp04cdDGYkL42IaxpmHa7ITO2ZjorqNI8/tR+YPkFwbHxiXpIaLl5U6pmcUO3moGHWwYrO1O72kT71gsJExJxrHD3eoNJS7gg362BFZ2r38kifATHnWtRjTV5Wtp84aJh1sKLrNmSNAOoVeemqKjPCK/8OD5VZvCh7bkYvB9VWc/OUWQvNNJNs3oJH1TfB6txR7V5WtVXy1q3IW7/cw2fnj2saZi2SNW/i4mtvn7LUaK2zTl5Sd3v1c0ISMEoDYqA7pmLMWl4NrHqtD+eLao6OrWlIug94HJgA9kfESklHApuA44H7gDdFxKPtKqPZTNTrn8i6qW3ePsKV39+V+Vxfve1Bbrxrb2YtpFvSj89Fveamfk1Z3iqFahqSFkn6/yR9Mn18kqTXNrdoAJwVEcurxgqvBf4lIk4C/iV9bNYVZrKudKUGkTeqaHRsvO6s515WGpCbm9qoaPPUPwFPA/8ufbwb+GBTSlTfucBn0t8/A6xuQxnMZiXv23HW9rnOuyj1aMOzgI0XnOaaRBsV/WidGBF/CYwDRMQYB1PON0sA35C0TdK70m3PjYgH0zI8CDwn60RJ75K0VdLWvXv3NrmYZsXMJMfRXOddjB+Y0+lNNdcbhwNGexXt03hGUpl0YIakE0lqHs10ZkTskfQc4JuS7ip6YkRcBlwGSRqRZhXQbCaychyddfKSyeytlcc33rW37giobh0hNShxIIKhRSUikrkTS4fKPPrk0+wrGOU8dLb9igaNS4CvA8dKugI4E/gPzSoUQETsSf99WNKXgDOAn0o6OiIelHQ08HAzy2A236o7abNme19+ywMNn6MbAwYcnPX96L5xyqVBPnrhclavGOaEtdcXOt9DZztDoeapiPgmcB5JoLgSWBkRNzWrUJIOk/Tsyu/Aq4A7gOuAt6eHvR34crPKYNZs3Z4vqqisZVmrZ7Xn1R4WLyp56GwHKlTTkHQmsCMirpf0NuD9kj4eEfc3qVzPBb6k5MO2APh8RHxd0q3A1ZLeCTwAXNCk1zdrun4Y/VQv2WCl32bNqmWZE/Iued0pDhIdqGjz1N8Bp0k6DVgDfAr4LPBbzShURPwbcFrG9p8DL2/Ga5q10ubtI13bNzETExG511mpYVQCw7rr7pxcHfDQXh3+1QOK/mX2R7LwxrnAX0fEx4FnN69YZr1t45adPR8wKrKuM6t/4un9BzvDH9033nC2vLVH0ZrG45IuBt4GvFTSIJCdGczMGuaY6vZU5rMxVC5NjpiqfT9mOlve2qdo0LgQ+PfAOyPiIUnHARubVyyz7tVoDYzN20cYmIeFhbrNYYcsYMclr8rcN5PZ8tZehYJGRDwE/Peqxw+Q9GmYWY1Ga2DUSw/Sy+oFgCLZfK0zFM09dZ6kf5X0mKRfSHpc0i+aXTizTrJ5+whnbriBE9Zez5kbbshtb8+7OY6MjnHRph19Mcw2S70AMJPZ8tZeRZun/hJ4XUT8pJmFMetURZdd7aamp9IA7D/QmhFcIj/NO2TPlm+01oi1R9Gg8VMHDOtneU1Of3z1bcDBvopuanqaCPjohcsBuGjTjrrHlksDPLM/Gl5bJVVIuTQwJTVIANdsG2Hl847MDQROad4digaNrZI2AZupyjkVEdc2o1BmnSavyWkigjVfuI1Lv3Inj+7rrnWoD0TSv7L+vFMbHrv/QOOAAQdThWTlkvJoqN5QNGgcDuwjSedREYCDhvWFoUWl3KAwfiC6LmBUjI1PNKxlHLZwkCefmZ9+GI+G6n5FR0/9x2YXxKxTbd4+whNP7W93MdqiNMC8BQzwaKheUHT01DGSviTpYUk/lXSNpGOaXTizTrBxy86+WEI1S5GM5aVBsXhR47m+Hg3VG4o2T/0T8HkOJgh8W7rtlc0olFmrNJq5DW5SaWR8IohIgkL1YIHSoDhs4YLcWeDWnYoGjSUR8U9Vjz8t6aImlMesZfKG0W69/xFuvGvvZCA5olyaTKRn2R4bG+ejFy73kNk+UDRo/CxNiX5l+vgtwM+bUySz1sgbRnvFLQ9Mzl0YGR2jNChKA+rbJqoilg6VPWS2TxTNcvsO4E3AQ+nPG9NtZl0rr9mpNjSMTwSlwbmubN273FfRX4qOnnoAeH2Ty2I2K0X6JbLk5TvKUnQN637gvor+VnT01PMlfUXS3nQE1ZclPb/ZhTNrpNIvMTI6RnCwX6LIOgxZ+Y76tT4xoCR1eSPDQ2U2vvE0dlzyKu7d8BrWrFrGxi07G+bjst5RtHnq88DVwNHAUuALHOzfMGubRhllGzlkwdT/AuU+XTEuAta9/pRpQbSagJvXnj1Zq5hLwLbuVfR/iCLicxGxP/25nN5fqdK6wGzXYfjA5tu5aNOOaaOi+rUZqtKRvf68U1FOdeuImprIXAO2daeio6dulLQWuIokWFwIXC/pSICIeKRJ5TOrq+g6DNX9Hh5CO1V1R/bqFcO5ebRqg4kXTupPM1m5D+DdNdvfQRJE3L9hbbFm1bIpcy1g+miezdtHWPPF2xifSCrHDhhJU1OQ9FHUdmSP5uTRqt3uhZP6U9HRUyc0uyBms1FkHYY1X9hRKB1GvxiU+MibTssd8VQ0GBQJ2NZ7CgUNSRcAX4+IxyV9APg14C8iYntTS2eWqjestnZSWWWFvT2jYyxaONgXAWN4qMxZJy/hmm0jdVcGLJcGWX/eqXWHyBYNBl44qT8pCuTIl/SjiHiRpJcA64G/At4fEb/R7ALO1cqVK2Pr1q3tLobNQW26D8ieKwCw7ro7+7b5qVwa5PzTh6ekQDnr5CVTHhe9qc927ov1DknbImLltO0Fg8b2iFghaT1we0R8vrKtGYWdTw4a3e/MDTc0nIRXGhCIyX6LfjU8VObmtWe3uxjWA/KCRtEhtyOSPkGSSuRrkg6Zwblmc1JkNM74gej7gAEeuWTNV/TG/yZgC/DqiBgFjgTWNKtQZtU8Gqc4v1fWbIWCRkTsAx4GXpJu2g/8a7MKZVYtK91HvyqXBjlsYf574ZFL1mxFR09dAqwElpEsvlQCLgfObF7RzBK1o3SGFpV44qn9fZmqfGx8Ijc/Vrk04M5qa7qik/veAKwAfggQEXskPbtppapD0quBjwODwD9ExIZ2lMNaq3ZY7Vs/+T1uvqc/ExHkhcqn+mFssbVd0aDxTESEpACQdFgTy5RL0iDwtyTLzO4GbpV0XUT8uB3lsbnJG9a5efvIlKGzixeVuOR1p2Tu60eDEhMZox7dn2Gt0DBoSBLw1XT01JCk/0SSPuSTzS5chjOAuyPi39KyXQWcCzhodJl6S61u+sGuKU1Pj+4bZ80Xb2Pr/Y80nLzW68qlQX7tuCP47j2PTKlxeCa2tUrDjvBIJnKsBr4IXEPSr/FnEfE3zS1apmFgV9Xj3em2KSS9S9JWSVv37t3bssJZcXkZUq/8/q7MvorxieDyWx7o64ABcP7pw/zwgcemBAyl292fYa1QtHnqe8BoRLR7mG1WH+C0O0xEXAZcBsnkvmYXymYubz5BVrOLJYaHytx4195pgTOAG+/ylyNrjaLzNM4CvifpHkk/qvw0s2A5dgPHVj0+BtjThnLYHLn9fWYqzU9OR27tVjRo/DZwInA28Lqqn1a7FThJ0gmSFgJvBq5rQzlsjjz3orjFi0qTSQbzgq2DsLVK0dTo9ze7IEVExH5J7yGZnT4IfCoi7mxzsWwWqudeNMor1asqa1rkyUph7nTk1m5F+zQ6RkR8Dfhau8thc1e5GdbeBPuBgN888Uju+/kYI6Nj0wJIXgpzpyO3duu6oGHdp16a7axRVP0ggB8+8NhkYJhJKvLaiY5mrVQoNXo3c2r09spcC2NAPOvQBYzuG6/bPNMPnMrcOlVeanTXNKyp1l1357SaxPiB4NGcdaj7jUc9WbfxmhjWNJu3j/R1uo8iPOrJuo2DhjXNxi07212EjuZRT9aNHDSsadz0MlVpQCxeVEIkfRlZo6PMOp2DhjVNvze9DJVLDJVLk48nIunL8TBZ62YOGtY0/TzruzQgXnva0Ty9/+AaF5U8jJWMvpu3j7SpdGaz56Bh827z9hGWX/oNLtq0o+5Kc72gXBrkbS8+jsWLDtYohsolNl5wWmZywYqx8Qn3+VhX8pBbm1ebt4+w5gu3TUlv3stzMc4/fZgPrj6VD64+ddq+927aUfdc9/lYN3LQsFnJm8G8ccvOvlq7uzYlefX7MpCzwl5Fv/f5WHdy0LBCqm+GR5RLPPnMfsYnkhtipY0e+u/bc/X11s5+rxcwPNzWupWDRg+aSR6jIufU3gyzJuyNjU9w0aYduetX96rq2kJeHq3Ke1L5d9ijp6yLOWj0mKy1t9d84TYu/cqdjOYM98xbrxuYbHIqmlSwnwJGbW0hr5Z1IIL7NrymVcUyayqPnuoxWTf4Sq6nIHu4Z9563Zd+JVmqpN+anIoYlKZNzvMCSdYPHDR6TJEbfPVwz83bR3IXQXp03zjHr72eAfXyoNnZORAxrXkpa16K+y6s1zho9Jii32r3jI5NNks1ktfkNNhjsWQwDY6VfyspP7Jkvc+rVwyz/rxTGR4qO1WI9Sz3afSYrOVAsywdKs95AaSJHuq+yFspL2s9kHq1By+QZL3ONY0eU/ttd6hcolRTJRBw1slLeq6vYlCa/Ib/thcfN/ke1NOoRuDag9lUrmn0qCef3k+QDI9dWBM0Arhm2whDi0o9tRjSgQjuzRildOaGGzL7bYqumufag9lBDho9JiuNxzMZ7Uhj4xMcsmCA0qAmJ+l1uyAJEGedvIQb79o7OefkrJOXcM22kcJNTGaWz81TPWYmaTweGxvnsIW99b1hZHSMy295gJHRsckhxtdsG+H804fdxGQ2D3rrjmEz6qdYOlTuuX6NLGPjE9x4195CTVFmVp9rGj2m6JDbSvNMv0w864fgaNYKDho9Zs2qZZQGpo8ZGhCTI4kGJc4/Penc7faFkqpXxqun0t/hhY/M5sZBo8esXjHMxgtOm3IzPWzhIIPS5LoWExFcs22EzdtHJoeUDhaY9b14UamjPjDl0gDrXn9K4aDnFfPM5q6T7gE2T1avGGbHJa/ivg2v4b4Nr2Fo0cJpnePVqURWrxjmQJ1EgwNKAsbovnE6aRm+Q0uDk0GveuW8erxintncuCO8A80mtXm958gLB9VzF5YOlXNzUB0IJudztCKJbbk0yCELBjJTsFcbTctUmUfxgc23c8UtDzRcKdD9G2azp+jxVNYrV66MrVu3trsYhWWlrSgNiGcdumBaavO84JL1HHkWLypxyetOASh8znwrlwY48rBDplxHkfJkTc4rsnJe0Ul9Zv1M0raIWFm7veOapyStkzQiaUf6c07Vvosl3S1pp6RV7SxnsxRNbf6Bzbdz8bW3T5mPUGmvn0lOqUf3jU8mLayky5gv5VKxj5eAfc/sn7KtOn1H5Zipz52M/tq8fYQzN9zACWuv58wNNwBw89qzuXfDa/jIm05z1lmzedZxNQ1J64AnIuKvara/ELgSOANYCnwLeEFE1L07dltN44S11zdsXgFyV8gbTudezOavWllR7r2bdjQ8v8gKfYsXlXhq/MCMay9ZyQOzalUwvTZSe+58NPWZ9aO8mkY3BY2LASJiffp4C7AuIr5X7/m6JWhUbm55/QpFifr9E42UBpJgUG9SuYC3vvg4Nt26q24KEgEfvXD55HUJCgezIk1Ic80pZWb5uqZ5KvUeST+S9ClJi9Ntw8CuqmN2p9umkfQuSVslbd27d2+zyzpnlT6IuQYMYPLb9GznXowfqB8wABYMwOW3PNAwZ9XSoTKrVwxz89qzuW/Da/johcsLN38V6azOO8Yd3WbN05agIelbku7I+DkX+DvgRGA58CDwkcppGU+VedeKiMsiYmVErFyyZEkzLmFe1euDGCqXyJirl6vSN1B07sVsjB9ofExW30ElgBQJHEVmqnt5VbPWa0vQiIhXRMSvZvx8OSJ+GhETEXEA+CRJHwYkNYtjq57mGGBPq8veDHnfjAWse/0pDM4gajy6b5z3btrB1vsfqTv3olkBBZLJhE+NT3DRph2cePHX+MDmqasDNqoJlQbEvmf2T3Zu503G8/KqZq3Xcc1Tko6uevgG4I709+uAN0s6RNIJwEnAD1pdvmYYypmYNrSolGStnWHq8gCuuOWB3OddvKjEPevP4WMXLp/XFCLl0iBnnngkTz4zMWX2+eW3PDAlcGQtFFVZWnWoXAIxbbRYVuDwAklmrdeJHeGfI2maCuA+4N0R8WC670+BdwD7gYsi4p8bPV83dIQvv/QbmRPZhsolHhsbn9VIqMr5Tz6zPzPoVOZnbL3/ES6/5YFZvsJBw+m6FXnPNShxz/pzGo5mcue2WWfI6wjvuBnhEfE7dfZ9CPhQC4vTEo/lzHx+bGy84UioeiOSHhsb54hyKTMgVZqxFsyxrlkZ4gpMzvfIMhExbdJhpRYBTAYOd26bdbaOa57qR3kdtwMSZ528ZFoTUqU3YniozFtffFzu8x6R1lTyBMU6teHg+tvVTUnVzUGNJhRK2R3+tbmg3Llt1tk6rqbRj9asWpaZMqOSjfb804enLF9a26Rz7bbd7Mu4+zfK3VRU1mS7Wo1qAoLcGlP1uVnvhTu3zTqHg0abNUr7UWTVubGi1YVZqM5NdeaGG3IDV6NmtAORP4u8uhZReU7P4jbrTA4abVQ0sWCjb/FDi0qTWWjn2/Y/e1Whvoi82lK1rICRN5/DQcKsM7lPY57VJtCrt+BP0cSCA1Lu823ePsITT+3PObOxcmkwN7FgZRJekb6I2gSDRSxeVPIQWbMu46Axj6rTgTSaYwD5bfy1JiJyn2/jlp3TFlgqqtKRvf68F9WdJFd0RFNlxnfR+R+LFi5wwDDrMm6emkf1vpFn3RzrZYoVZK4HMZbOtN64ZSdrVi2b9VDUrHkPef0Ief0VeSOaavsl8kKah9GadR8HjXk00zkG9VKL37vhNZyw9vrc/ZVaR948jHpm2o8wmxFN1c+XN2HPw2jNuo+bp+bRTOcY5LX/V7Y3uqmOjU/w2FMzCxiz6UeYa7oO54gy6x2uacyjmX4jzzv+rJOXTH47b7QGRaMsMJXzh8olpGRd7UoH9kwDx2z7HzyM1qx3OGhkmO1qbzO9OWYdf9bJS6YsbjSXzGCDEh9502kADYfMVjRrpTsPozXrDR2XsHC+zTRhYdbciSIzoufLij//xrzNuRBJ30jRJIDtvnYz6xzdtnJf2xSZk9BM8zlJr9InUrSDvt3Xbmadz0GjRjdkWRVw5olH1p0LUd2XUrSDvhuu3czay0GjRjOyrM5klvhQOXvhpGoB3PfzsdyFjGpHNxUdveQMs2bWiDvCa8x3ltUieZuqrXv9Kaz5wm0NZ3mPjI7NqcM963hnmDWzRhw0asz38NCZzhKvff2sWeEVlc7tRoGosr16X6X2k3WNHhprZnkcNDLM5/DQRv0EeUNcK6+/efsI7920o+HQ23qBqFaj2o+DhJnlcZ9Gk+X1BwTJ8No1X7itboLD1SuGC8/VKNph7VFSZjZbDhpNtmbVMkqDytz36L7xaX0XWTfvounGi3ZYe5SUmc2Wg0aTrV4xzGELZ9YKWHvzzhr9VGsmHdYeJWVms+Wg0QKPzTALbe3NOyth4NtefJwTCJpZy7kjvAUarZ9dLe/mPZ8d1B4lZWaz5aDRAvXWzy4NisMWLuCxsfGW3rw9SsrMZsNBowWqv9mPjI5Nrtg3PE9BolmZac3MajlotEizvtnPdMa5mdlcuCO8y3nOhZm1kmsaM9CJzUCec2FmrdSWmoakCyTdKemApJU1+y6WdLeknZJWVW0/XdLt6b6/lpQ9Y65JKs1A9WZvt4PnXJhZK7WreeoO4DzgO9UbJb0QeDNwCvBq4H9Kqkwo+DvgXcBJ6c+rW1ZaOrcZyHMuzKyV2tI8FRE/AcioLJwLXBURTwP3SrobOEPSfcDhEfG99LzPAquBf25VmTu1GchzLsyslTqtT2MYuKXq8e5023j6e+32pqv0Y+QlDeyEZiDPuTCzVmla0JD0LeCXMnb9aUR8Oe+0jG1RZ3vea7+LpCmL4447rkFJ89UOZ63lZiAz6zdNCxoR8YpZnLYbOLbq8THAnnT7MRnb8177MuAygJUrVxbNLD5NVj9GxXxNzDMz6yadNk/jOuDNkg6RdAJJh/cPIuJB4HFJL05HTf0ukFdbmTd5/RUCbl57tgOGmfWddg25fYOk3cC/A66XtAUgIu4ErgZ+DHwd+IOIqHzV/8/APwB3A/fQgk5wD2c1M5uqXaOnvgR8KWffh4APZWzfCvxqk4s2RVaiQfdjmFk/67TRUx3Fw1nNzKZy0GjAw1nNzA7qtI5wMzPrYA4aZmZWmIOGmZkV5qBhZmaFOWiYmVlhiph1lo2uIGkvcH/68CjgZ20sznzqpWuB3rqeXroW6K3r6aVrgeZez/MiYkntxp4PGtUkbY2IlY2P7Hy9dC3QW9fTS9cCvXU9vXQt0J7rcfOUmZkV5qBhZmaF9VvQuKzdBZhHvXQt0FvX00vXAr11Pb10LdCG6+mrPg0zM5ubfqtpmJnZHDhomJlZYX0XNCT9haQfSdoh6RuSlra7TLMlaaOku9Lr+ZKkoXaXaS4kXSDpTkkHJHXlsEhJr5a0U9Ldkta2uzxzIelTkh6WdEe7yzJXko6VdKOkn6SfsT9qd5lmS9Khkn4g6bb0Wi5t6ev3W5+GpMMj4hfp738IvDAifr/NxZoVSa8CboiI/ZI+DBAR72tzsWZN0q8AB4BPAH+SLrzVNSQNAv8HeCXJuva3Am+JiB+3tWCzJOmlwBPAZyOipQugzTdJRwNHR8QPJT0b2Aas7sa/Tbrk9WER8YSkEvC/gT+KiFta8fp9V9OoBIzUYUDXRs2I+EZE7E8f3gIc087yzFVE/CQidra7HHNwBnB3RPxbRDwDXAWc2+YyzVpEfAd4pN3lmA8R8WBE/DD9/XHgJ0BXLpQTiSfSh6X0p2X3sb4LGgCSPiRpF/BW4M/aXZ558g5asG661TUM7Kp6vJsuvTH1MknHAyuA77e5KLMmaVDSDuBh4JsR0bJr6cmgIelbku7I+DkXICL+NCKOBa4A3tPe0tbX6FrSY/4U2E9yPR2tyPV0MWVs69qabC+S9CzgGuCimlaHrhIRExGxnKR14QxJLWs+7MnlXiPiFQUP/TxwPXBJE4szJ42uRdLbgdcCL48u6KCawd+mG+0Gjq16fAywp01lsRpp+/81wBURcW27yzMfImJU0k3Aq4GWDFjoyZpGPZJOqnr4euCudpVlriS9Gngf8PqI2Nfu8hi3AidJOkHSQuDNwHVtLpMx2Xn8j8BPIuK/t7s8cyFpSWWkpKQy8ApaeB/rx9FT1wDLSEbp3A/8fkSMtLdUsyPpbuAQ4Ofpplu6dSQYgKQ3AH8DLAFGgR0RsaqthZohSecAHwMGgU9FxIfaW6LZk3Ql8DKS9Ns/BS6JiH9sa6FmSdJLgP8F3E7yfx/g/RHxtfaVanYkvQj4DMlnbAC4OiL+vGWv329Bw8zMZq/vmqfMzGz2HDTMzKwwBw0zMyvMQcPMzApz0DAzs8IcNMxaRNIfpllWr0gf/7qkCUlvbHfZzIrqyRnhZh3qvwC/HRH3phlxPwxsaXOZzGbEQcOsBST9PfB84DpJnyLJSXUN8OttLZjZDDlomLVARPx+mvblLJJZ/J8HzsZBw7qM+zTMWu9jwPsiYqLdBTGbKdc0zFpvJXBVkkOPo4BzJO2PiM1tLZVZAQ4aZi0WESdUfpf0aeCrDhjWLdw8ZWZmhTnLrZmZFeaahpmZFeagYWZmhTlomJlZYQ4aZmZWmIOGmZkV5qBhZmaFOWiYmVlh/z8arwpfbRUBHAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's plot f4 & response, cuz f4 corr value is close to 1\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(dt.f4, dt.response)\n",
    "plt.xlabel('f4')\n",
    "plt.ylabel('response')\n",
    "plt.title('relationship between f4 & response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         f1        f2        f3        f4        f5   response\n0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.764216</td>\n      <td>-1.016209</td>\n      <td>0.149410</td>\n      <td>-0.050119</td>\n      <td>-0.578127</td>\n      <td>6.242514</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.763880</td>\n      <td>-1.159509</td>\n      <td>-0.721492</td>\n      <td>-0.654067</td>\n      <td>-0.431670</td>\n      <td>-8.118241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.519329</td>\n      <td>-0.664621</td>\n      <td>-1.694904</td>\n      <td>1.339779</td>\n      <td>0.182764</td>\n      <td>66.722455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.177388</td>\n      <td>0.515623</td>\n      <td>0.135144</td>\n      <td>-0.647634</td>\n      <td>-0.405631</td>\n      <td>-27.716793</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.104022</td>\n      <td>0.749665</td>\n      <td>-0.939338</td>\n      <td>-0.090725</td>\n      <td>-0.639963</td>\n      <td>8.192075</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine each column to be processed\n",
    "columns = ['f1','f2','f3','f4','f5','response']\n",
    "dt = dt.loc[:, columns]\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the training and test set with the ratio of 8:2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "features = ['f1','f2','f3','f4','f5'] # Data that we want to utilize as training & test\n",
    "#X = dt.loc[:, features] # X are the data we want to use from 'features' = independent variable\n",
    "#y = dt.loc[:, ['response']] # y is the data we want to use as target = dependent variable\n",
    "\n",
    "X_data = np.array(dt.iloc[:,4])\n",
    "y_data = np.array(dt.iloc[:,-1])\n",
    "\n",
    "#X = dt[['f1','f2','f3','f4','f5']]\n",
    "#y = dt['response']\n",
    "#y = np.array((y-y.mean())/y.std())\n",
    "#X = X.apply(lambda rec:(rec-rec.mean())/rec.std(),axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=42, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.43233578e-01  1.29708222e-01 -5.05474951e-01 -1.45673015e+00\n",
      " -2.87227313e-01  1.02388856e+00 -6.30259835e-01  6.47133487e-01\n",
      " -4.51004082e-01 -1.25182980e+00 -5.37871808e-01 -8.54856516e-01\n",
      "  9.53672927e-01  1.14084691e+00 -1.24760250e-01  8.44818951e-01\n",
      "  2.80146667e-01 -1.45314479e+00  1.59063828e-01  1.37371403e+00\n",
      " -2.21643200e-01 -1.12204740e+00  6.34058371e-01  2.10690554e+00\n",
      "  1.01310187e+00 -6.50705320e-01  5.65847620e-01  1.15138314e+00\n",
      " -1.33014896e+00  5.59302961e-01 -1.58885135e+00 -8.00119911e-01\n",
      "  5.19013886e-01  1.19656250e+00 -1.77125048e+00 -2.20357270e-02\n",
      " -7.91072058e-01  5.66733883e-01 -7.44129727e-01  7.47275491e-01\n",
      "  9.14018250e-01 -2.16927780e+00  1.20780102e+00 -8.26788157e-01\n",
      " -6.40783958e-01 -6.01851880e-02 -3.46261000e-04  7.99828126e-01\n",
      " -5.44331315e-01  6.58181930e-02  9.77098785e-01  1.16675177e+00\n",
      "  9.54000000e-05 -1.01367488e+00  6.21544516e-01 -8.46025188e-01\n",
      " -2.73838080e-02  8.55309243e-01  2.60936170e-01 -3.90571000e-04\n",
      "  1.50731816e+00  4.88832872e-01  1.22789944e+00  1.12329830e+00\n",
      " -1.76298486e+00 -2.17497534e+00  5.84699331e-01  1.16853441e+00\n",
      "  4.99692070e-02 -7.26402818e-01  2.51235055e-01  7.83101446e-01\n",
      " -5.55211603e-01  1.08106378e+00 -7.03911174e-01 -2.02602740e+00\n",
      " -7.69127200e-01 -2.41834030e-02  8.37936395e-01  1.59743491e+00\n",
      "  4.12881789e-01 -1.06440437e+00  2.63913589e-01 -8.02571617e-01\n",
      "  1.22629307e+00  4.08989790e-02 -2.00138650e-01  1.05053870e-02\n",
      " -1.09048142e+00 -6.43312938e-01  1.09057727e+00 -8.74027247e-01\n",
      "  6.33029099e-01 -1.46862980e-02 -1.02204765e+00  2.28478650e-02\n",
      " -2.60222900e-01 -1.33829154e-01  4.59418695e-01  1.24161849e+00\n",
      "  6.22094979e-01  1.53224836e+00  6.38300448e-01  7.46010449e-01\n",
      " -7.44180695e-01 -6.62743940e-02 -3.53848587e-01 -1.49828218e+00\n",
      " -3.33476890e-02  6.28614675e-01  3.35892736e-01  1.47194462e+00\n",
      "  1.49539087e+00 -9.45311963e-01  4.43781395e-01  1.27711982e+00\n",
      " -6.34115487e-01 -9.72576918e-01  6.78997801e-01 -1.83870326e-01\n",
      " -1.77417142e+00  4.98388862e-01  2.47776525e-01 -6.05436078e-01\n",
      "  2.13061780e+00 -2.42011300e-03 -9.12972546e-01 -7.85679636e-01\n",
      "  4.73409901e-01 -7.33525750e-01 -2.65990735e+00  1.86451516e+00\n",
      " -1.42291351e+00  4.30062768e-01 -1.71464901e+00  5.93636270e-02\n",
      " -5.37821548e-01 -1.73544688e+00  1.06748872e+00  1.38224967e+00\n",
      " -3.36426597e-01  1.44145057e-01  8.76084090e-02  9.91543455e-01\n",
      "  5.50265343e-01  7.82265427e-01  1.11287157e+00 -3.31299190e-01\n",
      "  2.18240407e-01 -1.81963196e-01  6.12313719e-01  1.03844092e-01\n",
      " -2.99125360e-01 -1.11050819e-01 -5.22266200e-02  8.03656170e-02\n",
      "  6.57882435e-01  2.45330541e-01  9.04772040e-01 -8.74078744e-01\n",
      "  6.49126590e-01 -6.82447804e-01 -1.33307323e+00  1.25671463e+00\n",
      "  1.15939679e+00 -1.67231506e+00 -1.59374355e+00  4.29808229e-01\n",
      "  7.29526753e-01  1.53974521e+00 -1.34436430e+00  1.56559415e-01\n",
      " -9.68412234e-01 -4.61751874e-01 -1.60124715e-01 -3.88180302e-01\n",
      "  5.79467802e-01  1.78878456e+00 -3.17013494e-01 -7.85438107e-01\n",
      "  3.00773425e+00  8.75802676e-01 -1.19182762e+00 -1.95613891e+00\n",
      " -1.10720048e+00  1.92165305e+00  3.12903550e-02 -1.76578663e+00\n",
      " -1.79890143e-01 -1.29499697e+00  7.78856321e-01  1.80907311e+00\n",
      "  2.07775026e+00 -4.58695614e-01 -7.39881474e-01  4.44583925e-01\n",
      "  1.08175252e+00 -1.61078333e+00 -5.24773405e-01  5.08284917e-01]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "#print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.1 # Set learning rate to 0.1\n",
    "max_epoch = 1500 # Set max iteration to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) (800,)\n",
      "(200,) (200,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "def loss_fn(y, yhat):\n",
    "    loss = np.sum((y-yhat)**2)/len(y)\n",
    "    return loss\n",
    "    #loss = mean_squared_error(y, yhat)\n",
    "    #return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "def train_model(X, y, alpha, max_epoch):\n",
    "    w = b = 0\n",
    "    n = len(X)\n",
    "    losses = []\n",
    "    weights = []\n",
    "\n",
    "    def prediction(w, X):\n",
    "            yhat = (w * X) + b\n",
    "            return yhat;\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        y_predict = prediction(w, X)\n",
    "        loss = loss_fn(y, y_predict)\n",
    "\n",
    "        losses.append(loss)\n",
    "        weights.append(w)\n",
    "\n",
    "        loss_fn(y, y_predict)\n",
    "\n",
    "        wd = -(2/n)*sum(X*(y-y_predict))\n",
    "        bd = -(2/n)*sum(y-y_predict)\n",
    "\n",
    "        w = w - alpha * wd\n",
    "        b = b - alpha * bd\n",
    "\n",
    "        print(f\"Iteration {i+1}: Loss {loss}, Weight {w}, Bias {b}\");\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(weights, losses)\n",
    "    plt.scatter(weights, losses, marker='o', color='red')\n",
    "    plt.title(\"Loss vs Weights\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Weight\")\n",
    "    plt.show()\n",
    "\n",
    "    return w, b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss 1721.3069867699319, Weight -0.20427783377291586, Bias 2.4463995536742518\n",
      "Iteration 2: Loss 1667.0651748347786, Weight -0.368581914746362, Bias 4.403642592126182\n",
      "Iteration 3: Loss 1632.3434607882796, Weight -0.5007281326604676, Bias 5.969536271963891\n",
      "Iteration 4: Loss 1610.1170947995126, Weight -0.6070054786864416, Bias 7.222331039718946\n",
      "Iteration 5: Loss 1595.8893488516874, Weight -0.6924740662765135, Bias 8.224631051527505\n",
      "Iteration 6: Loss 1586.7817462015198, Weight -0.7612050405279487, Bias 9.026522688896852\n",
      "Iteration 7: Loss 1580.9516948796704, Weight -0.8164736955755654, Bias 9.668077516237656\n",
      "Iteration 8: Loss 1577.219700264532, Weight -0.8609149170429552, Bias 10.181354763543707\n",
      "Iteration 9: Loss 1574.8307342913547, Weight -0.8966482921364797, Bias 10.592003406431886\n",
      "Iteration 10: Loss 1573.301481640302, Weight -0.9253788004776791, Bias 10.920543905747774\n",
      "Iteration 11: Loss 1572.3225579468633, Weight -0.9484778472621747, Bias 11.183393660073191\n",
      "Iteration 12: Loss 1571.695917020262, Weight -0.9670484728161288, Bias 11.393687416680887\n",
      "Iteration 13: Loss 1571.2947834707975, Weight -0.9819778255770275, Bias 11.561933639688593\n",
      "Iteration 14: Loss 1571.038004455056, Weight -0.9939793838798138, Bias 11.696539636278972\n",
      "Iteration 15: Loss 1570.8736314921862, Weight -1.003626927416059, Bias 11.804231683179934\n",
      "Iteration 16: Loss 1570.7684107053587, Weight -1.0113818690748597, Bias 11.890391148369604\n",
      "Iteration 17: Loss 1570.701055213459, Weight -1.0176152437155677, Bias 11.959323404950302\n",
      "Iteration 18: Loss 1570.657938586101, Weight -1.022625397476236, Bias 12.014472975530188\n",
      "Iteration 19: Loss 1570.6303380922132, Weight -1.026652217573872, Bias 12.058595658413948\n",
      "Iteration 20: Loss 1570.6126700176642, Weight -1.0298885786052958, Bias 12.093896237150949\n",
      "Iteration 21: Loss 1570.6013600363117, Weight -1.0324895493793347, Bias 12.122138655087989\n",
      "Iteration 22: Loss 1570.5941200982863, Weight -1.0345797980746914, Bias 12.14473416057295\n",
      "Iteration 23: Loss 1570.5894855426511, Weight -1.0362595480074983, Bias 12.162811827590861\n",
      "Iteration 24: Loss 1570.586518787939, Weight -1.037609367468763, Bias 12.177274975870365\n",
      "Iteration 25: Loss 1570.5846196545515, Weight -1.0386940217016278, Bias 12.188846309862239\n",
      "Iteration 26: Loss 1570.5834039456915, Weight -1.0395655705121123, Bias 12.198104032249024\n",
      "Iteration 27: Loss 1570.5826257228427, Weight -1.040265859135498, Bias 12.205510736623854\n",
      "Iteration 28: Loss 1570.5821275516, Weight -1.0408285211155195, Bias 12.21143652313816\n",
      "Iteration 29: Loss 1570.5818086522686, Weight -1.0412805887277774, Bias 12.216177492229669\n",
      "Iteration 30: Loss 1570.581604511926, Weight -1.0416437877915625, Bias 12.219970540577608\n",
      "Iteration 31: Loss 1570.5814738333645, Weight -1.0419355786794846, Bias 12.223005198648996\n",
      "Iteration 32: Loss 1570.5813901806337, Weight -1.042169993238738, Bias 12.22543310136452\n",
      "Iteration 33: Loss 1570.5813366310415, Weight -1.042358307607339, Bias 12.227375565136757\n",
      "Iteration 34: Loss 1570.581302351699, Weight -1.0425095830812359, Bias 12.228929649907213\n",
      "Iteration 35: Loss 1570.5812804080388, Weight -1.0426311008919082, Bias 12.230173009102629\n",
      "Iteration 36: Loss 1570.5812663609636, Weight -1.042728711689703, Bias 12.23116776986268\n",
      "Iteration 37: Loss 1570.5812573688245, Weight -1.0428071164548078, Bias 12.231963637433234\n",
      "Iteration 38: Loss 1570.5812516125648, Weight -1.0428700922817493, Bias 12.232600378850647\n",
      "Iteration 39: Loss 1570.5812479277301, Weight -1.0429206738485794, Bias 12.233109810025583\n",
      "Iteration 40: Loss 1570.581245568904, Weight -1.0429612992631316, Bias 12.233517385519695\n",
      "Iteration 41: Loss 1570.5812440589145, Weight -1.0429939272748534, Bias 12.233843470455064\n",
      "Iteration 42: Loss 1570.581243092302, Weight -1.043020131470629, Bias 12.234104358112546\n",
      "Iteration 43: Loss 1570.5812424735295, Weight -1.0430411759713276, Bias 12.234313084067367\n",
      "Iteration 44: Loss 1570.5812420774246, Weight -1.043058076259983, Bias 12.234480077543306\n",
      "Iteration 45: Loss 1570.5812418238597, Weight -1.0430716480603455, Bias 12.234613682532801\n",
      "Iteration 46: Loss 1570.5812416615413, Weight -1.0430825466119062, Bias 12.234720574722543\n",
      "Iteration 47: Loss 1570.5812415576336, Weight -1.0430912982271872, Bias 12.234806095057685\n",
      "Iteration 48: Loss 1570.581241491117, Weight -1.0430983256470008, Bias 12.234874516612276\n",
      "Iteration 49: Loss 1570.5812414485367, Weight -1.0431039684118928, Bias 12.234929258100912\n",
      "Iteration 50: Loss 1570.5812414212792, Weight -1.043108499228838, Bias 12.234973054700374\n",
      "Iteration 51: Loss 1570.5812414038303, Weight -1.0431121371200354, Bias 12.235008094716816\n",
      "Iteration 52: Loss 1570.5812413926603, Weight -1.0431150579861403, Bias 12.235036128927465\n",
      "Iteration 53: Loss 1570.58124138551, Weight -1.0431174030920827, Bias 12.235058558060354\n",
      "Iteration 54: Loss 1570.5812413809326, Weight -1.0431192858838043, Bias 12.235076502783244\n",
      "Iteration 55: Loss 1570.5812413780025, Weight -1.0431207974640453, Bias 12.23509085969887\n",
      "Iteration 56: Loss 1570.581241376127, Weight -1.0431220109908172, Bias 12.235102346144451\n",
      "Iteration 57: Loss 1570.5812413749259, Weight -1.0431229852104147, Bias 12.235111536033957\n",
      "Iteration 58: Loss 1570.5812413741573, Weight -1.043123767295165, Bias 12.235118888534046\n",
      "Iteration 59: Loss 1570.5812413736653, Weight -1.0431243951226752, Bias 12.235124771006541\n",
      "Iteration 60: Loss 1570.5812413733502, Weight -1.0431248991064364, Bias 12.235129477363781\n",
      "Iteration 61: Loss 1570.5812413731485, Weight -1.0431253036660535, Bias 12.235133242754008\n",
      "Iteration 62: Loss 1570.5812413730196, Weight -1.0431256284080141, Bias 12.235136255310566\n",
      "Iteration 63: Loss 1570.5812413729366, Weight -1.0431258890739503, Bias 12.235138665551975\n",
      "Iteration 64: Loss 1570.5812413728836, Weight -1.0431260983021677, Bias 12.23514059390256\n",
      "Iteration 65: Loss 1570.5812413728502, Weight -1.0431262662391814, Bias 12.235142136709415\n",
      "Iteration 66: Loss 1570.5812413728283, Weight -1.0431264010308043, Bias 12.235143371056342\n",
      "Iteration 67: Loss 1570.5812413728145, Weight -1.0431265092164843, Bias 12.235144358615305\n",
      "Iteration 68: Loss 1570.5812413728056, Weight -1.043126596045954, Bias 12.235145148727826\n",
      "Iteration 69: Loss 1570.5812413728, Weight -1.0431266657334899, Bias 12.235145780870292\n",
      "Iteration 70: Loss 1570.5812413727963, Weight -1.0431267216620645, Bias 12.23514628662636\n",
      "Iteration 71: Loss 1570.581241372794, Weight -1.0431267665472672, Bias 12.235146691265\n",
      "Iteration 72: Loss 1570.5812413727929, Weight -1.0431268025689076, Bias 12.235147015003024\n",
      "Iteration 73: Loss 1570.5812413727917, Weight -1.0431268314766884, Bias 12.235147274015203\n",
      "Iteration 74: Loss 1570.581241372791, Weight -1.0431268546750374, Bias 12.235147481242407\n",
      "Iteration 75: Loss 1570.5812413727906, Weight -1.0431268732912153, Bias 12.235147647038184\n",
      "Iteration 76: Loss 1570.5812413727906, Weight -1.0431268882299976, Bias 12.23514777968605\n",
      "Iteration 77: Loss 1570.5812413727901, Weight -1.0431269002175692, Bias 12.235147885813367\n",
      "Iteration 78: Loss 1570.5812413727901, Weight -1.0431269098367608, Bias 12.235147970722462\n",
      "Iteration 79: Loss 1570.58124137279, Weight -1.043126917555342, Bias 12.235148038655549\n",
      "Iteration 80: Loss 1570.58124137279, Weight -1.0431269237487237, Bias 12.23514809300668\n",
      "Iteration 81: Loss 1570.58124137279, Weight -1.0431269287181926, Bias 12.235148136491327\n",
      "Iteration 82: Loss 1570.58124137279, Weight -1.0431269327055375, Bias 12.235148171282047\n",
      "Iteration 83: Loss 1570.58124137279, Weight -1.0431269359047968, Bias 12.235148199117031\n",
      "Iteration 84: Loss 1570.58124137279, Weight -1.0431269384716846, Bias 12.23514822138695\n",
      "Iteration 85: Loss 1570.58124137279, Weight -1.0431269405311587, Bias 12.235148239204436\n",
      "Iteration 86: Loss 1570.58124137279, Weight -1.0431269421834921, Bias 12.23514825345967\n",
      "Iteration 87: Loss 1570.58124137279, Weight -1.043126943509149, Bias 12.235148264864854\n",
      "Iteration 88: Loss 1570.58124137279, Weight -1.0431269445726972, Bias 12.235148273989802\n",
      "Iteration 89: Loss 1570.58124137279, Weight -1.0431269454259449, Bias 12.235148281290403\n",
      "Iteration 90: Loss 1570.58124137279, Weight -1.0431269461104637, Bias 12.2351482871314\n",
      "Iteration 91: Loss 1570.58124137279, Weight -1.04312694665961, Bias 12.235148291804611\n",
      "Iteration 92: Loss 1570.58124137279, Weight -1.0431269471001479, Bias 12.235148295543512\n",
      "Iteration 93: Loss 1570.58124137279, Weight -1.0431269474535514, Bias 12.235148298534899\n",
      "Iteration 94: Loss 1570.58124137279, Weight -1.0431269477370502, Bias 12.23514830092822\n",
      "Iteration 95: Loss 1570.58124137279, Weight -1.0431269479644671, Bias 12.235148302843049\n",
      "Iteration 96: Loss 1570.58124137279, Weight -1.0431269481468943, Bias 12.23514830437505\n",
      "Iteration 97: Loss 1570.58124137279, Weight -1.0431269482932293, Bias 12.235148305600761\n",
      "Iteration 98: Loss 1570.58124137279, Weight -1.0431269484106114, Bias 12.235148306581419\n",
      "Iteration 99: Loss 1570.58124137279, Weight -1.0431269485047665, Bias 12.235148307366016\n",
      "Iteration 100: Loss 1570.58124137279, Weight -1.0431269485802903, Bias 12.23514830799375\n",
      "Iteration 101: Loss 1570.58124137279, Weight -1.0431269486408679, Bias 12.235148308495983\n",
      "Iteration 102: Loss 1570.58124137279, Weight -1.0431269486894568, Bias 12.235148308897806\n",
      "Iteration 103: Loss 1570.58124137279, Weight -1.0431269487284285, Bias 12.235148309219293\n",
      "Iteration 104: Loss 1570.58124137279, Weight -1.0431269487596866, Bias 12.235148309476505\n",
      "Iteration 105: Loss 1570.58124137279, Weight -1.043126948784757, Bias 12.235148309682295\n",
      "Iteration 106: Loss 1570.58124137279, Weight -1.0431269488048645, Bias 12.235148309846942\n",
      "Iteration 107: Loss 1570.58124137279, Weight -1.0431269488209913, Bias 12.23514830997867\n",
      "Iteration 108: Loss 1570.58124137279, Weight -1.0431269488339252, Bias 12.235148310084066\n",
      "Iteration 109: Loss 1570.58124137279, Weight -1.043126948844298, Bias 12.235148310168388\n",
      "Iteration 110: Loss 1570.58124137279, Weight -1.0431269488526176, Bias 12.235148310235852\n",
      "Iteration 111: Loss 1570.58124137279, Weight -1.0431269488592896, Bias 12.235148310289828\n",
      "Iteration 112: Loss 1570.58124137279, Weight -1.0431269488646404, Bias 12.235148310333013\n",
      "Iteration 113: Loss 1570.58124137279, Weight -1.0431269488689314, Bias 12.235148310367565\n",
      "Iteration 114: Loss 1570.58124137279, Weight -1.0431269488723722, Bias 12.235148310395209\n",
      "Iteration 115: Loss 1570.58124137279, Weight -1.0431269488751311, Bias 12.235148310417326\n",
      "Iteration 116: Loss 1570.58124137279, Weight -1.0431269488773447, Bias 12.235148310435022\n",
      "Iteration 117: Loss 1570.58124137279, Weight -1.0431269488791193, Bias 12.23514831044918\n",
      "Iteration 118: Loss 1570.58124137279, Weight -1.0431269488805426, Bias 12.235148310460508\n",
      "Iteration 119: Loss 1570.58124137279, Weight -1.0431269488816841, Bias 12.23514831046957\n",
      "Iteration 120: Loss 1570.58124137279, Weight -1.043126948882599, Bias 12.235148310476822\n",
      "Iteration 121: Loss 1570.58124137279, Weight -1.0431269488833328, Bias 12.235148310482623\n",
      "Iteration 122: Loss 1570.58124137279, Weight -1.0431269488839208, Bias 12.235148310487265\n",
      "Iteration 123: Loss 1570.58124137279, Weight -1.0431269488843924, Bias 12.235148310490978\n",
      "Iteration 124: Loss 1570.58124137279, Weight -1.0431269488847705, Bias 12.235148310493948\n",
      "Iteration 125: Loss 1570.58124137279, Weight -1.043126948885074, Bias 12.235148310496324\n",
      "Iteration 126: Loss 1570.58124137279, Weight -1.0431269488853174, Bias 12.235148310498227\n",
      "Iteration 127: Loss 1570.58124137279, Weight -1.0431269488855126, Bias 12.235148310499747\n",
      "Iteration 128: Loss 1570.58124137279, Weight -1.0431269488856691, Bias 12.235148310500964\n",
      "Iteration 129: Loss 1570.58124137279, Weight -1.0431269488857946, Bias 12.235148310501938\n",
      "Iteration 130: Loss 1570.58124137279, Weight -1.0431269488858954, Bias 12.235148310502717\n",
      "Iteration 131: Loss 1570.58124137279, Weight -1.0431269488859765, Bias 12.235148310503341\n",
      "Iteration 132: Loss 1570.58124137279, Weight -1.043126948886041, Bias 12.23514831050384\n",
      "Iteration 133: Loss 1570.58124137279, Weight -1.0431269488860924, Bias 12.23514831050424\n",
      "Iteration 134: Loss 1570.58124137279, Weight -1.043126948886134, Bias 12.23514831050456\n",
      "Iteration 135: Loss 1570.58124137279, Weight -1.0431269488861679, Bias 12.235148310504815\n",
      "Iteration 136: Loss 1570.58124137279, Weight -1.0431269488861952, Bias 12.23514831050502\n",
      "Iteration 137: Loss 1570.58124137279, Weight -1.043126948886216, Bias 12.235148310505183\n",
      "Iteration 138: Loss 1570.5812413727897, Weight -1.0431269488862334, Bias 12.235148310505314\n",
      "Iteration 139: Loss 1570.58124137279, Weight -1.0431269488862465, Bias 12.23514831050542\n",
      "Iteration 140: Loss 1570.58124137279, Weight -1.0431269488862578, Bias 12.235148310505503\n",
      "Iteration 141: Loss 1570.58124137279, Weight -1.0431269488862662, Bias 12.23514831050557\n",
      "Iteration 142: Loss 1570.58124137279, Weight -1.043126948886273, Bias 12.235148310505624\n",
      "Iteration 143: Loss 1570.58124137279, Weight -1.0431269488862787, Bias 12.235148310505666\n",
      "Iteration 144: Loss 1570.58124137279, Weight -1.043126948886283, Bias 12.2351483105057\n",
      "Iteration 145: Loss 1570.58124137279, Weight -1.0431269488862869, Bias 12.235148310505727\n",
      "Iteration 146: Loss 1570.58124137279, Weight -1.0431269488862898, Bias 12.235148310505748\n",
      "Iteration 147: Loss 1570.58124137279, Weight -1.0431269488862922, Bias 12.235148310505766\n",
      "Iteration 148: Loss 1570.58124137279, Weight -1.0431269488862944, Bias 12.23514831050578\n",
      "Iteration 149: Loss 1570.58124137279, Weight -1.0431269488862964, Bias 12.23514831050579\n",
      "Iteration 150: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.2351483105058\n",
      "Iteration 151: Loss 1570.58124137279, Weight -1.0431269488862989, Bias 12.235148310505807\n",
      "Iteration 152: Loss 1570.58124137279, Weight -1.0431269488862995, Bias 12.235148310505812\n",
      "Iteration 153: Loss 1570.58124137279, Weight -1.0431269488863, Bias 12.235148310505817\n",
      "Iteration 154: Loss 1570.58124137279, Weight -1.0431269488863, Bias 12.23514831050582\n",
      "Iteration 155: Loss 1570.5812413727901, Weight -1.0431269488862998, Bias 12.235148310505824\n",
      "Iteration 156: Loss 1570.58124137279, Weight -1.0431269488862995, Bias 12.235148310505828\n",
      "Iteration 157: Loss 1570.58124137279, Weight -1.043126948886299, Bias 12.23514831050583\n",
      "Iteration 158: Loss 1570.58124137279, Weight -1.0431269488862989, Bias 12.235148310505831\n",
      "Iteration 159: Loss 1570.58124137279, Weight -1.0431269488862984, Bias 12.235148310505833\n",
      "Iteration 160: Loss 1570.58124137279, Weight -1.0431269488862982, Bias 12.235148310505833\n",
      "Iteration 161: Loss 1570.58124137279, Weight -1.043126948886298, Bias 12.235148310505833\n",
      "Iteration 162: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 163: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 164: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 165: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 166: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 167: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 168: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 169: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 170: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 171: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 172: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 173: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 174: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 175: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 176: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 177: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 178: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 179: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 180: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 181: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 182: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 183: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 184: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 185: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 186: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 187: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 188: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 189: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 190: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 191: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 192: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 193: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 194: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 195: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 196: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 197: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 198: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 199: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 200: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 201: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 202: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 203: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 204: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 205: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 206: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 207: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 208: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 209: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 210: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 211: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 212: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 213: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 214: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 215: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 216: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 217: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 218: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 219: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 220: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 221: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 222: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 223: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 224: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 225: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 226: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 227: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 228: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 229: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 230: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 231: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 232: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 233: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 234: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 235: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 236: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 237: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 238: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 239: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 240: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 241: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 242: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 243: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 244: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 245: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 246: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 247: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 248: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 249: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 250: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 251: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 252: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 253: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 254: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 255: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 256: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 257: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 258: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 259: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 260: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 261: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 262: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 263: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 264: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 265: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 266: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 267: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 268: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 269: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 270: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 271: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 272: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 273: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 274: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 275: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 276: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 277: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 278: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 279: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 280: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 281: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 282: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 283: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 284: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 285: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 286: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 287: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 288: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 289: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 290: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 291: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 292: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 293: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 294: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 295: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 296: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 297: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 298: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 299: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 300: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 301: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 302: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 303: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 304: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 305: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 306: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 307: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 308: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 309: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 310: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 311: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 312: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 313: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 314: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 315: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 316: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 317: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 318: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 319: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 320: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 321: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 322: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 323: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 324: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 325: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 326: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 327: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 328: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 329: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 330: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 331: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 332: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 333: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 334: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 335: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 336: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 337: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 338: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 339: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 340: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 341: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 342: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 343: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 344: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 345: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 346: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 347: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 348: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 349: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 350: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 351: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 352: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 353: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 354: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 355: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 356: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 357: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 358: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 359: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 360: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 361: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 362: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 363: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 364: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 365: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 366: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 367: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 368: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 369: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 370: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 371: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 372: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 373: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 374: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 375: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 376: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 377: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 378: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 379: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 380: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 381: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 382: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 383: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 384: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 385: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 386: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 387: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 388: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 389: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 390: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 391: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 392: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 393: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 394: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 395: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 396: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 397: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 398: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 399: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 400: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 401: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 402: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 403: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 404: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 405: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 406: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 407: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 408: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 409: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 410: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 411: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 412: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 413: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 414: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 415: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 416: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 417: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 418: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 419: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 420: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 421: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 422: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 423: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 424: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 425: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 426: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 427: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 428: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 429: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 430: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 431: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 432: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 433: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 434: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 435: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 436: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 437: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 438: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 439: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 440: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 441: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 442: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 443: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 444: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 445: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 446: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 447: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 448: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 449: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 450: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 451: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 452: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 453: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 454: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 455: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 456: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 457: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 458: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 459: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 460: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 461: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 462: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 463: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 464: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 465: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 466: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 467: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 468: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 469: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 470: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 471: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 472: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 473: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 474: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 475: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 476: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 477: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 478: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 479: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 480: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 481: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 482: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 483: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 484: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 485: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 486: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 487: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 488: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 489: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 490: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 491: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 492: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 493: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 494: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 495: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 496: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 497: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 498: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 499: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 500: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 501: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 502: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 503: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 504: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 505: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 506: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 507: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 508: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 509: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 510: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 511: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 512: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 513: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 514: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 515: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 516: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 517: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 518: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 519: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 520: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 521: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 522: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 523: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 524: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 525: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 526: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 527: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 528: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 529: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 530: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 531: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 532: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 533: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 534: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 535: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 536: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 537: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 538: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 539: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 540: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 541: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 542: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 543: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 544: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 545: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 546: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 547: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 548: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 549: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 550: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 551: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 552: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 553: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 554: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 555: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 556: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 557: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 558: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 559: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 560: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 561: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 562: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 563: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 564: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 565: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 566: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 567: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 568: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 569: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 570: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 571: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 572: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 573: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 574: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 575: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 576: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 577: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 578: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 579: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 580: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 581: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 582: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 583: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 584: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 585: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 586: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 587: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 588: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 589: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 590: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 591: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 592: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 593: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 594: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 595: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 596: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 597: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 598: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 599: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 600: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 601: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 602: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 603: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 604: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 605: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 606: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 607: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 608: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 609: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 610: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 611: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 612: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 613: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 614: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 615: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 616: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 617: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 618: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 619: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 620: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 621: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 622: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 623: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 624: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 625: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 626: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 627: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 628: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 629: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 630: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 631: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 632: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 633: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 634: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 635: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 636: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 637: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 638: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 639: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 640: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 641: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 642: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 643: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 644: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 645: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 646: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 647: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 648: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 649: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 650: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 651: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 652: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 653: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 654: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 655: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 656: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 657: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 658: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 659: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 660: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 661: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 662: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 663: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 664: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 665: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 666: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 667: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 668: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 669: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 670: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 671: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 672: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 673: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 674: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 675: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 676: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 677: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 678: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 679: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 680: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 681: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 682: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 683: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 684: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 685: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 686: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 687: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 688: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 689: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 690: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 691: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 692: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 693: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 694: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 695: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 696: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 697: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 698: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 699: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 700: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 701: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 702: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 703: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 704: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 705: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 706: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 707: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 708: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 709: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 710: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 711: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 712: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 713: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 714: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 715: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 716: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 717: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 718: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 719: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 720: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 721: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 722: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 723: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 724: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 725: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 726: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 727: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 728: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 729: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 730: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 731: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 732: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 733: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 734: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 735: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 736: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 737: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 738: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 739: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 740: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 741: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 742: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 743: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 744: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 745: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 746: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 747: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 748: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 749: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 750: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 751: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 752: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 753: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 754: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 755: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 756: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 757: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 758: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 759: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 760: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 761: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 762: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 763: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 764: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 765: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 766: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 767: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 768: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 769: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 770: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 771: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 772: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 773: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 774: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 775: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 776: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 777: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 778: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 779: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 780: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 781: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 782: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 783: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 784: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 785: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 786: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 787: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 788: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 789: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 790: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 791: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 792: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 793: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 794: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 795: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 796: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 797: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 798: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 799: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 800: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 801: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 802: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 803: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 804: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 805: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 806: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 807: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 808: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 809: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 810: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 811: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 812: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 813: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 814: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 815: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 816: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 817: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 818: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 819: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 820: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 821: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 822: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 823: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 824: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 825: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 826: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 827: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 828: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 829: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 830: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 831: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 832: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 833: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 834: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 835: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 836: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 837: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 838: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 839: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 840: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 841: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 842: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 843: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 844: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 845: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 846: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 847: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 848: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 849: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 850: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 851: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 852: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 853: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 854: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 855: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 856: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 857: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 858: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 859: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 860: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 861: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 862: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 863: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 864: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 865: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 866: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 867: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 868: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 869: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 870: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 871: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 872: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 873: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 874: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 875: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 876: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 877: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 878: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 879: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 880: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 881: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 882: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 883: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 884: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 885: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 886: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 887: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 888: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 889: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 890: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 891: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 892: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 893: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 894: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 895: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 896: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 897: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 898: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 899: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 900: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 901: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 902: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 903: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 904: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 905: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 906: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 907: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 908: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 909: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 910: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 911: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 912: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 913: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 914: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 915: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 916: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 917: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 918: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 919: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 920: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 921: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 922: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 923: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 924: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 925: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 926: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 927: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 928: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 929: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 930: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 931: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 932: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 933: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 934: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 935: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 936: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 937: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 938: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 939: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 940: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 941: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 942: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 943: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 944: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 945: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 946: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 947: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 948: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 949: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 950: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 951: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 952: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 953: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 954: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 955: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 956: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 957: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 958: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 959: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 960: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 961: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 962: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 963: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 964: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 965: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 966: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 967: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 968: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 969: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 970: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 971: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 972: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 973: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 974: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 975: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 976: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 977: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 978: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 979: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 980: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 981: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 982: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 983: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 984: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 985: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 986: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 987: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 988: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 989: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 990: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 991: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 992: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 993: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 994: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 995: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 996: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 997: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 998: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 999: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1000: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1001: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1002: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1003: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1004: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1005: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1006: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1007: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1008: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1009: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1010: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1011: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1012: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1013: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1014: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1015: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1016: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1017: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1018: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1019: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1020: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1021: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1022: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1023: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1024: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1025: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1026: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1027: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1028: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1029: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1030: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1031: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1032: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1033: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1034: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1035: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1036: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1037: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1038: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1039: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1040: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1041: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1042: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1043: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1044: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1045: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1046: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1047: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1048: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1049: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1050: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1051: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1052: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1053: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1054: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1055: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1056: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1057: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1058: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1059: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1060: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1061: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1062: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1063: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1064: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1065: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1066: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1067: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1068: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1069: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1070: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1071: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1072: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1073: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1074: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1075: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1076: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1077: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1078: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1079: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1080: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1081: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1082: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1083: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1084: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1085: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1086: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1087: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1088: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1089: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1090: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1091: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1092: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1093: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1094: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1095: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1096: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1097: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1098: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1099: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1100: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1101: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1102: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1103: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1104: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1105: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1106: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1107: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1108: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1109: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1110: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1111: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1112: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1113: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1114: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1115: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1116: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1117: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1118: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1119: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1120: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1121: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1122: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1123: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1124: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1125: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1126: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1127: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1128: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1129: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1130: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1131: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1132: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1133: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1134: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1135: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1136: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1137: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1138: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1139: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1140: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1141: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1142: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1143: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1144: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1145: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1146: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1147: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1148: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1149: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1150: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1151: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1152: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1153: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1154: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1155: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1156: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1157: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1158: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1159: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1160: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1161: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1162: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1163: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1164: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1165: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1166: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1167: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1168: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1169: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1170: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1171: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1172: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1173: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1174: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1175: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1176: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1177: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1178: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1179: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1180: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1181: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1182: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1183: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1184: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1185: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1186: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1187: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1188: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1189: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1190: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1191: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1192: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1193: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1194: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1195: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1196: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1197: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1198: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1199: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1200: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1201: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1202: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1203: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1204: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1205: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1206: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1207: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1208: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1209: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1210: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1211: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1212: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1213: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1214: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1215: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1216: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1217: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1218: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1219: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1220: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1221: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1222: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1223: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1224: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1225: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1226: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1227: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1228: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1229: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1230: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1231: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1232: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1233: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1234: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1235: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1236: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1237: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1238: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1239: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1240: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1241: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1242: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1243: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1244: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1245: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1246: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1247: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1248: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1249: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1250: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1251: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1252: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1253: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1254: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1255: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1256: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1257: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1258: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1259: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1260: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1261: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1262: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1263: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1264: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1265: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1266: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1267: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1268: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1269: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1270: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1271: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1272: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1273: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1274: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1275: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1276: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1277: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1278: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1279: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1280: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1281: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1282: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1283: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1284: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1285: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1286: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1287: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1288: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1289: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1290: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1291: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1292: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1293: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1294: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1295: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1296: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1297: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1298: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1299: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1300: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1301: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1302: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1303: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1304: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1305: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1306: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1307: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1308: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1309: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1310: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1311: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1312: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1313: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1314: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1315: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1316: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1317: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1318: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1319: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1320: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1321: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1322: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1323: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1324: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1325: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1326: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1327: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1328: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1329: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1330: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1331: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1332: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1333: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1334: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1335: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1336: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1337: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1338: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1339: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1340: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1341: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1342: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1343: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1344: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1345: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1346: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1347: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1348: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1349: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1350: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1351: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1352: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1353: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1354: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1355: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1356: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1357: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1358: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1359: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1360: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1361: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1362: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1363: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1364: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1365: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1366: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1367: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1368: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1369: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1370: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1371: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1372: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1373: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1374: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1375: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1376: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1377: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1378: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1379: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1380: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1381: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1382: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1383: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1384: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1385: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1386: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1387: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1388: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1389: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1390: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1391: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1392: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1393: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1394: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1395: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1396: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1397: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1398: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1399: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1400: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1401: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1402: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1403: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1404: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1405: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1406: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1407: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1408: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1409: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1410: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1411: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1412: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1413: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1414: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1415: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1416: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1417: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1418: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1419: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1420: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1421: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1422: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1423: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1424: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1425: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1426: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1427: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1428: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1429: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1430: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1431: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1432: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1433: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1434: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1435: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1436: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1437: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1438: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1439: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1440: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1441: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1442: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1443: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1444: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1445: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1446: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1447: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1448: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1449: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1450: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1451: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1452: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1453: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1454: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1455: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1456: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1457: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1458: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1459: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1460: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1461: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1462: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1463: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1464: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1465: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1466: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1467: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1468: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1469: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1470: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1471: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1472: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1473: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1474: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1475: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1476: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1477: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1478: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1479: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1480: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1481: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1482: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1483: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1484: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1485: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1486: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1487: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1488: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1489: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1490: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1491: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1492: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1493: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1494: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1495: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1496: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1497: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1498: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1499: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n",
      "Iteration 1500: Loss 1570.58124137279, Weight -1.0431269488862978, Bias 12.235148310505833\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Weight: -1.0431269488862978\n",
      "Estimated Bias: 12.235148310505833\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "est_weight, est_bias = train_model(X_train, y_train, alpha, max_epoch)\n",
    "print(f\"Estimated Weight: {est_weight}\\nEstimated Bias: {est_bias}\")\n",
    "y_pred = (est_weight*X_test) + est_bias\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(y_test, y_pred, marker='o', color='red')\n",
    "\n",
    "plt.plot([min(X_test), max(X_test)], [min(y_pred), max(y_pred)], color='blue', label=\"line1\")\n",
    "#plt.plot([min(X_test), max(X_test)], [min(y_test), max(y_test)], color='orange', label=\"line2\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.841913305022075\n",
      "1719.2467180299154\n",
      "-0.013453292642622738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}